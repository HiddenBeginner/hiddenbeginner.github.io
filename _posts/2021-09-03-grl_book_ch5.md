---
layout: post
title:  "[GRL Book 정리] Chapter 5. The Graph Neural Network Model"
date:   2021-9-3 12:00
categories: [Others]
use_math: true
comments: true
---

![intro](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/earth-network.jpg){: .center}
<center>사진 출처: <a href="#ref1">[1]</a></center>

# <center>Chapter 5. The Graph Neural Network Model</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 네 번째 이야기. 부디 완주하게 기도해주세요 !

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

**왜 Chapter 3 다음에 Chapter 5인가요? Chapter 4는 어디다 팔아드셨나요?**

우리나라에서는 유독 숫자 4를 사용하는걸 꺼려한다. 한자 死 (죽을 사)와 발음이 같아서 그렇다. 옛날 건물을 보면 4층이 없고 바로 3층에서 5층으로 넘어간다. 이와 같은 맥락으로 Chapter 4를 스킵하고 Chapter 5로 넘어가려고 한다.

는 장난이고, 지금 참여하고 있는 스터디에서 CS224W와 GRL Book을 병행하고 있다. 메인이 CS224W이기 때문에 이와 진도와 맞추기 위하여  Chapter  4는 Chapter 7을 공부한 후에 다루는 것으로 계획되어 있다. 그 때까지 내가 포기하지 않고 포스팅을 하고 있다면, Chapter 4 정리를 볼 수 있을 것이다 ... !!

</div>

<br>

---

# 5.0 Gentle Introduction
Chapter 3까지 배웠던 노드 임베딩 기법들은 각 노드의 속성값 (attribute 또는 feature)을 사용하지 않는다는 한계점이 있었다. 이번 챕터에서는 그래프의 구조 뿐만 아니라 노드의 속성값도 고려하여 노드 임베딩 벡터를 만들 수 있는 그래프 신경망 (Graph Neural Network)에 대해서 알아본다.

<br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! </p>

노드마다 부여된 정보를 속성값 (attribute) 또는 속성 벡터라고 부른다. 소셜 네트워크에서 사람 한 명이 노드를 나타낸다고 했을 때, 그 사람의 성별, 나이, 지역 등은 노드의 속성값들이다. 한편, 속성 벡터를 포함하여 노드를 나타내는 벡터들을 포괄적으로 feature 벡터라고 부른다. 본 포스팅에서 feature 벡터라는 표현만 사용하도록 하겠다. 

</div>

<br>

지금까지 우리는 그래프 데이터를 머신러닝 모델에 입력하기 위해 많은 노력을 해왔다. 지금부터는 딥러닝에 그래프 데이터를 입력할 수 있는 방법들에 대해 알아본다. 다층 퍼셉트론 (Multilayer perceptron, MLP)은 정해진 크기의 벡터를 입력 받는다. MLP에 그래프 $\mathcal{G}=(\mathcal{V}, \mathcal{E})$를 입력할 수 있는 가장 간단한 방법은 인접행렬 $\mathbf{A}$를 쭉 펼쳐서 벡터로 만드는 것이다. 즉,

$$\mathbf{z}_{\mathcal{G}}=\text{MLP}(\mathbf{A}[1] \oplus \mathbf{A}[2] \oplus \cdots \oplus \mathbf{A}[| \mathcal{V}|]), \quad \quad (5.1)$$

<br>

이때, $\mathbf{A}[u] \in \mathbb{R}^{\mid \mathcal{V} \mid}$는 인접행렬의 $u$번째 행이고, $\oplus$는 벡터를 한 줄로 쭉 연결 (concatenate)하는 연산이다. $\mathbf{z}_{\mathcal{G}}$는 그래프 $\mathcal{G}$를 MLP에 통과시켜서 얻은 결과물이다. <br><br>

여기서 가장 큰 문제점은 노드에 어떤 순서를 부여하여 인접행렬을 만들었다는 것이다. 노드의 순서를 뒤섞으면 인접행렬도 뒤섞인 순서에 맞게 바뀌게 된다. 하지만 인접행렬이 나타내는 그래프는 여전히 동일하다. 같은 그래프를 나타내지만 노드 순서만 바뀐 인접행렬을 상상해보자. 이 인접행렬을 MLP에 통과시킬 경우 원래와 다른 결과를 출력해줄 것이다.  그래서 식 $(5.1)$과 같은 전략은 그래프를 상대로는 적합하지 않다.<br><br>

우리는 노드 순서가 뒤바뀌어도 여전히 같은 결과를 출력해주는 함수를 원한다. 아니면 적어도 결과물의 순서를 뒤바꿔서 원래의 결과물로 만들어 줄 수 있는 함수를 원한다. 첫 번째 같은 함수를 `permutation invariant` 함수라고 부르고 두 번째 함수는 `permutation equivariant` 함수라고 부른다.<br><br>

행렬 $\mathbf{P}$를 permutation 행렬이라고 하자. 어떤 행렬 $\mathbf{A}$에 permutation 행렬을 왼쪽에 곱하면 $\mathbf{P}\mathbf{A}$는 $\mathbf{A}$에서 행의 순서만 바뀐 행렬이 된다. 반대로 오른쪽에 곱하면 $\mathbf{A}$에서 열의 순서만 바뀐 행렬이 된다.<br><br>

한편, 그래프의 경우 노드의 순서가 바뀌게 되면 인접행렬의 행과 열이 모두 바뀌게 된다. 따라서 인접행렬 $\mathbf{A}$에서 노드 순서가 바뀔 경우 새로운 인접행렬은 $\mathbf{P}\mathbf{A}\mathbf{P}^\top$가 된다. 그래프에 대해서 `permutation invariant`한 함수 $f$는 다음의 성질을 만족시킨다. (이때, 함수 $f:\mathbb{R}^{\mid \mathcal{V} \mid \times \mid \mathcal{V} \mid}\rightarrow \mathbb{R}^{\mid \mathcal{V} \mid}$는 인접행렬을 입력 받아서 노드마다 어떤 값을 주는 벡터를 출력해주는 함수이다.)

$$f(\mathbf{P} \mathbf{A} \mathbf{P}^\top)=f(\mathbf{A}) \quad \quad (\text{Permutation Invariance}) \quad \quad (5.2)$$

<br>

즉, 노드 순서가 뒤바뀐 인접행렬을 넣었을 때 함수값이 그냥 인접행렬을 넣었을 때의 함수값과 똑같다는 것이다. 한편, `permutation equivariant`한 함수 $f$는 다음의 성질을 만족시킨다.

$$f(\mathbf{P} \mathbf{A} \mathbf{P}^\top)=\mathbf{P}f(\mathbf{A}) \quad \quad (\text{Permutation Equivariant}) \quad \quad (5.3)$$

<br>

즉, 노드 순서를 바꿨던대로 $f(\mathbf{A})$의 원소의 순서를 바꿔준 것과 $f(\mathbf{P}\mathbf{A}\mathbf{P}^\top)$이 같다는 것이다. 정리하자면, 우리는 `permutation invariant`한 함수나 아니면 적어도 `permutation equivariant`한 성질을 같는 딥러닝 모델을 고려해야 한다.

---

# 5.1 Neural Message Passing
**Coming soon!**

<br>

---

## 참고문헌
<p id="ref1">[1] <a href="https://pixabay.com/ko/illustrations/%ec%a7%80%ea%b5%ac-%ed%9a%8c%eb%a1%9c%eb%a7%9d-3537401/" target="_blank">https://pixabay.com/ko/illustrations/지구-회로망-3537401/</a></p>
<p id="ref2">[2] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref3">[3] <a href="http://web.stanford.edu/class/cs224w/" target="_blank">CS224W: Machine Learning with Graphs</a></p>
