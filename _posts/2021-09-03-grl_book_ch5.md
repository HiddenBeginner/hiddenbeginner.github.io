---
layout: post
title:  "[GRL Book 정리] Chapter 5. The Graph Neural Network Model"
date:   2021-9-3 12:00
categories: [Others]
use_math: true
comments: true
---

![intro](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/earth-network.jpg){: .center}
<center>사진 출처: <a href="#ref1">[1]</a></center>

# <center>Chapter 5. The Graph Neural Network Model</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 네 번째 이야기. 부디 완주하게 기도해주세요 !

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

**왜 Chapter 3 다음에 Chapter 5인가요? Chapter 4는 어디다 팔아드셨나요?**

우리나라에서는 유독 숫자 4를 사용하는걸 꺼려한다. 한자 死 (죽을 사)와 발음이 같아서 그렇다. 옛날 건물을 보면 4층이 없고 바로 3층에서 5층으로 넘어간다. 이와 같은 맥락으로 Chapter 4를 스킵하고 Chapter 5로 넘어가려고 한다.

는 장난이고, 지금 참여하고 있는 스터디에서 CS224W와 GRL Book을 병행하고 있다. 메인이 CS224W이기 때문에 이와 진도와 맞추기 위하여  Chapter  4는 Chapter 7을 공부한 후에 다루는 것으로 계획되어 있다. 그 때까지 내가 포기하지 않고 포스팅을 하고 있다면, Chapter 4 정리를 볼 수 있을 것이다 ... !!

</div>

<br>

---

# 5.0 Gentle Introduction
Chapter 3까지 배웠던 노드 임베딩 기법들은 각 노드의 속성값 (attribute 또는 feature)을 사용하지 않는다는 한계점이 있었다. 이번 챕터에서는 그래프의 구조 뿐만 아니라 노드의 속성값도 고려하여 노드 임베딩 벡터를 만들 수 있는 그래프 신경망 (Graph Neural Network)에 대해서 알아본다.

<br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! </p>

노드마다 부여된 정보를 속성값 (attribute) 또는 속성 벡터라고 부른다. 소셜 네트워크에서 사람 한 명이 노드를 나타낸다고 했을 때, 그 사람의 성별, 나이, 지역 등은 노드의 속성값들이다. 한편, 속성 벡터를 포함하여 노드를 나타내는 벡터들을 포괄적으로 feature 벡터라고 부른다. 본 포스팅에서 feature 벡터라는 표현만 사용하도록 하겠다. 

</div>

<br>

지금까지 우리는 그래프 데이터를 머신러닝 모델에 입력하기 위해 많은 노력을 해왔다. 지금부터는 딥러닝에 그래프 데이터를 입력할 수 있는 방법들에 대해 알아본다. 다층 퍼셉트론 (Multilayer perceptron, MLP)은 정해진 크기의 벡터를 입력 받는다. MLP에 그래프 $\mathcal{G}=(\mathcal{V}, \mathcal{E})$를 입력할 수 있는 가장 간단한 방법은 인접행렬 $\mathbf{A}$를 쭉 펼쳐서 벡터로 만드는 것이다. 즉,

$$\mathbf{z}_{\mathcal{G}}=\text{MLP}(\mathbf{A}[1] \oplus \mathbf{A}[2] \oplus \cdots \oplus \mathbf{A}[| \mathcal{V}|]), \quad \quad (5.1)$$

<br>

이때, $\mathbf{A}[u] \in \mathbb{R}^{\mid \mathcal{V} \mid}$는 인접행렬의 $u$번째 행이고, $\oplus$는 벡터를 한 줄로 쭉 연결 (concatenate)하는 연산이다. $\mathbf{z}_{\mathcal{G}}$는 그래프 $\mathcal{G}$를 MLP에 통과시켜서 얻은 결과물이다. <br><br>

여기서 가장 큰 문제점은 노드에 어떤 순서를 부여하여 인접행렬을 만들었다는 것이다. 노드의 순서를 뒤섞으면 인접행렬도 뒤섞인 순서에 맞게 바뀌게 된다. 하지만 인접행렬이 나타내는 그래프는 여전히 동일하다. 같은 그래프를 나타내지만 노드 순서만 바뀐 인접행렬을 상상해보자. 이 인접행렬을 MLP에 통과시킬 경우 원래와 다른 결과를 출력해줄 것이다.  그래서 식 $(5.1)$과 같은 전략은 그래프를 상대로는 적합하지 않다.<br><br>

우리는 노드 순서가 뒤바뀌어도 여전히 같은 결과를 출력해주는 함수를 원한다. 아니면 적어도 결과물의 순서를 뒤바꿔서 원래의 결과물로 만들어 줄 수 있는 함수를 원한다. 첫 번째 같은 함수를 `permutation invariant` 함수라고 부르고 두 번째 함수는 `permutation equivariant` 함수라고 부른다.<br><br>

행렬 $\mathbf{P}$를 permutation 행렬이라고 하자. 어떤 행렬 $\mathbf{A}$에 permutation 행렬을 왼쪽에 곱하면 $\mathbf{P}\mathbf{A}$는 $\mathbf{A}$에서 행의 순서만 바뀐 행렬이 된다. 반대로 오른쪽에 곱하면 $\mathbf{A}$에서 열의 순서만 바뀐 행렬이 된다.<br><br>

한편, 그래프의 경우 노드의 순서가 바뀌게 되면 인접행렬의 행과 열이 모두 바뀌게 된다. 따라서 인접행렬 $\mathbf{A}$에서 노드 순서가 바뀔 경우 새로운 인접행렬은 $\mathbf{P}\mathbf{A}\mathbf{P}^\top$가 된다. 그래프에 대해서 `permutation invariant`한 함수 $f$는 다음의 성질을 만족시킨다. (이때, 함수 $f:\mathbb{R}^{\mid \mathcal{V} \mid \times \mid \mathcal{V} \mid}\rightarrow \mathbb{R}^{\mid \mathcal{V} \mid}$는 인접행렬을 입력 받아서 노드마다 어떤 값을 주는 벡터를 출력해주는 함수이다.)

$$f(\mathbf{P} \mathbf{A} \mathbf{P}^\top)=f(\mathbf{A}) \quad \quad (\text{Permutation Invariance}) \quad \quad (5.2)$$

<br>

즉, 노드 순서가 뒤바뀐 인접행렬을 넣었을 때 함수값이 그냥 인접행렬을 넣었을 때의 함수값과 똑같다는 것이다. 한편, `permutation equivariant`한 함수 $f$는 다음의 성질을 만족시킨다.

$$f(\mathbf{P} \mathbf{A} \mathbf{P}^\top)=\mathbf{P}f(\mathbf{A}) \quad \quad (\text{Permutation Equivariant}) \quad \quad (5.3)$$

<br>

즉, 노드 순서를 바꿨던대로 $f(\mathbf{A})$의 원소의 순서를 바꿔준 것과 $f(\mathbf{P}\mathbf{A}\mathbf{P}^\top)$이 같다는 것이다. 정리하자면, 우리는 `permutation invariant`한 함수나 아니면 적어도 `permutation equivariant`한 성질을 같는 딥러닝 모델을 고려해야 한다.

---

# 5.1 Neural Message Passing
지금까지 등장한 GNN 모델들은 정말 다양한 동기로부터 만들어졌다. 하지만 그들 모두가 공유하고 있는 한 가지 구조가 있다. 바로 신경망 메세지 전달 (Neural Message Passing) 구조이다.  신경망 메세지 전달이란 뉴럴 네트워크를 통하여 노드끼리  메세지 벡터를 교환하는 것을 말한다.

<br>

## 5.1.1 Overview of the Message Passing Framework
노드 $u$의 $k$ 번째 hidden state 벡터를 $\mathbf{h}^{(k)}_u$라고 하자. $k+1$ 번째 hidden state $\mathbf{h}^{(k+1)}_u$는 노드 $u$의 이웃 노드로부터 전달 받은 정보들을 취합 (`AGGREGATE`)하고 업데이트 (`UPDATE`)하여 만들어진다. 즉,

$$\begin{matrix}
\mathbf{h}^{(k+1)}_u 
&=& \text{UPDATE}^{(k)} \;(\mathbf{h}^{(k)}_u, \text{AGGREGATE}^{(k)}(\{ \mathbf{h}^{(k)}_v:v \in \mathcal{N}(u) \}) & (5.4) \\
& = & \text{UPDATE}^{(k)} \;(\mathbf{h}^{(k)}_u, \mathbf{m}^{(k)}_{\mathcal{N}(u)}) & (5.5)
\end{matrix}$$

<br>

이때, `AGGREGATE`와 `UPDATE`는 임의의 미분 가능한 함수이다. `AGGREGATE` 함수와 `UPDATE` 함수를 어떻게 선택하느냐에 따라서 GNN 알고리즘이 다양하게 구분될 수 있다. 두 함수에 대한 다양한 예시를 이번 장에서 마주치게 될 것이다. $\mathbf{m}\_{\mathcal{N}(u)}$은 노드 $u$의 이웃 노드의 정보를 취합하여 만든 메세지 벡터이다.<br><br>

우리는 먼저 $k=0$일 때 각 노드 $u$의 임베딩 벡터를 $u$의 feature 벡터로 설정한다. 즉, $\mathbf{h}^{(0)}_u=\mathbf{x}_u, \; \forall u \in V$ 이다. 다음으로 다음의 신경망 메세지 전달을 반복한다. 각 $k$번 째 레이어 (또는 반복이라고도 말함)에서 하는 일은 다음과 같다.

- `AGGREGATE` 함수는 $u$의 이웃 노드 $\mathcal{N}(u)$들의 hidden state **집합**을 입력 받아서 메세지 벡터 $\mathbf{m}^{(k)}\_{\mathcal{N}(u)}$를 만든다.
- `UPDATE` 함수는 메세지 벡터 $\mathbf{m}^{(k)}\_{\mathcal{N}(u)}$와 노드 $u$의 $k$ 번째 hidden state $\mathbf{h}^{(k)}_u$를 입력 받아서 hidden state를 업데이트한다.

<br>

위의 설명에서 한 가지 주목해야 할 점은 `AGGREGATE` 함수가 $\mathcal{N}(u)$들의 hidden state **집합**을 입력 받는다는 것이다. 한 집합 안에서 원소들의 순서가 뒤섞여도 여전히 같은 집합이다. 따라서 `AGGREGATE` 함수는 입력 받는 hidden state들의 순서에 상관 없이 일정한 결과를 계산해줘야 한다 (permutation invariance). 또는 최소한 계산된 결과를 다시 뒤섞어서 순서를 맞춰줄 수 있어야 한다(permutation equivariance).<br><br>

$K$번의 메세지 반복을 마치고 얻은 각 노드 $u$의 임베딩 벡터 $\mathbf{h}^{(K)}_u$를 노드 $u$의 임베딩 벡터로 사용하게 된다. 즉, $\mathbf{z}_u=\mathbf{h}^{(K)}_u, \; \forall u \in V$이다.<br><br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 노드의 속성 벡터가 없으면 어떡하나요?</p>

만약 노드 속성 벡터 (attribute vector)가 없다면, 이전 챕터들에서 배운 방법들을 사용하여 feature 벡터들을 메뉴얼하게 만들어줘야 한다.

만약, 주어진 그래프에서 노드 임베딩만하는 것이 목적이라면 identity feature를 사용해도 된다. 노드 $u$의 identity feature $\mathbf{e}_u \in \mathbb{R}^{\mid V \mid}$는 $u$번 째 원소만 1이고 나머지 원소는 0인 원핫 벡터이다. 하지만 이렇게 만든 GNN 모델의 경우 새로 유입되는 노드를 임베딩해줄 수 없는 transductive 성질을 갖게 된다.

</div>

---

## 5.1.2 Motivations and Intuitions
GNN에서 메세지 전달을 한 번 진행하면 한 노드가 이웃 노드들의 정보를 반영하여 벡터를 만들게 된다. 메세지 전달을 두 번 진행하면 한 노드에 대하여 이웃 노드의 이웃 노드의 정보까지 반영하여 임베딩 벡터가 만들어지게 된다. 이렇게 메세지 전달이 많이 반복될 수록 한 노드에 대하여 더 멀리 떨어진 이웃 노드의 정보를 반영하여 벡터를 만들 수 있게 된다.<br><br>

그럼 어떤 정보를 반영하는 것일까? 노드 $u$의 $k$ 번째 hidden state $\mathbf{h}^{(k)}_u$는 크게 두 가지 정보를 반영하게 된다. 

- 그래프 안의 구조적인 정보. $\mathbf{h}^{(k)}_u$는 노드 $u$의 $k$-hop 이웃 노드 정보를 반영하고 있다.
- feature 기반 정보: $\mathbf{h}^{(k)}_u$은 노드 $u$의 $k$-hop 이웃 노드의 feature 벡터 정보를 반영하고 있다.

<br>

이웃한 데이터로부터 feature 벡터를 취합한다는 점에서 합성곱 신경망 (CNN)과 유사하다고 볼 수 있다. CNN은 정해진 크기의 패치 안의 feature 정보를 취합 (합성곱)한다는 점과 GNN은 이웃 노드의 정보를 취합한다는 점에서 차이가 있다.

---

## The Basic GNN
이번 장에서는 가장 간단한 `AGGREGATE` 함수와 `UPDATE` 함수를 사용하여 기초적인 GNN 모델을 만들어 볼 예정이다.  다음과 같이 $k$ 번째 레이어에서의 GNN 메세지 전달식을 생각해보자. (앞으로 이 모델을 GNN 기초 모델이라고 부르겠다.)

$$\mathbf{h}^{(k)}_u=\sigma\left(\mathbf{W}^{(k)}_{\text{self}}\mathbf{h}^{(k-1)}_u+\mathbf{W}^{(k)}_{\text{neigh}}\sum\limits_{v \in \mathcal{N}(v)}\mathbf{h}^{(k-1)}_v+\mathbf{b}^{(k)}\right), \quad \quad (5.7)$$

<br>

이때, $\mathbf{W}^{(k)}\_{\text{self}}, \mathbf{W}^{(k)}\_{\text{neigh}} \in \mathbb{R}^{d^{(k)} \times d^{(k-1)}}$는 학습 가능한 파라미터로 이루어진 행렬이다. $d^{(k-1)}$차원 hidden state 벡터들을 $d^{(k)}$차원 벡터로 변환해주는 역할을 한다. $\sigma$ 함수는 벡터를 입력 받아 원소마다 비선형함수를 적용하는 함수이다. 대표적으로 `tanh`나 `ReLU`이 있을 것이다. $\mathbf{b}^{(k)} \in \mathbb{R}^{d^{(k)}}$는 편향 벡터 (bias term)로 학습 가능한 파라미터 벡터이다. 편향 벡터는 표기의 편의를 위해 생략해주기도 하지만, 실제 구현에서는 큰 역할을 한다.
- 파라미터 행렬과 편향 벡터를 $k$에 따라 구분지었지만, 매 $k$마다 같은 파라미터 행렬과 편향 벡터를 공유하여 사용해주기도 한다.

<br>

식 $(5.7)$에서 이웃 노드들의 hidden state들을 단순 합해주는 것이 바로 `AGGREGATE` 함수이다. 즉,

$$\mathbf{m}^{(k)}_{\mathcal{N}(u)}=\text{AGGREGATE}^{(k)}\left(\{ \mathbf{h}^{(k-1)}_v:v \in \mathcal{N}(u) \}\right)=\sum\limits_{v \in \mathcal{N}(u)}\mathbf{h}^{(k-1)}_v, \quad \quad (5.8)$$

<br>

한편, `UPDATE` 함수는 노드의 이전 hidden state 벡터와 메세지 벡터를 각각 변환해주고 편향 벡터와 함께 더해주어 비선형 함수를 적용시키는 함수이다. 즉,

$$\text{UPDATE}^{(k)} \;(\mathbf{h}^{(k)}_u, \mathbf{m}^{(k)}_{\mathcal{N}(u)})=\sigma\left(\mathbf{W}^{(k)}_{\text{self}}\mathbf{h}^{(k-1)}_{u}+\mathbf{W}^{(k)}_{\text{neigh}}\mathbf{m}^{(k)}_{}+\mathbf{b}^{(k)}\right), \quad \quad (5.9)$$

<br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 업데이트식 행렬 버전</p>

참고사항으로 식 $(5.7)$은 노드의 hidden state 단위로 메세지 전달식을 나타낸 것이다. 한편, hidden state들을 한 행렬에 묶어서 메세지 전달식을 나타낼 수도 있다. 두 표기법 모두 잘 사용되지만 후반부 어텐션 (attention) 설명을 위해서는 hidden state 단위로 메세지 전달식을 표기하는 것이 더욱 좋다.

$$\mathbf{H}^{(k)}=\sigma\left(\mathbf{H}^{(k-1)}\mathbf{W}^{(k)}_{\text{self}}+\mathbf{A}\mathbf{H}^{(k-1)}\mathbf{W}^{(k)}_{\text{neigh}}\right), \quad \quad (5.11)$$

<br>

이때, $\mathbf{H}^{(k)} \in \mathbb{R}^{\mid V \mid \times d^{(k)}}$은 $k$ 번째 hidden state 벡터를 행벡터로 갖는 행렬이다.  $\mathbf{A}$는 인접행렬이다. 편향 벡터는 생략해주었지만 괄호 안에서 행렬의 행벡터마다 편향 벡터를 더해주면 된다. 
- 그리고 개인적인 생각인데, 식 $(5.7)$에 정의된 파라미터 행렬이라면 전치 행렬을 곱해주는 것이 맞는 것 같다. 즉, $\mathbf{H}^{(k-1)}(\mathbf{W}^{(k)}\_{\text{self}})^\top+\mathbf{A}\mathbf{H}^{(k-1)}(\mathbf{W}^{(k)}\_{\text{neigh}})^\top )$

</div>

<br>

---

## 5.1.4 Message Passing with Self-loops
GNN 모델을 조금 더 간략하게 만들기 위한 방법으로 노드마다 자기 자신으로 가는 셀프 루프를 추가해주는 방법이 있다. 셀프 루프를 추가하면 자신의 이웃 노드에 자신도 포함되게 된다. 따라서 자신의 hidden state 벡터를 `AGGREGATE` 함수에서 사용할 수 있게 된다. 그렇게 되면 자신의 hidden state 벡터와 메세지 벡터를 합쳐주는 `UPDATE` 함수를 생략해도 된다. 즉,

$$\mathbf{h}^{(k)}_u=\text{AGGREGATE}\left(\{ \mathbf{h}^{(k-1)}_v:v \in\mathcal{N}(u) \cup \{u\}\}\right), \quad \quad (5.12)$$

<br>

`AGGREGATE` 함수가 $\mathcal{N}(u) \cup \{u\}$을 대상으로 계산을 하겠다는 것이다.  물론, 자기 자신의 정보와 이웃 노드의 정보에 각각 다른 연산을 취해주는 원래 버전보다는 모델의 표현력 (expressivity)이 감소한다는 단점이 있지만, 반대로 말하면 오버피팅을 방지해줄 수 있다는 것이기도 하다. 셀프 루프를 추가해서 GNN을 수행하는 기법들을 앞으로 셀프 루프 GNN (`self-loop GNN`)으로 부를 것이다.<br><br>

예를 들어 식 $(5.7)$에서 셀프 루프를 추가한다는 것은 $\mathbf{W}\_{\text{self}}$와 $\mathbf{W}\_{\text{neigh}}$을 서로 같은 파라미터 행렬로 사용하는 것과 동일한 것으로 볼 수 있다 (노드의 hidden state 벡터와 메세지 벡터에 정확히 동일한 연산을 취해주기 때문이다.).  이를 그래프 단위 메세지 전달식으로 나타내면 다음과 같다.

$$\mathbf{H}^{(k)}=\sigma\left((\mathbf{I}+\mathbf{A})\mathbf{H}^{(k-1)}\mathbf{W}^{(k)}\right). \quad \quad (5.13)$$

---

# 5.2 Generalized Neighborhood Aggregation
식 $(5.7)$은 기초적인 GNN 모델이지만 꽤 좋은 성능을 보일 뿐만 아니라 이론적으로도 연구가 잘 되어 있다. 하지만 우린 아직 목마르다. 이번 섹션에서는 다양한 `AGGREGATE` 함수에 대해 알아볼 것이다.

---

## 5.2.1 Neighborhood Normalization

GNN 기초 모델에서 사용한 `AGGREGATE` 함수 (식 $5.8$)는 이웃 노드들의 이전 hidden state들을 모두 더해준 것이였다. 그럼 이웃이 많은 노드의 hidden state 벡터는 자연스럽게 크기가 커질 것이다. 이는 GNN 모델을 학습시킬 때 수렴을 저해할 수 있다. 만약 노드 $u$의 차수가 노드 $u'$보다 100배 더 많다고 생각해보자. 그럼 $\lVert \sum\_{v\in\mathcal{N}(u)}\mathbf{h}_v \rVert >> \lVert \sum\_{v'\in\mathcal{N}(u')}\mathbf{h}\_{v'} \rVert$일 것이다. 벡터의 크기가 큰 영역은 다른 영역보다 그레디언트의 스케일이 크기 때문에 안정적인 최적화를 저해할 수 있다.<br><br>

생각해볼 수 있는 가장 쉬운 해결 방법은 식 $(5.8)$을 노드 차수로 나눠주는 것이다. 즉,

$$\mathbf{m}_{\mathcal{N}(u)}=\frac{\sum_{v \in \mathcal{N}(u)}\mathbf{h}_v}{\mid \mathcal{N}(u) \mid} \quad \quad (5.14)$$

<br>

다른 해결 방법은 Kipf and Welling, 2016a <a href="#kipf">[4]</a>에서 제안했던 `symmetric normalization` 을 사용하는 것이다. 즉,

$$\mathbf{m}_{\mathcal{N}(u)}=\sum\limits_{v \in \mathcal{N}(u)}\frac{\mathbf{h}_v}{\sqrt{\mid \mathcal{N}(u) \mid \mid \mathcal{N}(v) \mid}} \quad \quad (5.15)$$

<br>

책에서는 `symmetric normalization`과 GNN 기초 모델의 `UPDATE` 함수 (식 $5.9$)를 합친 것이 `spectral graph convolution`에 대한 1차 근사치라는 것도 언급하고 있다.<br><br>

어떤 문제에서는 차수가 높은 노드가 의외로 유용하지 않을 수 있다. 예를 들어, 논문 인용 네트워크 안에서 논문들을 군집화하는 문제를 생각해보자. 저명한 논문은 분야를 가리지 않고 인용이 많이 되기 때문에 군집화할 때 썩 유용한 정보는 아닐 것이다. 이런 관점에서는 정규화를 하는 것이 타당해보인다.<br><br> 

하지만, 그렇다고 정규화가 무조건 좋은 것은 아니다. 정규화를 하면 정보 손실이 발생한다. 정규화를 포함한 GNN으로 노드 임베딩을 한다고 생각해보자. 그럼 노드들의 차수와 관련된 특징들이 무시된채로 임베딩 벡터가 만들어질 수 있다. 사실, GNN 기초 모델에 식 $(5.14)$의 정규화를 적용하는 것보다 그냥 식 $(5.8)$의 `AGGRAGATE`를 사용하는 것이 더 좋다고 한다. 이와 관련된 이론적인 내용들은 Chapter $7$에서 다뤄볼 예정이라고 한다.<br><br>

그럼 도대체가 정규화를 하라는건가 말라는건가. 정답이 딱 정해져 있지는 않지만, 그래프의 구조적인 특징들보다 노드의 속성 벡터가 더 중요한 경우 정규화가 유용하다고 한다.  또는 노드 차수 때문에 모델 학습이 잘 안 될 경우도 정규화를 하는게 좋다고 한다.

<br>

---

## 5.2.2 Set Aggregators
섹션 $5.2.1$에서는 정규화 관점에서 `AGGREGATE` 함수를 바라보았다면, 이번 섹션에서는 permutation invariant 관점에서 `AGGREGATE` 함수를 다뤄볼 것이다. `AGGREGATE` 함수는 집합 $\\{ \mathbf{h}_v : v \in \mathcal{N}(u) \\}$를 입력 받아서 메세지 벡터 $\mathbf{m}\_{\mathcal{N}(u)}$를 출력해준다. 여기서 주목해야할 점은 집합을 입력 받는다는 것이다. 집합은 원소의 순서를 아무리 뒤죽박죽 섞어도 여전히 같은 집합이다. 따라서 `AGGREGATE` 함수는 집합 안의 순서가 달라지더라도 같은 결과를 출력해줘야 한다. 이와 같은 성질은 permutation invariant라고 한다.

<br>

---
### Set pooling
`Universal approximation theorem`에 의하면 다층퍼셉트론 (MLP)을 사용하여 모든 함수에 근사할 수 있다. 대충 말하자면 이론적으로 MLP를 사용하여 모든 함수를 만들어 낼 수 있다는 것이다. 이와 유사하게 아래 식 $(5.17)$의 `AGGREGATE` 함수를 사용하면 이론적으로 모든 permutation invariant 함수에 근사할 수 있다고 한다.

$$\mathbf{m}_{\mathcal{N}(u)}=\text{MLP}_{\theta}\left(\sum\limits_{v \in \mathcal{N}( u)}\text{MLP}_\phi(\mathbf{h}_v) \right) \quad \quad (5.17)$$

<br>

이웃 노드 $v \in \mathcal{N}(u)$ 마다 hidden state $\mathbf{h}\_v$를 $\text{MLP}\_\phi$에 통과시켜 $\text{MLP}\_\phi(\mathbf{h}\_v)$를 얻는다. 그리고 모든 $\text{MLP}\_\phi(\mathbf{h}\_v)$를 더해준 것을 또 다른 $\text{MLP}\_\theta$에 통과시키는 행위를 하면 모든 permutation invariant 함수에 근사할 수 있다는 것이다.<br><br>

식 $(5.17)$에 있는 덧셈 $\sum_{v \in \mathcal{N}(u)}$이 permutation invariant한 연산이다. $a+b+c$ 나 $a+c+b$, $b+a+c$ 등 더하기 순서를 바꾼다고 해서 결과가 달라지지 않기 때문이다. 따라서 hidden state의 순서가 어떻게 되어 있든 $\sum_{v \in \mathcal{N}(u)}$을 통과하고 나면 모두 같은 벡터가 된다. 따라서, 식 $(5.17)$의 `AGGREGATE` 함수는 permutation invariant 하다. 식 $(5.17)$에서 덧셈 대신 element-wise 최대값이나 최소값 등 다른 permutation invariant한 연산을 사용해도 괜찮다. 그리고 식 $(5.17)$에 섹션 $5.2.1$에서 다뤘던 정규화 방법들도 추가해도 좋다.<br><br>

한편, 책에서는 `set pooling`이 무엇인지 정의가 나와있지는 않지만, 식 $(5.17)$이 `set pooling`인 것 같다. `Set pooling`을 사용할 경우 성능 향상이 조금 있지만, 반대로 오버피팅이 발생할 수도 있다. 그래서 보통 `set pooling`을 할 때는 하나의 히든 레이어를 갖는 MLP를 사용하는 것이 일반적이라고 한다.

<br>

---

### Janossy pooling
`Set pooling`에서는 각 이웃 노드의 hidden state를 MLP에 통과시켜 주었다. 그리고 출력된 벡터를 모두 더해주어 permutation invariant 성질을 만족할 수 있었다. 하지만 hidden state를 MLP에 하나씩 통과시켜 주는 것은 각 hidden state가 독립이라는 전제를 깔고 있다. 우리는 더 이상 MLP를 사용하고 싶지 않다.<br><br>

`Janossy pooling`는 MLP를 고집하지 않고 순서 변화에 민감한 함수를 사용한다. 순서 변화에 민감한 함수의 가장 대표적인 예는 LSTM이다. LSTM은 들어오는 hidden state들의 순서에 따라 결과가 달라지는 모델이다. 순서 변화에 민감한 함수를 사용하는 대신 모든 순서에 대해서 함수값을 구한다. 그리고 함수값들의 평균값을 이용하게 된다.<br><br>

$\pi \in \Pi$를 집합을 입력 받아서 순서를 갖고 있는 수열로 만들어주는 permutation 함수라고 하자. $\Pi$는 모든 permutation 함수가 살고 있는 공간이다. 예를 들어, 원소가 8개인 집합의 경우 원소를 나열하는 모든 경우의 수는 $8!$이 될 것이다. 이 경우 $\Pi$에는 $8!$개의 $\pi$가 살고 있는 것이다. 즉, $\pi$는 집합 $\\{ \mathbf{h}\_v : v \in \mathcal{N}(u) \\}$를 입력 받아 순서가 존재하는 수열 $(\mathbf{h}\_{v_1}, \mathbf{h}\_{v_2}, \cdots, \mathbf{h}\_{v\_{\mid \mathcal{N}(u)}\mid})\_\pi$를 출력해준다. `Janossy pooling`은 다음과 같은 `AGGREGATE` 함수를 사용한다.

$$\mathbf{m}_{\mathcal{N}(u)}=\text{MLP}_\theta \left(\frac{1}{\mid \Pi \mid}\sum\limits_{\pi \in \Pi}\rho_\phi((\mathbf{h}_{v_1}, \mathbf{h}_{v_2}, \cdots, \mathbf{h}_{v_{\mid \mathcal{N}(u)}\mid})_\pi) \right) \quad \quad (5.18)$$

<br>

여기서 $\rho_\phi$는 순서에 민감한 함수로서 보통 LSTM을 사용한다. 식 $(5.18)$처럼 모든 순서를 고려하여 평균낼 수 있다면 이 역시 이론적으로 모든 permutation invariant 함수에 근사할 수 있다고 한다. 하지만 큰 네트워크의 경우 모든 순서를 고려한다는 것은 불가능할 것이다. 그래서 `Janossy pooling`에서는 다음 두 가지 방법을 선택한다.

- 임의의 순서만 몇 개 샘플링해서 그것들로만 함수값을 구하고 평균을 낸다.
- `cononical`하게 정렬된 순서에 대해서만 함수값을 구하고 평균을 낸다.
    - `cononical` 정렬은 노드를 차수에 대하여 내림차순으로 정렬하는 것 같다.
    - 모든 순서를 고려하는 것이 아니라 적어도 차수가 정렬된 순서만 고려하겠다는 것이다. 차수를 기준으로 노드를 정렬할 경우 동일한 차수를 갖는 노드들에 대해서만 순서를 결정해주면 된다.
    - 예를 들어, 원소가 8개인 집합의 경우 원소를 나열하는 모든 경우의 수는 $8!$이다.  이때, 차수가 1, 2, 3, 4인 노드가 각각 2개씩 있다고 하자. 그럼, 차수를 기준으로 노드가 정렬된 경우의 수는 $2! \times 2! \times 2! \times 2!$이 된다.
    
<br>

---

## 5.2.3 Neighborhood Attention

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 어텐션의 등장 배경</p>
내가 아는 한 어텐션 개념은 자연어처리 분야에서 등장하였다. 딥러닝에서 히든 레이어 하나를 지나가면 hidden state 벡터가 업데이트된다. 어떤 데이터 $\mathbf{x}$를 딥러닝에 입력한다는 것은 데이터의 hidden state 벡터를 업데이트하는 과정으로 이해할 수 있다. 즉, 

$$\mathbf{x}\rightarrow\mathbf{h}^{(1)}\rightarrow\mathbf{h}^{(2)}\rightarrow\cdots$$

의 과정을 거치게 된다는 것이다. 여기서 $\mathbf{h}^{(k)}\in \mathbb{R}^{d_k}$는 $k$ 번째 히든 레이어를 통과했을 때의 hidden state 벡터이다. 어텐션은 이 hidden state 벡터를 계산하는 방법론 중 하나라고 생각하면 쉽다. 핵심적인 아이디어는 다음과 같다.
- 내 다음 hidden state 벡터를 계산할 때, 다른 데이터들의 hidden state 벡터들도 참고할 것이다.
- 특히, 다른 데이터들의 hidden state들의 가중합으로 내 다음 hidden state 벡터를 만들 것이다.
- 나와 비슷한 데이터에 주목 (어텐션)하여 더 큰 가중치를 줄 것이다. 


조금 더 형식을 갖춰 기술해보자. 자연어처리인 분야를 가정하여 데이터를 단어라고 생각하겠다.  주어진 $n$개의 단어들의 $k$ 번째 hidden state를 $\mathbf{h}_1^{(k)}, \mathbf{h}_2^{(k)}, \cdots,\mathbf{h}_n^{(k)}$ 라고 하자. $i$ 번 째 단어의 $(k+1)$ 번째 hidden state를 현재 hidden state들에 대한 가중합으로 계산하자. 즉,

$$\mathbf{h}_i^{(k+1)}=\sum\limits_{j=1}^n a_{i,j} \mathbf{h}_j^{(k)}$$


이때, 가중치를 어텐션 스코어 (attention score)라고 부른다. 초창기 어텐션 스코어는 단순하게 내적을 사용하여 계산되었다. 즉, 자신과 유사한 단어에 더욱 가중치를 줘서 내 상태를 업데이트한다는 말이다.

$$a_{i,j} = \mathbf{h}_i^\top \mathbf{h}_j$$

또는

$$a_{i,j} = \frac{\exp\left(\mathbf{h}_i^\top\mathbf{h}_j\right)}{\sum_{l=1}^{n}\exp\left(\mathbf{h}_i^\top\mathbf{h}_l\right)}$$


어텐션 연산에도 학습할 수 있는 파라미터를 부여하고 싶어진다. 따라서 hidden state에 가중치 행렬을 곱해준 후 attention 하는 방법으로 발전한다. 즉,

$$a_{i,j} = \frac{\exp\left(\left(\mathbf{W}^Q\mathbf{h}_i\right)^\top\mathbf{W}^K\mathbf{h}_j\right)}{\sum_{l=1}^{n}\exp\left(\left(\mathbf{W}^Q\mathbf{h}_i\right)^\top\mathbf{W}^K\mathbf{h}_l\right)}$$


여기서 $\mathbf{W}^Q$와 $\mathbf{W}^K$가 가중치 행렬이다. $Q$는 query를 $K$는 key를 의미하는데, 그냥 가중치 행렬이라는 것만 기억하자. 지금은 각각 하나의 가중치 행렬만 사용하고 있다. 그럼 자연스럽게 하나의 가중치 행렬만 쓰지 말고 여러 가중치 행렬을 사용하고 싶을 것이다. 그럼 다양한 관점에서 어텐션할 수 있을 것이다.  이것이 multi-head attention이다. 즉,

$$a_{i,j,k} = \frac{\exp\left(\left(\mathbf{W}^Q_{(k)}\mathbf{h}_i\right)^\top\mathbf{W}^K_{(k)}\mathbf{h}_j\right)}{\sum_{l=1}^{n}\exp\left(\left(\mathbf{W}^Q_{(k)}\mathbf{h}_i\right)^\top\mathbf{W}^K_{(k)}\mathbf{h}_l\right)}$$

</div>

<br>

자연어처리 분야에서 어텐션을 이용하여 성공하는 사례가 늘어나면서 GNN에도 어텐션이 도입되었다.  계속 반복해서 말하지만, 노드 $u$의 hidden state $\mathbf{h}_u$를 만들 때 이웃 노드 $v$의 hidden state $\mathbf{h}_v$들을 취합하여 만든다. 여기서 어텐션을 사용한다는 것은 $\mathbf{h}_u$와 $\mathbf{h}_v$ 사이의 어텐션 스코어를 계산하고, 그 스코어만큼 가중치를 주어 $\mathbf{h}_v$들을 합한다는 것이다. 즉,

$$\mathbf{m}_{\mathcal{N}(u)}=\sum\limits_{v \in \mathcal{N}(u)}\alpha_{u, v}\mathbf{h}_v \quad \quad (5.19)$$

<br>

여기서 $\alpha_{u, v}$는 $u$와 $v$ 사이의 어텐션 점수이다. 어텐션 점수는 $v$를 얼마나 집중해서 볼 것이냐를 나타낸다고 생각해도 좋다. 초기 어텐션 점수는 단순히 두 벡터의 내적이었다. 즉, 자신과 유사한 벡터를 더 많이 반영하여 내 벡터를 만들겠다는 취지였다. 

<br>

Graph Attention Network (GAT) 논문에서는 다음과 같은 어텐션 점수를 사용한다.

$$\alpha_{u, v}=\frac{\exp\left( \mathbf{a}^\top[\mathbf{W}\mathbf{h}_u\oplus\mathbf{W}\mathbf{h}_v]\right)}{\sum_{v' \in \mathcal{N}(u)}\exp\left( \mathbf{a}^\top[\mathbf{W}\mathbf{h}_u\oplus\mathbf{W}\mathbf{h}_{v'}]\right)} \quad \quad (5.20)$$

<br>

여기서 $\mathbf{a}$는 학습 가능한 어텐션 벡터, $\mathbf{W}$는 학습 가능한 행렬, 그리고 $\oplus$는 두 벡터를 한 줄로 쭉 연결 (concatenation)해주는 연산이다. 다음과 같은 bilinear 어텐션 스코어도 가능하다.

$$\alpha_{u, v}=\frac{\exp\left(\mathbf{h}_u^\top \mathbf{W} \mathbf{h}_v\right)}{\sum_{v' \in \mathcal{N}(u)}\exp\left( \mathbf{h}_u^\top \mathbf{W} \mathbf{h}_{v'} \right)} \quad \quad (5.21)$$

<br>

MLP를 사용하는 어텐션 스코어도 있다.

$$\alpha_{u, v}=\frac{\exp\left(\text{MLP}(\mathbf{h}_u, \mathbf{h}_v)\right)}{\sum_{v' \in \mathcal{N}(u)}\exp\left(\text{MLP}(\mathbf{h}_u, \mathbf{h}_{v'}) \right)} \quad \quad (5.22)$$

<br>

여기서 MLP는 스칼라 값을 출력해줘야 한다.

<br>

지금까지는 가중치 행렬 $\mathbf{W}$ 하나만 사용해서 노드쌍 $(u,v)$마다 하나의 어텐션 스코어를 계산해주었다. 물론, 가중치 행렬을 $K$개 사용하여 노드쌍 $(u, v)$마다 $K$개의 어텐션 스코어를 만들어 줄 수도 있다. 이런 구조를  `multi-head attention`라고 부르며, `Transformer` 논문에서 소개되었다. 즉,

$$\mathbf{m}_{\mathcal{N}(u)}=[\mathbf{a}_1 \oplus \mathbf{a}_2 \oplus \cdots \oplus \mathbf{a}_K] \quad \quad (5.23)$$

$$\mathbf{a}_k=\mathbf{W}_k\sum\limits_{v \in \mathcal{N}(u)}\alpha_{u, v, k}\mathbf{h}_v \quad \quad (5.24)$$

<br>

여기서 $\alpha_{u, v, k}$는 위에서 알아보았던 어텐션 스코어들로 구하면 된다.

> 식 $(5.24)$의 가중치 행렬 $\mathbf{W}_k$ 표기가 적절하지 않다고 생각한다. $\mathbf{W}_k$ 표기는 식 $(5.20)$ ~ 식 $(5.22)$에서 $\alpha\_{u, v, k}$를 구할 때 사용하는 것이 바람직하다고 생각한다. 식 $(5.24)$에서는 $\mathbf{U}_k$ 등 다른 표기를 사용했어야 할 것 같다.

<br>

어텐션을 이용하여 `AGGREGATE` 함수를 사용하면 GNN의 벡터 표현력 (representational power)이 증가한다고 한다. 특히, 이웃 노드 중에서도 더 유용한 노드들과 덜 유용한 노드들이 있는 경우 어텐션을 사용하는 것이 더욱 효과적이라고 한다. 한편, 어텐션을 사용할 경우 노드쌍 $(u, v)$마다 어텐션 점수를 계산해야 하기 때문에 시간 복잡도가 높다는 단점이 있다.

<br>

---

# 5.3 Generalized Update Methods
섹션 $5.2$에서는 다양한 `AGGREGATE` 방법에 대해 알아보았다. 섹션 $5.3$에서는 다양한 `UPDATE` 방법에 대해 알아본다. 그 동안 주로 `AGGREGATE` 방법에 대한 연구들이 주목 받은 것은 사실이다. 하지만 `UPDATE` 방법도 GNN 모델의 성능을 결정하는 중요한 요소이기 때문에 잘 알아두어야 한다. 본격적으로 `UPDATE` 방법을 다루기 전에 GNN 모델들이 겪고 있는 고질병인 `Over-smoothing`에 대해 알아보자.

---

## 5.3.0 Over-smoothing
`Over-smoothing` 문제는 노드들의 hidden state 벡터가 GNN 레이어를 통과하면서 점점 비슷해지는 현상을 말한다. 이 문제는 특히 GNN 기초 모델 (식 $5.7$)과 self-loop를 사용하는 GNN 모델 (식 $5.13$)에서 더 자주 발생한다고 한다. GNN 레이어를 많이 통과할수록 노드들의 hidden state 벡터가 비슷해지기 때문에 `Over-smoothing` 문제는 더 깊은 GNN 모델을 사용할 수 없게 만든다. 따라서 그래프 안에서 더 넓은 구조적인 정보를 활용할 수 없게 된다.<br><br>

섹션 $5.3.1$부터는 `over-smoothing`을 완화할 수 있는 방법들에 대해 알아본다. 섹션 $5.3.0$의 남은 부분은 `over-smoothing`이 발생하는 이유를 설명하고자 한다. 이론적인 설명이 짙은 반면 중요도는 높지 않다고 생각하기 때문에 바로 섹션 $5.3.1$으로 넘어가도 괜찮다. 앞으로의 설명이 보이고 싶은 목적지는 다음과 같다.

- 모든 노드가 어떤 한 노드 $v$의 마지막 hidden state 벡터  $\mathbf{h}^{(K)}_v$에 미치는 영향이 서로 비슷하다.
- 모든 노드 $v$에 대해서 위가 성립하여, 모든 노드의 hidden state 벡터가 서로 유사하다.

<br>

먼저, 노드 $u$의 초기 특징 벡터 $\mathbf{h}^{(0)}_u=\mathbf{x}_u$가 노드 $v$의 $K$ 번째 hidden state 벡터 $\mathbf{h}^{(K)}_v$에 미치는 영향력을 측정해야 한다. 직관적으로 $\mathbf{x}_u$가 아주 조금 변했을 때 $\mathbf{h}_v^{(K)}$가 많이 변한다면 $\mathbf{x}_u$의 영향력이 크다는 것을 알 수 있다. 따라서 $\mathbf{x}_u$가  $\mathbf{h}_v^{(K)}$에 미치는 영향력을 다음과 같이 정의한다.

$$I_K(u,v)=\mathbf{1}^{\top}\left( \frac{\partial \mathbf{h}_v^{(K)}}{\partial\mathbf{h}_u^{(0)}} \right) \mathbf{1} \quad \quad (5.25)$$

<br>

$\frac{\partial \mathbf{h}_v^{(K)}}{\partial\mathbf{h}_u^{(0)}}$을 `Jacobian 행렬`이라고 부른다. $\mathbf{h}_v^{(K)}$의 각 원소들을 $\mathbf{h}_u^{(0)}$의 각 원소들로 미분한 값을 저장해놓은 행렬이라고 생각하면 된다. $\mathbf{1}$은 모든 원소가 1인 벡터이다. 식 $(5.25)$처럼 행렬의 양 옆에  $\mathbf{1}^\top$와 $\mathbf{1}$을 곱해주면 행렬을 모든 원소를 더하게 된다. 요컨데 `Jacobian 행렬`의 모든 원소를 더한 것을 "노드 $u$의 초기 특징 벡터가 노드 $v$의 최종 hidden state 벡터에 미치는 영향력"으로 사용하는 것이다.<br><br>

그리고 다음 `AGGREGATE` 함수를 사용하는 GNN 모델을 생각해보자. 

$$\text{AGGREGATE}(\{ \mathbf{h}_v, \forall v \in \mathcal{N}(u) \cup \{ u \}\})=\frac{1}{f_n(\mid \mathcal{N}(u) \cup \{ u \}\mid)}\sum\limits_{v \in \mathcal{N}(u) \cup \{ u \}}\mathbf{h}_v, \quad \quad (5.26)$$

<br>

여기서 $f:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}$는 임의의 미분가능한 정규화 함수이다. 식이 많이 복잡하지만 쉽게 self-loop를 포함하고 있는 간단한 GNN 모델으로 생각해도 좋다. 식 $(5.26)$을 만족하는 GNN 모델은 다음을 만족한다고 한다.

$$I_K(u,v) \propto p_{\mathcal{G}, K}(v|u), \quad \quad (5.27)$$

<br>

여기서 $p\_{\mathcal{G}, K}(v \mid u)$는 노드 $u$에서 시작하여 길이 $K$의 랜덤 워크를 했을 때 노드 $v$를 방문할 확률이다. 쉽게 말해서 self-loop를 갖고 있는 간단한 GNN 모델에 대해서 식 $I_K(u,v)$는 노드 $u$에서 시작한 길이 $K$의 랜덤 워크에서 노드 $v$를 방문할 확률에 비례한다는 것이다.<br><br>

여기까지는 큰 문제가 없어 보인다. 하지만 GNN 레이어가 많아질 수록, 즉 $K$가 점점 커질 수록 $p\_{\mathcal{G}, K}(v \mid u)$가 랜덤 워크의 `stationary distribution`으로 수렴한다는 점이 문제이다. `Stationary distribution`은 그래프에서 랜덤 워킹을 무한 번 했을 때 각 노드에 방문할 확률 분포이다. 이는 랜덤 워크를 시작한 노드와 상관 없이 일정하다. 이는 다시 말하면 깊은 GNN을 통과하게 되면 노드 $u$가 노드 $v$의 얼마나 가까운지는 상관 없이 $u$의 `stationary distribution` 확률값만큼 영향을 미친다는 것이다. 즉, local neighborhood 정보를 잃게 된다.<br><br>

여기에 더해서 어떤 네트워크에 차수가 매우 높은 노드를 포함하고 있으면 `stationary distribution`이 거의 균등 (almost-uniform) 분포로 수렴한다는 것이 증명되어 있다. 위의 문단과 현재 문단을 합쳐보자. 

- 깊은 GNN의 경우
- $\mathbf{x}_u$가 $\mathbf{h}_v^{(K)}$에 `stationary distribution`의 확률값만큼 영향력을 미친다.
- 그런데 그 영향력은 모든 노드가 균등한 값을 갖는다.

<br>

즉, 그래프 안의 각 노드에 대한 영향력이 모든 노드가 같다는 것이다. 굉장히 어지러운데, 모든 노드가 모든 노드에 대해 미치는 영향력이 같다는 것이다. 따라서 레이어가 지나갈 수록 hidden state 벡터가 점점 유사해지는 것이다.<br><br>

Self-loop를 포함하는 간단한 GNN 모델에 대해 내용을 전개하였다. 하지만 self-loop를 포함하지 않더라도 식 $(5.9)$에서 $\lVert \mathbf{W}^{(k)}\_{\text{self}} \rVert \le \lVert \mathbf{W}^{(k)}\_{\text{neigh}} \rVert$인 간단한 GNN 모델에 대해서도 해당 내용들이 확장될 수 있다고 한다.<br><br>  

요약하건데, GNN 레이어를 지나갈 수록 local neighborhood 정보를 점점 잃게되며 노드들의 hidden state 벡터들이 점점 비슷해지는 현상을 `over-smoothing`이라고 한다.

---

## 5.3.1 Concatenation and Skip Connections
**Coming soon!**

## 참고문헌
<p id="ref1">[1] <a href="https://pixabay.com/ko/illustrations/%ec%a7%80%ea%b5%ac-%ed%9a%8c%eb%a1%9c%eb%a7%9d-3537401/" target="_blank">https://pixabay.com/ko/illustrations/지구-회로망-3537401/</a></p>
<p id="ref2">[2] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref3">[3] <a href="http://web.stanford.edu/class/cs224w/" target="_blank">CS224W: Machine Learning with Graphs</a></p>
<p id="kipf">[4] T.N. Kipf and M. Welling. Semi-supervised classification with graph convolutional
networks. In ICLR, 2016a.</p>
