---
layout: post
title:  "[GRL Book 정리] Chapter 5. The Graph Neural Network Model"
date:   2021-9-3 12:00
categories: [Others]
use_math: true
comments: true
---

![intro](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/earth-network.jpg){: .center}
<center>사진 출처: <a href="#ref1">[1]</a></center>

# <center>Chapter 5. The Graph Neural Network Model</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 네 번째 이야기. 부디 완주하게 기도해주세요 !

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

**왜 Chapter 3 다음에 Chapter 5인가요? Chapter 4는 어디다 팔아드셨나요?**

우리나라에서는 유독 숫자 4를 사용하는걸 꺼려한다. 한자 死 (죽을 사)와 발음이 같아서 그렇다. 옛날 건물을 보면 4층이 없고 바로 3층에서 5층으로 넘어간다. 이와 같은 맥락으로 Chapter 4를 스킵하고 Chapter 5로 넘어가려고 한다.

는 장난이고, 지금 참여하고 있는 스터디에서 CS224W와 GRL Book을 병행하고 있다. 메인이 CS224W이기 때문에 이와 진도와 맞추기 위하여  Chapter  4는 Chapter 7을 공부한 후에 다루는 것으로 계획되어 있다. 그 때까지 내가 포기하지 않고 포스팅을 하고 있다면, Chapter 4 정리를 볼 수 있을 것이다 ... !!

</div>

<br>

---

# 5.0 Gentle Introduction
Chapter 3까지 배웠던 노드 임베딩 기법들은 각 노드의 속성값 (attribute 또는 feature)을 사용하지 않는다는 한계점이 있었다. 이번 챕터에서는 그래프의 구조 뿐만 아니라 노드의 속성값도 고려하여 노드 임베딩 벡터를 만들 수 있는 그래프 신경망 (Graph Neural Network)에 대해서 알아본다.

<br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! </p>

노드마다 부여된 정보를 속성값 (attribute) 또는 속성 벡터라고 부른다. 소셜 네트워크에서 사람 한 명이 노드를 나타낸다고 했을 때, 그 사람의 성별, 나이, 지역 등은 노드의 속성값들이다. 한편, 속성 벡터를 포함하여 노드를 나타내는 벡터들을 포괄적으로 feature 벡터라고 부른다. 본 포스팅에서 feature 벡터라는 표현만 사용하도록 하겠다. 

</div>

<br>

지금까지 우리는 그래프 데이터를 머신러닝 모델에 입력하기 위해 많은 노력을 해왔다. 지금부터는 딥러닝에 그래프 데이터를 입력할 수 있는 방법들에 대해 알아본다. 다층 퍼셉트론 (Multilayer perceptron, MLP)은 정해진 크기의 벡터를 입력 받는다. MLP에 그래프 $\mathcal{G}=(\mathcal{V}, \mathcal{E})$를 입력할 수 있는 가장 간단한 방법은 인접행렬 $\mathbf{A}$를 쭉 펼쳐서 벡터로 만드는 것이다. 즉,

$$\mathbf{z}_{\mathcal{G}}=\text{MLP}(\mathbf{A}[1] \oplus \mathbf{A}[2] \oplus \cdots \oplus \mathbf{A}[| \mathcal{V}|]), \quad \quad (5.1)$$

<br>

이때, $\mathbf{A}[u] \in \mathbb{R}^{\mid \mathcal{V} \mid}$는 인접행렬의 $u$번째 행이고, $\oplus$는 벡터를 한 줄로 쭉 연결 (concatenate)하는 연산이다. $\mathbf{z}_{\mathcal{G}}$는 그래프 $\mathcal{G}$를 MLP에 통과시켜서 얻은 결과물이다. <br><br>

여기서 가장 큰 문제점은 노드에 어떤 순서를 부여하여 인접행렬을 만들었다는 것이다. 노드의 순서를 뒤섞으면 인접행렬도 뒤섞인 순서에 맞게 바뀌게 된다. 하지만 인접행렬이 나타내는 그래프는 여전히 동일하다. 같은 그래프를 나타내지만 노드 순서만 바뀐 인접행렬을 상상해보자. 이 인접행렬을 MLP에 통과시킬 경우 원래와 다른 결과를 출력해줄 것이다.  그래서 식 $(5.1)$과 같은 전략은 그래프를 상대로는 적합하지 않다.<br><br>

우리는 노드 순서가 뒤바뀌어도 여전히 같은 결과를 출력해주는 함수를 원한다. 아니면 적어도 결과물의 순서를 뒤바꿔서 원래의 결과물로 만들어 줄 수 있는 함수를 원한다. 첫 번째 같은 함수를 `permutation invariant` 함수라고 부르고 두 번째 함수는 `permutation equivariant` 함수라고 부른다.<br><br>

행렬 $\mathbf{P}$를 permutation 행렬이라고 하자. 어떤 행렬 $\mathbf{A}$에 permutation 행렬을 왼쪽에 곱하면 $\mathbf{P}\mathbf{A}$는 $\mathbf{A}$에서 행의 순서만 바뀐 행렬이 된다. 반대로 오른쪽에 곱하면 $\mathbf{A}$에서 열의 순서만 바뀐 행렬이 된다.<br><br>

한편, 그래프의 경우 노드의 순서가 바뀌게 되면 인접행렬의 행과 열이 모두 바뀌게 된다. 따라서 인접행렬 $\mathbf{A}$에서 노드 순서가 바뀔 경우 새로운 인접행렬은 $\mathbf{P}\mathbf{A}\mathbf{P}^\top$가 된다. 그래프에 대해서 `permutation invariant`한 함수 $f$는 다음의 성질을 만족시킨다. (이때, 함수 $f:\mathbb{R}^{\mid \mathcal{V} \mid \times \mid \mathcal{V} \mid}\rightarrow \mathbb{R}^{\mid \mathcal{V} \mid}$는 인접행렬을 입력 받아서 노드마다 어떤 값을 주는 벡터를 출력해주는 함수이다.)

$$f(\mathbf{P} \mathbf{A} \mathbf{P}^\top)=f(\mathbf{A}) \quad \quad (\text{Permutation Invariance}) \quad \quad (5.2)$$

<br>

즉, 노드 순서가 뒤바뀐 인접행렬을 넣었을 때 함수값이 그냥 인접행렬을 넣었을 때의 함수값과 똑같다는 것이다. 한편, `permutation equivariant`한 함수 $f$는 다음의 성질을 만족시킨다.

$$f(\mathbf{P} \mathbf{A} \mathbf{P}^\top)=\mathbf{P}f(\mathbf{A}) \quad \quad (\text{Permutation Equivariant}) \quad \quad (5.3)$$

<br>

즉, 노드 순서를 바꿨던대로 $f(\mathbf{A})$의 원소의 순서를 바꿔준 것과 $f(\mathbf{P}\mathbf{A}\mathbf{P}^\top)$이 같다는 것이다. 정리하자면, 우리는 `permutation invariant`한 함수나 아니면 적어도 `permutation equivariant`한 성질을 같는 딥러닝 모델을 고려해야 한다.

---

# 5.1 Neural Message Passing
지금까지 등장한 GNN 모델들은 정말 다양한 동기로부터 만들어졌다. 하지만 그들 모두가 공유하고 있는 한 가지 구조가 있다. 바로 신경망 메세지 전달 (Neural Message Passing) 구조이다.  신경망 메세지 전달이란 뉴럴 네트워크를 통하여 노드끼리  메세지 벡터를 교환하는 것을 말한다.

<br>

## 5.1.1 Overview of the Message Passing Framework
노드 $u$의 $k$ 번째 hidden state 벡터를 $\mathbf{h}^{(k)}_u$라고 하자. $k+1$ 번째 hidden state $\mathbf{h}^{(k+1)}_u$는 노드 $u$의 이웃 노드로부터 전달 받은 정보들을 취합 (`AGGREGATE`)하고 업데이트 (`UPDATE`)하여 만들어진다. 즉,

$$\begin{matrix}
\mathbf{h}^{(k+1)}_u 
&=& \text{UPDATE}^{(k)} \;(\mathbf{h}^{(k)}_u, \text{AGGREGATE}^{(k)}(\{ \mathbf{h}^{(k)}_v:v \in \mathcal{N}(u) \}) & (5.4) \\
& = & \text{UPDATE}^{(k)} \;(\mathbf{h}^{(k)}_u, \mathbf{m}^{(k)}_{\mathcal{N}(u)}) & (5.5)
\end{matrix}$$

<br>

이때, `AGGREGATE`와 `UPDATE`는 임의의 미분 가능한 함수이다. `AGGREGATE` 함수와 `UPDATE` 함수를 어떻게 선택하느냐에 따라서 GNN 알고리즘이 다양하게 구분될 수 있다. 두 함수에 대한 다양한 예시를 이번 장에서 마주치게 될 것이다. $\mathbf{m}\_{\mathcal{N}(u)}$은 노드 $u$의 이웃 노드의 정보를 취합하여 만든 메세지 벡터이다.<br><br>

우리는 먼저 $k=0$일 때 각 노드 $u$의 임베딩 벡터를 $u$의 feature 벡터로 설정한다. 즉, $\mathbf{h}^{(0)}_u=\mathbf{x}_u, \; \forall u \in V$ 이다. 다음으로 다음의 신경망 메세지 전달을 반복한다. 각 $k$번 째 레이어 (또는 반복이라고도 말함)에서 하는 일은 다음과 같다.

- `AGGREGATE` 함수는 $u$의 이웃 노드 $\mathcal{N}(u)$들의 hidden state **집합**을 입력 받아서 메세지 벡터 $\mathbf{m}^{(k)}\_{\mathcal{N}(u)}$를 만든다.
- `UPDATE` 함수는 메세지 벡터 $\mathbf{m}^{(k)}\_{\mathcal{N}(u)}$와 노드 $u$의 $k$ 번째 hidden state $\mathbf{h}^{(k)}_u$를 입력 받아서 hidden state를 업데이트한다.

<br>

위의 설명에서 한 가지 주목해야 할 점은 `AGGREGATE` 함수가 $\mathcal{N}(u)$들의 hidden state **집합**을 입력 받는다는 것이다. 한 집합 안에서 원소들의 순서가 뒤섞여도 여전히 같은 집합이다. 따라서 `AGGREGATE` 함수는 입력 받는 hidden state들의 순서에 상관 없이 일정한 결과를 계산해줘야 한다 (permutation invariance). 또는 최소한 계산된 결과를 다시 뒤섞어서 순서를 맞춰줄 수 있어야 한다(permutation equivariance).<br><br>

$K$번의 메세지 반복을 마치고 얻은 각 노드 $u$의 임베딩 벡터 $\mathbf{h}^{(K)}_u$를 노드 $u$의 임베딩 벡터로 사용하게 된다. 즉, $\mathbf{z}_u=\mathbf{h}^{(K)}_u, \; \forall u \in V$이다.<br><br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 노드의 속성 벡터가 없으면 어떡하나요?</p>

만약 노드 속성 벡터 (attribute vector)가 없다면, 이전 챕터들에서 배운 방법들을 사용하여 feature 벡터들을 메뉴얼하게 만들어줘야 한다.

만약, 주어진 그래프에서 노드 임베딩만하는 것이 목적이라면 identity feature를 사용해도 된다. 노드 $u$의 identity feature $\mathbf{e}_u \in \mathbb{R}^{\mid V \mid}$는 $u$번 째 원소만 1이고 나머지 원소는 0인 원핫 벡터이다. 하지만 이렇게 만든 GNN 모델의 경우 새로 유입되는 노드를 임베딩해줄 수 없는 transductive 성질을 갖게 된다.

</div>

---

## 5.1.2 Motivations and Intuitions
GNN에서 메세지 전달을 한 번 진행하면 한 노드가 이웃 노드들의 정보를 반영하여 벡터를 만들게 된다. 메세지 전달을 두 번 진행하면 한 노드에 대하여 이웃 노드의 이웃 노드의 정보까지 반영하여 임베딩 벡터가 만들어지게 된다. 이렇게 메세지 전달이 많이 반복될 수록 한 노드에 대하여 더 멀리 떨어진 이웃 노드의 정보를 반영하여 벡터를 만들 수 있게 된다.<br><br>

그럼 어떤 정보를 반영하는 것일까? 노드 $u$의 $k$ 번째 hidden state $\mathbf{h}^{(k)}_u$는 크게 두 가지 정보를 반영하게 된다. 

- 그래프 안의 구조적인 정보. $\mathbf{h}^{(k)}_u$는 노드 $u$의 $k$-hop 이웃 노드 정보를 반영하고 있다.
- feature 기반 정보: $\mathbf{h}^{(k)}_u$은 노드 $u$의 $k$-hop 이웃 노드의 feature 벡터 정보를 반영하고 있다.

<br>

이웃한 데이터로부터 feature 벡터를 취합한다는 점에서 합성곱 신경망 (CNN)과 유사하다고 볼 수 있다. CNN은 정해진 크기의 패치 안의 feature 정보를 취합 (합성곱)한다는 점과 GNN은 이웃 노드의 정보를 취합한다는 점에서 차이가 있다.

---

## The Basic GNN
이번 장에서는 가장 간단한 `AGGREGATE` 함수와 `UPDATE` 함수를 사용하여 기초적인 GNN 모델을 만들어 볼 예정이다.  다음과 같이 $k$ 번째 레이어에서의 GNN 메세지 전달식을 생각해보자. (앞으로 이 모델을 GNN 기초 모델이라고 부르겠다.)

$$\mathbf{h}^{(k)}_u=\sigma\left(\mathbf{W}^{(k)}_{\text{self}}\mathbf{h}^{(k-1)}_u+\mathbf{W}^{(k)}_{\text{neigh}}\sum\limits_{v \in \mathcal{N}(v)}\mathbf{h}^{(k-1)}_v+\mathbf{b}^{(k)}\right), \quad \quad (5.7)$$

<br>

이때, $\mathbf{W}^{(k)}\_{\text{self}}, \mathbf{W}^{(k)}\_{\text{neigh}} \in \mathbb{R}^{d^{(k)} \times d^{(k-1)}}$는 학습 가능한 파라미터로 이루어진 행렬이다. $d^{(k-1)}$차원 hidden state 벡터들을 $d^{(k)}$차원 벡터로 변환해주는 역할을 한다. $\sigma$ 함수는 벡터를 입력 받아 원소마다 비선형함수를 적용하는 함수이다. 대표적으로 `tanh`나 `ReLU`이 있을 것이다. $\mathbf{b}^{(k)} \in \mathbb{R}^{d^{(k)}}$는 편향 벡터 (bias term)로 학습 가능한 파라미터 벡터이다. 편향 벡터는 표기의 편의를 위해 생략해주기도 하지만, 실제 구현에서는 큰 역할을 한다.
- 파라미터 행렬과 편향 벡터를 $k$에 따라 구분지었지만, 매 $k$마다 같은 파라미터 행렬과 편향 벡터를 공유하여 사용해주기도 한다.

<br>

식 $(5.7)$에서 이웃 노드들의 hidden state들을 단순 합해주는 것이 바로 `AGGREGATE` 함수이다. 즉,

$$\mathbf{m}^{(k)}_{\mathcal{N}(u)}=\text{AGGREGATE}^{(k)}\left(\{ \mathbf{h}^{(k-1)}_v:v \in \mathcal{N}(u) \}\right)=\sum\limits_{v \in \mathcal{N}(u)}\mathbf{h}^{(k-1)}_v, \quad \quad (5.8)$$

<br>

한편, `UPDATE` 함수는 노드의 이전 hidden state 벡터와 메세지 벡터를 각각 변환해주고 편향 벡터와 함께 더해주어 비선형 함수를 적용시키는 함수이다. 즉,

$$\text{UPDATE}^{(k)} \;(\mathbf{h}^{(k)}_u, \mathbf{m}^{(k)}_{\mathcal{N}(u)})=\sigma\left(\mathbf{W}^{(k)}_{\text{self}}\mathbf{h}^{(k-1)}_{u}+\mathbf{W}^{(k)}_{\text{neigh}}\mathbf{m}^{(k)}_{}+\mathbf{b}^{(k)}\right), \quad \quad (5.9)$$

<br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 업데이트식 행렬 버전</p>

참고사항으로 식 $(5.7)$은 노드의 hidden state 단위로 메세지 전달식을 나타낸 것이다. 한편, hidden state들을 한 행렬에 묶어서 메세지 전달식을 나타낼 수도 있다. 두 표기법 모두 잘 사용되지만 후반부 어텐션 (attention) 설명을 위해서는 hidden state 단위로 메세지 전달식을 표기하는 것이 더욱 좋다.

$$\mathbf{H}^{(k)}=\sigma\left(\mathbf{H}^{(k-1)}\mathbf{W}^{(k)}_{\text{self}}+\mathbf{A}\mathbf{H}^{(k-1)}\mathbf{W}^{(k)}_{\text{neigh}}\right), \quad \quad (5.11)$$

<br>

이때, $\mathbf{H}^{(k)} \in \mathbb{R}^{\mid V \mid \times d^{(k)}}$은 $k$ 번째 hidden state 벡터를 행벡터로 갖는 행렬이다.  $\mathbf{A}$는 인접행렬이다. 편향 벡터는 생략해주었지만 괄호 안에서 행렬의 행벡터마다 편향 벡터를 더해주면 된다. 
- 그리고 개인적인 생각인데, 식 $(5.7)$에 정의된 파라미터 행렬이라면 전치 행렬을 곱해주는 것이 맞는 것 같다. 즉, $\mathbf{H}^{(k-1)}(\mathbf{W}^{(k)}\_{\text{self}})^\top+\mathbf{A}\mathbf{H}^{(k-1)}(\mathbf{W}^{(k)}\_{\text{neigh}})^\top )$

</div>

<br>

---

## 5.1.4 Message Passing with Self-loops
GNN 모델을 조금 더 간략하게 만들기 위한 방법으로 노드마다 자기 자신으로 가는 셀프 루프를 추가해주는 방법이 있다. 셀프 루프를 추가하면 자신의 이웃 노드에 자신도 포함되게 된다. 따라서 자신의 hidden state 벡터를 `AGGREGATE` 함수에서 사용할 수 있게 된다. 그렇게 되면 자신의 hidden state 벡터와 메세지 벡터를 합쳐주는 `UPDATE` 함수를 생략해도 된다. 즉,

$$\mathbf{h}^{(k)}_u=\text{AGGREGATE}\left(\{ \mathbf{h}^{(k-1)}_v:v \in\mathcal{N}(u) \cup \{u\}\}\right), \quad \quad (5.12)$$

<br>

`AGGREGATE` 함수가 $\mathcal{N}(u) \cup \{u\}$을 대상으로 계산을 하겠다는 것이다.  물론, 자기 자신의 정보와 이웃 노드의 정보에 각각 다른 연산을 취해주는 원래 버전보다는 모델의 표현력 (expressivity)이 감소한다는 단점이 있지만, 반대로 말하면 오버피팅을 방지해줄 수 있다는 것이기도 하다. 셀프 루프를 추가해서 GNN을 수행하는 기법들을 앞으로 셀프 루프 GNN (`self-loop GNN`)으로 부를 것이다.<br><br>

예를 들어 식 $(5.7)$에서 셀프 루프를 추가한다는 것은 $\mathbf{W}\_{\text{self}}$와 $\mathbf{W}\_{\text{neigh}}$을 서로 같은 파라미터 행렬로 사용하는 것과 동일한 것으로 볼 수 있다 (노드의 hidden state 벡터와 메세지 벡터에 정확히 동일한 연산을 취해주기 때문이다.).  이를 그래프 단위 메세지 전달식으로 나타내면 다음과 같다.

$$\mathbf{H}^{(k)}=\sigma\left((\mathbf{I}+\mathbf{A})\mathbf{H}^{(k-1)}\mathbf{W}^{(k)}\right). \quad \quad (5.13)$$

---

# 5.2 Generalized Neighborhood Aggregation
식 $(5.7)$은 기초적인 GNN 모델이지만 꽤 좋은 성능을 보일 뿐만 아니라 이론적으로도 연구가 잘 되어 있다. 하지만 우린 아직 목마르다. 이번 섹션에서는 다양한 `AGGREGATE` 함수에 대해 알아볼 것이다.

---

## 5.2.1 Neighborhood Normalization

GNN 기초 모델에서 사용한 `AGGREGATE` 함수 (식 $5.8$)는 이웃 노드들의 이전 hidden state들을 모두 더해준 것이였다. 그럼 이웃이 많은 노드의 hidden state 벡터는 자연스럽게 크기가 커질 것이다. 이는 GNN 모델을 학습시킬 때 수렴을 저해할 수 있다. 만약 노드 $u$의 차수가 노드 $u'$보다 100배 더 많다고 생각해보자. 그럼 $\lVert \sum\_{v\in\mathcal{N}(u)}\mathbf{h}_v \rVert >> \lVert \sum\_{v'\in\mathcal{N}(u')}\mathbf{h}\_{v'} \rVert$일 것이다. 벡터의 크기가 큰 영역은 다른 영역보다 그레디언트의 스케일이 크기 때문에 안정적인 최적화를 저해할 수 있다.<br><br>

생각해볼 수 있는 가장 쉬운 해결 방법은 식 $(5.8)$을 노드 차수로 나눠주는 것이다. 즉,

$$\mathbf{m}_{\mathcal{N}(u)}=\frac{\sum_{v \in \mathcal{N}(u)}\mathbf{h}_v}{\mid \mathcal{N}(u) \mid} \quad \quad (5.14)$$

<br>

다른 해결 방법은 Kipf and Welling, 2016a <a href="#kipf">[4]</a>에서 제안했던 `symmetric normalization` 을 사용하는 것이다. 즉,

$$\mathbf{m}_{\mathcal{N}(u)}=\sum\limits_{v \in \mathcal{N}(u)}\frac{\mathbf{h}_v}{\sqrt{\mid \mathcal{N}(u) \mid \mid \mathcal{N}(v) \mid}} \quad \quad (5.15)$$

<br>

책에서는 `symmetric normalization`과 GNN 기초 모델의 `UPDATE` 함수 (식 $5.9$)를 합친 것이 `spectral graph convolution`에 대한 1차 근사치라는 것도 언급하고 있다.<br><br>

어떤 문제에서는 차수가 높은 노드가 의외로 유용하지 않을 수 있다. 예를 들어, 논문 인용 네트워크 안에서 논문들을 군집화하는 문제를 생각해보자. 저명한 논문은 분야를 가리지 않고 인용이 많이 되기 때문에 군집화할 때 썩 유용한 정보는 아닐 것이다. 이런 관점에서는 정규화를 하는 것이 타당해보인다.<br><br> 

하지만, 그렇다고 정규화가 무조건 좋은 것은 아니다. 정규화를 하면 정보 손실이 발생한다. 정규화를 포함한 GNN으로 노드 임베딩을 한다고 생각해보자. 그럼 노드들의 차수와 관련된 특징들이 무시된채로 임베딩 벡터가 만들어질 수 있다. 사실, GNN 기초 모델에 식 $(5.14)$의 정규화를 적용하는 것보다 그냥 식 $(5.8)$의 `AGGRAGATE`를 사용하는 것이 더 좋다고 한다. 이와 관련된 이론적인 내용들은 Chapter $7$에서 다뤄볼 예정이라고 한다.<br><br>

그럼 도대체가 정규화를 하라는건가 말라는건가. 정답이 딱 정해져 있지는 않지만, 그래프의 구조적인 특징들보다 노드의 속성 벡터가 더 중요한 경우 정규화가 유용하다고 한다.  또는 노드 차수 때문에 모델 학습이 잘 안 될 경우도 정규화를 하는게 좋다고 한다.

<br>

---

## 5.2.2 Set Aggregators
**Coming soon!**

## 참고문헌
<p id="ref1">[1] <a href="https://pixabay.com/ko/illustrations/%ec%a7%80%ea%b5%ac-%ed%9a%8c%eb%a1%9c%eb%a7%9d-3537401/" target="_blank">https://pixabay.com/ko/illustrations/지구-회로망-3537401/</a></p>
<p id="ref2">[2] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref3">[3] <a href="http://web.stanford.edu/class/cs224w/" target="_blank">CS224W: Machine Learning with Graphs</a></p>
<p id="kipf">[4] T.N. Kipf and M. Welling. Semi-supervised classification with graph convolutional
networks. In ICLR, 2016a.</p>
