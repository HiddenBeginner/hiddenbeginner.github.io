---
layout: post
title:  "[GRL Book 정리] Chapter 5. The Graph Neural Network Model"
date:   2021-9-3 12:00
categories: [Others]
use_math: true
comments: true
---

![intro](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/earth-network.jpg){: .center}
<center>사진 출처: <a href="#ref1">[1]</a></center>

# <center>Chapter 5. The Graph Neural Network Model</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 네 번째 이야기. 부디 완주하게 기도해주세요 !

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

**왜 Chapter 3 다음에 Chapter 5인가요? Chapter 4는 어디다 팔아드셨나요?**

우리나라에서는 유독 숫자 4를 사용하는걸 꺼려한다. 한자 死 (죽을 사)와 발음이 같아서 그렇다. 옛날 건물을 보면 4층이 없고 바로 3층에서 5층으로 넘어간다. 이와 같은 맥락으로 Chapter 4를 스킵하고 Chapter 5로 넘어가려고 한다.

는 장난이고, 지금 참여하고 있는 스터디에서 CS224W와 GRL Book을 병행하고 있다. 메인이 CS224W이기 때문에 이와 진도와 맞추기 위하여  Chapter  4는 Chapter 7을 공부한 후에 다루는 것으로 계획되어 있다. 그 때까지 내가 포기하지 않고 포스팅을 하고 있다면, Chapter 4 정리를 볼 수 있을 것이다 ... !!

</div>

<br>

---

# 5.0 Gentle Introduction
Chapter 3까지 배웠던 노드 임베딩 기법들은 각 노드의 속성값 (attribute 또는 feature)을 사용하지 않는다는 한계점이 있었다. 이번 챕터에서는 그래프의 구조 뿐만 아니라 노드의 속성값도 고려하여 노드 임베딩 벡터를 만들 수 있는 그래프 신경망 (Graph Neural Network)에 대해서 알아본다.

<br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! </p>

노드마다 부여된 정보를 속성값 (attribute) 또는 속성 벡터라고 부른다. 소셜 네트워크에서 사람 한 명이 노드를 나타낸다고 했을 때, 그 사람의 성별, 나이, 지역 등은 노드의 속성값들이다. 한편, 속성 벡터를 포함하여 노드를 나타내는 벡터들을 포괄적으로 feature 벡터라고 부른다. 본 포스팅에서 feature 벡터라는 표현만 사용하도록 하겠다. 

</div>

<br>

지금까지 우리는 그래프 데이터를 머신러닝 모델에 입력하기 위해 많은 노력을 해왔다. 지금부터는 딥러닝에 그래프 데이터를 입력할 수 있는 방법들에 대해 알아본다. 다층 퍼셉트론 (Multilayer perceptron, MLP)은 정해진 크기의 벡터를 입력 받는다. MLP에 그래프 $\mathcal{G}=(\mathcal{V}, \mathcal{E})$를 입력할 수 있는 가장 간단한 방법은 인접행렬 $\mathbf{A}$를 쭉 펼쳐서 벡터로 만드는 것이다. 즉,

$$\mathbf{z}_{\mathcal{G}}=\text{MLP}(\mathbf{A}[1] \oplus \mathbf{A}[2] \oplus \cdots \oplus \mathbf{A}[| \mathcal{V}|]), \quad \quad (5.1)$$

<br>

이때, $\mathbf{A}[u] \in \mathbb{R}^{\mid \mathcal{V} \mid}$는 인접행렬의 $u$번째 행이고, $\oplus$는 벡터를 한 줄로 쭉 연결 (concatenate)하는 연산이다. $\mathbf{z}_{\mathcal{G}}$는 그래프 $\mathcal{G}$를 MLP에 통과시켜서 얻은 결과물이다. <br><br>

여기서 가장 큰 문제점은 노드에 어떤 순서를 부여하여 인접행렬을 만들었다는 것이다. 노드의 순서를 뒤섞으면 인접행렬도 뒤섞인 순서에 맞게 바뀌게 된다. 하지만 인접행렬이 나타내는 그래프는 여전히 동일하다. 같은 그래프를 나타내지만 노드 순서만 바뀐 인접행렬을 상상해보자. 이 인접행렬을 MLP에 통과시킬 경우 원래와 다른 결과를 출력해줄 것이다.  그래서 식 $(5.1)$과 같은 전략은 그래프를 상대로는 적합하지 않다.<br><br>

우리는 노드 순서가 뒤바뀌어도 여전히 같은 결과를 출력해주는 함수를 원한다. 아니면 적어도 결과물의 순서를 뒤바꿔서 원래의 결과물로 만들어 줄 수 있는 함수를 원한다. 첫 번째 같은 함수를 `permutation invariant` 함수라고 부르고 두 번째 함수는 `permutation equivariant` 함수라고 부른다.<br><br>

행렬 $\mathbf{P}$를 permutation 행렬이라고 하자. 어떤 행렬 $\mathbf{A}$에 permutation 행렬을 왼쪽에 곱하면 $\mathbf{P}\mathbf{A}$는 $\mathbf{A}$에서 행의 순서만 바뀐 행렬이 된다. 반대로 오른쪽에 곱하면 $\mathbf{A}$에서 열의 순서만 바뀐 행렬이 된다.<br><br>

한편, 그래프의 경우 노드의 순서가 바뀌게 되면 인접행렬의 행과 열이 모두 바뀌게 된다. 따라서 인접행렬 $\mathbf{A}$에서 노드 순서가 바뀔 경우 새로운 인접행렬은 $\mathbf{P}\mathbf{A}\mathbf{P}^\top$가 된다. 그래프에 대해서 `permutation invariant`한 함수 $f$는 다음의 성질을 만족시킨다. (이때, 함수 $f:\mathbb{R}^{\mid \mathcal{V} \mid \times \mid \mathcal{V} \mid}\rightarrow \mathbb{R}^{\mid \mathcal{V} \mid}$는 인접행렬을 입력 받아서 노드마다 어떤 값을 주는 벡터를 출력해주는 함수이다.)

$$f(\mathbf{P} \mathbf{A} \mathbf{P}^\top)=f(\mathbf{A}) \quad \quad (\text{Permutation Invariance}) \quad \quad (5.2)$$

<br>

즉, 노드 순서가 뒤바뀐 인접행렬을 넣었을 때 함수값이 그냥 인접행렬을 넣었을 때의 함수값과 똑같다는 것이다. 한편, `permutation equivariant`한 함수 $f$는 다음의 성질을 만족시킨다.

$$f(\mathbf{P} \mathbf{A} \mathbf{P}^\top)=\mathbf{P}f(\mathbf{A}) \quad \quad (\text{Permutation Equivariant}) \quad \quad (5.3)$$

<br>

즉, 노드 순서를 바꿨던대로 $f(\mathbf{A})$의 원소의 순서를 바꿔준 것과 $f(\mathbf{P}\mathbf{A}\mathbf{P}^\top)$이 같다는 것이다. 정리하자면, 우리는 `permutation invariant`한 함수나 아니면 적어도 `permutation equivariant`한 성질을 같는 딥러닝 모델을 고려해야 한다.

---

# 5.1 Neural Message Passing
지금까지 등장한 GNN 모델들은 정말 다양한 동기로부터 만들어졌다. 하지만 그들 모두가 공유하고 있는 한 가지 구조가 있다. 바로 신경망 메세지 전달 (Neural Message Passing) 구조이다.  신경망 메세지 전달이란 뉴럴 네트워크를 통하여 노드끼리  메세지 벡터를 교환하는 것을 말한다.

<br>

## 5.1.1 Overview of the Message Passing Framework
노드 $u$의 $k$ 번째 hidden state 벡터를 $\mathbf{h}^{(k)}_u$라고 하자. $k+1$ 번째 hidden state $\mathbf{h}^{(k+1)}_u$는 노드 $u$의 이웃 노드로부터 전달 받은 정보들을 취합 (`AGGREGATE`)하고 업데이트 (`UPDATE`)하여 만들어진다. 즉,

$$\begin{matrix}
\mathbf{h}^{(k+1)}_u 
&=& \text{UPDATE}^{(k)} \;(\mathbf{h}^{(k)}_u, \text{AGGREGATE}^{(k)}(\{ \mathbf{h}^{(k)}_v:v \in \mathcal{N}(u) \}) & (5.4) \\
& = & \text{UPDATE}^{(k)} \;(\mathbf{h}^{(k)}_u, \mathbf{m}^{(k)}_{\mathcal{N}(u)}) & (5.5)
\end{matrix}$$

<br>

이때, `AGGREGATE`와 `UPDATE`는 임의의 미분 가능한 함수이다. `AGGREGATE` 함수와 `UPDATE` 함수를 어떻게 선택하느냐에 따라서 GNN 알고리즘이 다양하게 구분될 수 있다. 두 함수에 대한 다양한 예시를 이번 장에서 마주치게 될 것이다. $\mathbf{m}\_{\mathcal{N}(u)}$은 노드 $u$의 이웃 노드의 정보를 취합하여 만든 메세지 벡터이다.<br><br>

우리는 먼저 $k=0$일 때 각 노드 $u$의 임베딩 벡터를 $u$의 feature 벡터로 설정한다. 즉, $\mathbf{h}^{(0)}_u=\mathbf{x}_u, \; \forall u \in V$ 이다. 다음으로 다음의 신경망 메세지 전달을 반복한다. 각 $k$번 째 레이어 (또는 반복이라고도 말함)에서 하는 일은 다음과 같다.

- `AGGREGATE` 함수는 $u$의 이웃 노드 $\mathcal{N}(u)$들의 hidden state **집합**을 입력 받아서 메세지 벡터 $\mathbf{m}^{(k)}\_{\mathcal{N}(u)}$를 만든다.
- `UPDATE` 함수는 메세지 벡터 $\mathbf{m}^{(k)}\_{\mathcal{N}(u)}$와 노드 $u$의 $k$ 번째 hidden state $\mathbf{h}^{(k)}_u$를 입력 받아서 hidden state를 업데이트한다.

<br>

위의 설명에서 한 가지 주목해야 할 점은 `AGGREGATE` 함수가 $\mathcal{N}(u)$들의 hidden state **집합**을 입력 받는다는 것이다. 한 집합 안에서 원소들의 순서가 뒤섞여도 여전히 같은 집합이다. 따라서 `AGGREGATE` 함수는 입력 받는 hidden state들의 순서에 상관 없이 일정한 결과를 계산해줘야 한다 (permutation invariance). 또는 최소한 계산된 결과를 다시 뒤섞어서 순서를 맞춰줄 수 있어야 한다(permutation equivariance).<br><br>

$K$번의 메세지 반복을 마치고 얻은 각 노드 $u$의 임베딩 벡터 $\mathbf{h}^{(K)}_u$를 노드 $u$의 임베딩 벡터로 사용하게 된다. 즉, $\mathbf{z}_u=\mathbf{h}^{(K)}_u, \; \forall u \in V$이다.<br><br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 노드의 속성 벡터가 없으면 어떡하나요?</p>

만약 노드 속성 벡터 (attribute vector)가 없다면, 이전 챕터들에서 배운 방법들을 사용하여 feature 벡터들을 메뉴얼하게 만들어줘야 한다.

만약, 주어진 그래프에서 노드 임베딩만하는 것이 목적이라면 identity feature를 사용해도 된다. 노드 $u$의 identity feature $\mathbf{e}_u \in \mathbb{R}^{\mid V \mid}$는 $u$번 째 원소만 1이고 나머지 원소는 0인 원핫 벡터이다. 하지만 이렇게 만든 GNN 모델의 경우 새로 유입되는 노드를 임베딩해줄 수 없는 transductive 성질을 갖게 된다.

</div>

---

## 5.1.2 Motivations and Intuitions
GNN에서 메세지 전달을 한 번 진행하면 한 노드가 이웃 노드들의 정보를 반영하여 벡터를 만들게 된다. 메세지 전달을 두 번 진행하면 한 노드에 대하여 이웃 노드의 이웃 노드의 정보까지 반영하여 임베딩 벡터가 만들어지게 된다. 이렇게 메세지 전달이 많이 반복될 수록 한 노드에 대하여 더 멀리 떨어진 이웃 노드의 정보를 반영하여 벡터를 만들 수 있게 된다.<br><br>

그럼 어떤 정보를 반영하는 것일까? 노드 $u$의 $k$ 번째 hidden state $\mathbf{h}^{(k)}_u$는 크게 두 가지 정보를 반영하게 된다. 

- 그래프 안의 구조적인 정보. $\mathbf{h}^{(k)}_u$는 노드 $u$의 $k$-hop 이웃 노드 정보를 반영하고 있다.
- feature 기반 정보: $\mathbf{h}^{(k)}_u$은 노드 $u$의 $k$-hop 이웃 노드의 feature 벡터 정보를 반영하고 있다.

<br>

이웃한 데이터로부터 feature 벡터를 취합한다는 점에서 합성곱 신경망 (CNN)과 유사하다고 볼 수 있다. CNN은 정해진 크기의 패치 안의 feature 정보를 취합 (합성곱)한다는 점과 GNN은 이웃 노드의 정보를 취합한다는 점에서 차이가 있다.

---

## The Basic GNN
**Coming soon!**

## 참고문헌
<p id="ref1">[1] <a href="https://pixabay.com/ko/illustrations/%ec%a7%80%ea%b5%ac-%ed%9a%8c%eb%a1%9c%eb%a7%9d-3537401/" target="_blank">https://pixabay.com/ko/illustrations/지구-회로망-3537401/</a></p>
<p id="ref2">[2] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref3">[3] <a href="http://web.stanford.edu/class/cs224w/" target="_blank">CS224W: Machine Learning with Graphs</a></p>
