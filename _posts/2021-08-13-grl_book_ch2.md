---
layout: post
title:  "[GRL Book 정리]Chapter 2. Background and Traditional Approaches"
date:   2021-8-13 09:00
categories: [Others]
use_math: true
comments: true
---

# <center>Chapter 2. Background and Traditional Approaches</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 두 번째 이야기. 부디 완주하게 기도해주세요 !

<br>

---

이 책의 주된 주제는 그래프를 벡터로 표현하는 방법 **Graph Representation Learning**과 그래프에 딥러닝을 적용하는 방법 **Deep learning on graphs** 이다. 이번 챕터에서는 위 두 가지 주제를 보다 더 잘 이해하기 위한 사전지식을 배운다. 특히, 딥러닝 등장 이전에 그래프 자료구조에 사용됐던 기법들을 풀고자 하는 문제들에 따라서 알아본다.

1. *그래프 분류* 를 위한 **그래프 통계량 및 커널 메서드**
2. **이웃 노드 중복도**를 사용한 *relation prediction* 
3. *군집화 및 커뮤니티 탐지* 를 위한 **Laplacian 기반의 Spectral Clustering**

<br>

---

# Chapter 2.1 Graph Statistics and Kernel Methods
대부분의 머신러닝 모델들은 **벡터로 표현된 데이터**를 입력값으로 받는다. 하지만 노드, 엣지, 그래프는 벡터가 아니기 때문에 머신러닝 모델에 넣어줄 수 없다. 따라서, 그래프로 표현되는 데이터를 머신러닝 모델에 사용하기 위해서는 노드든 엣지든 그래프든 먼저 벡터로 표현해야 한다. 이번 섹션에서는 먼저 노드 레벨에서 feature 또는 통계량을 뽑아내는 방법을 알아본다. 다음으로 노드 레벨 feature를 그래프 레벨 feature로 확장시키는 방법과 추가적으로 그래프 커널 방법론에 대해 알아본다.

<br>

---

### 2.1.1 Node-level Statistics and Features
이번 서브 섹션에서는 대표적인 각 노드가 갖고 있는 통계량에 대해서 알아본다.

<br>

#### Node Degree (차수)
주어진 그래프의 한 노드 $u$의 차수는 노드 $u$에 인접한 이웃 노드의 개수이며 $d_u$라고 표기한다. 노드 $u$의 차수 $d_u$를 인접행렬 $\mathbf{A}$로 나타내면 다음과 같다.
- $d_u=\sum\limits_{v \in V} \mathbf{A}[u, v] \quad\quad (2.1)$

인접행렬 $\mathbf{A}$의 $u$행의 각 원소를 더해주는 것이다. 인접행렬의 정의에 따라 노드 $v$가 노드 $u$와 이웃이라면 $\mathbf{A}[u,v]$ 값이 1이 된다. 반대로 노드 $v$가 노드 $u$와 이웃이 아니라면 $\mathbf{A}[u,v]$ 값이 0이다. 따라서 이를 모두 더하면 이웃 노드의 개수가 된다. 노드 차수는 오래 전부터 가장 핵심적이고 유익한 정보 중 하나였다. 하지만, 이 통계량은 노드의 중요도를 나타내기에는 조금 부족하다. 차수가 1로 동일한 두 노드가 있다고 생각해보자. 연결된 하나의 이웃 노드가 얼마나 중요하느냐에 따라서 두 노드들의 중요도도 달라질 것이다. 차수는 이를 반영해주지 못한다.

<br>

#### Node Centrality
해당 노드가 그래프 안에서 얼마나 중요한지를 나타낼 수 있는 통계량이다. Node Centrality는 여러 종류가 있는데, 가장 먼저 `Eigenvector Centrality` 에 대해서 알아보자. `Eigenvector Centrality`의 핵심은 이웃 노드들의 중요도로 내 중요도를 계산한다. 주어진 그래프의 한 노드 $u$의 eigenvector centrality $e_u$는 다음과 같이 정의된다.

- $e_u=\frac{1}{\lambda}\sum_{v \in V}\mathbf{A}[u, v] \; e_v,\quad \forall u \in V \quad\quad (2.2)$

<br>

식 $(2.2)$는 recursive한 성질을 갖는다. 노드 $u$의 centrality $e_u$를 구할 때 이웃 노드의 centrality $e_v$를 사용한다. 그래서 먼저 $e_v$ 값을 구하러 가보면 노드 $v$의 이웃 노드 중에 $u$가 있기 때문에 $e_v$ 값 계산에 다시 $e_u$가 필요하다. 계란이 먼저냐 닭이 먼저냐의 문제가 되버린다. 계산하기 정말 막막하지만, 사실은 굉장히 쉽게 구해진다고 한다. 각 노드들의 eigenvector centrality를 원소로 갖는 벡터 $\mathbf{e}$ 는 인접행렬 $\mathbf{A}$의 eigen-equation을 풀어서 얻을 수 있다. 즉, $\mathbf{e}$는 다음을 만족한다.
- $\lambda\mathbf{e}=\mathbf{A}\mathbf{e} \quad \quad (2.3)$

<br>

사실, 이렇게 구한 벡터가 노드들의 중요도를 나타낼 수 있다는 것이 잘 와닿지 않는다. 그래서 내 나름대로 이해하기 위한 발버둥을 쳐봤다. 다음은 eigenvector centrality에 대한 나의 고찰이다.
- 인접행렬 $\mathbf{A}$에 어떤 벡터 $\mathbf{v}$를 곱한다는 것의 의미를 살펴보자.
- 쉬운 이해를 위해 $\mathbf{v}$에는 각 노드의 가치 또는 가격이 적혀있다고 생각하자. 예를 들어, 첫 번째 노드의 가격은 1,000원, 두 번째 노드의 가격은 20,000원 이런 식으로 가격을 갖는 노드 중요도 벡터를 상상해보자.  
- 인접행렬 $\mathbf{A}$의 $u$ 번 째 행은 노드 $u$의 이웃 노드에 1로 마킹한 행벡터이다.
- 그럼, $\mathbf{A} \mathbf{v}$의 $u$번 째 원소는 노드 $u$의 이웃 노드들의 가격을 모두 더한 것이다. 즉, 이웃들의 중요도를 다 더한 것이다.
    - 사실, $\mathbf{v}$를 어떻게 선택하느냐에 따라서, $\mathbf{A} \mathbf{v}$ 안의 원소들 (노드 중요도)의 크고 작음이 달라질 수 있다. 어떤 $\mathbf{v}$에 대해서는 세 번째 노드가 가장 중요했는데, 다른 $\mathbf{v}$에 대해서는 다섯 번째 노드가 가장 중요할 수도 있다는 의미이다. 
    - 뿐만 아니라, $\mathbf{A} \mathbf{v}$ 벡터를 다시 중요도 벡터로 사용할 수도 있을 것이다. 그럼 다시 $\mathbf{A}$를 곱하여 중요도 벡터를 구하게 되는데, 이때도 역시 노드 중요도의 순위가 달라질 수 있다. 
- 이를 대처할 수 있는 것이 eigenvector $\mathbf{e}$ 이다.
    - $A\mathbf{e}=\lambda\mathbf{e}$ 이다. 인접행렬 $\mathbf{A}$를 $n$번 곱해주면 $\lambda^n \mathbf{e}$가 된다. 상수배는 순위에 영향을 주지 않는다.


<br>

위와 같은 느낌에서 eigenvector를 중요도 벡터로 사용했을 때 얻는 이점이 많은 것 같다. 뿐만 아니라, Perron-Frobenius 이론이라는 것에 의하여 식 $(2.3)$을 풀었을 때 가장 큰 eigenvalue가 유일하게 존재하고, 대응하는 eigenvector의 원소는 모두 같은 부호를 갖는다고 한다. 따라서, 양수의 통계량을 얻을 수 있다는 이점도 있다.<br><br>

책에서는 eigenvector와 그래프 랜덤 워킹 사이의 관계에 대해서도 설명하고 있다. 깊게 다루고 있지는 않아서 가벼운 마음으로 받아들였다.
- Eigenvector centrality는 그래프 안에서 무한히 많이 랜덤워킹할 때, 해당 노드에 도착한 비율 또는 해당 노드에 도착할 확률이다. (이 둘은 무한 길이의 랜덤워킹의 경우 같다.)
- $\mathbf{e}^{(0)}=(1, 1, \cdots, 1)^T$에서 시작하여 다음과 같은 power iteration을 할 때,
    - $\mathbf{e}^{(t+1)}=A\mathbf{e}^{(t)} \quad \quad (2.4)$
- $\mathbf{e}^{(1)}$은 각 원소의 degree가 된다. 이는 해당 노드를 1회 워킹만으로 도착하는 경우의 수이다.
- $\mathbf{e}^{(t)}$는 해당 노드를 $t$회 워킹으로 도착하는 경우의 수이다.

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

그래프 랜덤워킹이란?
- 임의의 노드 $u$에서 시작해서 $u$의 이웃 노드 중에 하나를 임의로 선택하여 이동한다.
- 이동한 노드에서 이를 반복한다.

이런 과정을 통해 방문한 노드들을 순서대로 기록했을 때, 이 수열을 랜덤워크라고 부른다.
</div>

<br>

이 책에서는 eigenvector centrality 외에 betweenness centrality 와 closeness centrality도 소개한다.
- Betweenness Centrality
    - 해당 노드가 다른 두 노드 사이의 최단 경로에 있는 경우의 수
    - $c_u=\sum_{u \ne v_1 \ne v_2}\frac{\text{$v_1$과 $v_2$의 최단 경로에 u가 포함되는 횟수}}{\text{$v_1$과 $v_2$의 최단 경로의수}}$
    - (예) 대전
- Closeness Centrality
    - 해당 노드와 다른 노드 사이의 최단 경로의 평균값
    - $c_u=\frac{1}{\sum_{v \ne u} \text{$u$와 $v$사이의 최단 경로 길이}}$

<br>

---

#### The Clustering Coefficient
**Coming soon !**
