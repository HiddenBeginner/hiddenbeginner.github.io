---
layout: post
title:  "[GRL Book 정리] Chapter 2. Background and Traditional Approaches"
date:   2021-8-13 09:00
categories: [Others]
use_math: true
comments: true
---

# <center>Chapter 2. Background and Traditional Approaches</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 두 번째 이야기. 부디 완주하게 기도해주세요 !

<br>

---

이 책의 주된 주제는 그래프를 벡터로 표현하는 방법 **Graph Representation Learning**과 그래프에 딥러닝을 적용하는 방법 **Deep learning on graphs** 이다. 이번 챕터에서는 위 두 가지 주제를 보다 더 잘 이해하기 위한 사전지식을 배운다. 특히, 딥러닝 등장 이전에 그래프 자료구조에 사용됐던 기법들을 풀고자 하는 문제들에 따라서 알아본다.

1. *그래프 분류* 를 위한 **그래프 통계량 및 커널 메서드**
2. **이웃 노드 중복도**를 사용한 *relation prediction* 
3. *군집화 및 커뮤니티 탐지* 를 위한 **Laplacian 기반의 Spectral Clustering**

<br>

---

# Chapter 2.1 Graph Statistics and Kernel Methods
대부분의 머신러닝 모델들은 **벡터로 표현된 데이터**를 입력값으로 받는다. 하지만 노드, 엣지, 그래프는 벡터가 아니기 때문에 머신러닝 모델에 넣어줄 수 없다. 따라서, 그래프로 표현되는 데이터를 머신러닝 모델에 사용하기 위해서는 노드든 엣지든 그래프든 먼저 벡터로 표현해야 한다. 이번 섹션에서는 먼저 노드 레벨에서 feature 또는 통계량을 뽑아내는 방법을 알아본다. 다음으로 노드 레벨 feature를 그래프 레벨 feature로 확장시키는 방법과 추가적으로 그래프 커널 방법론에 대해 알아본다.

<br>

---

### 2.1.1 Node-level Statistics and Features
이번 서브 섹션에서는 대표적인 각 노드가 갖고 있는 통계량에 대해서 알아본다.

#### Node Degree (차수)
주어진 그래프의 한 노드 $u$의 차수는 노드 $u$에 인접한 이웃 노드의 개수이며 $d_u$라고 표기한다. 노드 $u$의 차수 $d_u$를 인접행렬 $\mathbf{A}$로 나타내면 다음과 같다.
- $d_u=\sum\limits_{v \in V} \mathbf{A}[u, v] \quad\quad (2.1)$

인접행렬 $\mathbf{A}$의 $u$행의 각 원소를 더해주는 것이다. 인접행렬의 정의에 따라 노드 $v$가 노드 $u$와 이웃이라면 $\mathbf{A}[u,v]$ 값이 1이 된다. 반대로 노드 $v$가 노드 $u$와 이웃이 아니라면 $\mathbf{A}[u,v]$ 값이 0이다. 따라서 이를 모두 더하면 이웃 노드의 개수가 된다. 노드 차수는 오래 전부터 가장 핵심적이고 유익한 정보 중 하나였다. 하지만, 이 통계량은 노드의 중요도를 나타내기에는 조금 부족하다. 차수가 1로 동일한 두 노드가 있다고 생각해보자. 연결된 하나의 이웃 노드가 얼마나 중요하느냐에 따라서 두 노드들의 중요도도 달라질 것이다. 차수는 이를 반영해주지 못한다.

<br>

---

#### Node Centrality
해당 노드가 그래프 안에서 얼마나 중요한지를 나타낼 수 있는 통계량이다. Node Centrality는 여러 종류가 있는데, 가장 먼저 `Eigenvector Centrality` 에 대해서 알아보자. `Eigenvector Centrality`의 핵심은 이웃 노드들의 중요도로 내 중요도를 계산한다. 주어진 그래프의 한 노드 $u$의 eigenvector centrality $e_u$는 다음과 같이 정의된다.

- $e_u=\frac{1}{\lambda}\sum_{v \in V}\mathbf{A}[u, v] \; e_v,\quad \forall u \in V \quad\quad (2.2)$

<br>

식 $(2.2)$는 recursive한 성질을 갖는다. 노드 $u$의 centrality $e_u$를 구할 때 이웃 노드의 centrality $e_v$를 사용한다. 그래서 먼저 $e_v$ 값을 구하러 가보면 노드 $v$의 이웃 노드 중에 $u$가 있기 때문에 $e_v$ 값 계산에 다시 $e_u$가 필요하다. 계란이 먼저냐 닭이 먼저냐의 문제가 되버린다. 계산하기 정말 막막하지만, 사실은 굉장히 쉽게 구해진다고 한다. 각 노드들의 eigenvector centrality를 원소로 갖는 벡터 $\mathbf{e}$ 는 인접행렬 $\mathbf{A}$의 eigen-equation을 풀어서 얻을 수 있다. 즉, $\mathbf{e}$는 다음을 만족한다.
- $\lambda\mathbf{e}=\mathbf{A}\mathbf{e} \quad \quad (2.3)$

<br>

사실, 이렇게 구한 벡터가 노드들의 중요도를 나타낼 수 있다는 것이 잘 와닿지 않는다. 그래서 내 나름대로 이해하기 위한 발버둥을 쳐봤다. 다음은 eigenvector centrality에 대한 나의 고찰이다.
- 인접행렬 $\mathbf{A}$에 어떤 벡터 $\mathbf{v}$를 곱한다는 것의 의미를 살펴보자.
- 쉬운 이해를 위해 $\mathbf{v}$에는 각 노드의 가치 또는 가격이 적혀있다고 생각하자. 예를 들어, 첫 번째 노드의 가격은 1,000원, 두 번째 노드의 가격은 20,000원 이런 식으로 가격을 갖는 노드 중요도 벡터를 상상해보자.  
- 인접행렬 $\mathbf{A}$의 $u$ 번 째 행은 노드 $u$의 이웃 노드에 1로 마킹한 행벡터이다.
- 그럼, $\mathbf{A} \mathbf{v}$의 $u$번 째 원소는 노드 $u$의 이웃 노드들의 가격을 모두 더한 것이다. 즉, 이웃들의 중요도를 다 더한 것이다.
    - 사실, $\mathbf{v}$를 어떻게 선택하느냐에 따라서, $\mathbf{A} \mathbf{v}$ 안의 원소들 (노드 중요도)의 크고 작음이 달라질 수 있다. 어떤 $\mathbf{v}$에 대해서는 세 번째 노드가 가장 중요했는데, 다른 $\mathbf{v}$에 대해서는 다섯 번째 노드가 가장 중요할 수도 있다는 의미이다. 
    - 뿐만 아니라, $\mathbf{A} \mathbf{v}$ 벡터를 다시 중요도 벡터로 사용할 수도 있을 것이다. 그럼 다시 $\mathbf{A}$를 곱하여 중요도 벡터를 구하게 되는데, 이때도 역시 노드 중요도의 순위가 달라질 수 있다. 
- 이를 대처할 수 있는 것이 eigenvector $\mathbf{e}$ 이다.
    - $A\mathbf{e}=\lambda\mathbf{e}$ 이다. 인접행렬 $\mathbf{A}$를 $n$번 곱해주면 $\lambda^n \mathbf{e}$가 된다. 상수배는 순위에 영향을 주지 않는다.


<br>

위와 같은 느낌에서 eigenvector를 중요도 벡터로 사용했을 때 얻는 이점이 많은 것 같다. 뿐만 아니라, Perron-Frobenius 이론이라는 것에 의하여 식 $(2.3)$을 풀었을 때 가장 큰 eigenvalue가 유일하게 존재하고, 대응하는 eigenvector의 원소는 모두 같은 부호를 갖는다고 한다. 따라서, 양수의 통계량을 얻을 수 있다는 이점도 있다.<br><br>

책에서는 eigenvector와 그래프 랜덤 워킹 사이의 관계에 대해서도 설명하고 있다. 깊게 다루고 있지는 않아서 가벼운 마음으로 받아들였다.
- Eigenvector centrality는 그래프 안에서 무한히 많이 랜덤워킹할 때, 해당 노드에 도착한 비율 또는 해당 노드에 도착할 확률이다. (이 둘은 무한 길이의 랜덤워킹의 경우 같다.)
- $\mathbf{e}^{(0)}=(1, 1, \cdots, 1)^T$에서 시작하여 다음과 같은 power iteration을 할 때,
    - $\mathbf{e}^{(t+1)}=A\mathbf{e}^{(t)} \quad \quad (2.4)$
- $\mathbf{e}^{(1)}$은 각 원소의 degree가 된다. 이는 해당 노드를 1회 워킹만으로 도착하는 경우의 수이다.
- $\mathbf{e}^{(t)}$는 해당 노드를 $t$회 워킹으로 도착하는 경우의 수이다.

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

그래프 랜덤워킹이란?
- 임의의 노드 $u$에서 시작해서 $u$의 이웃 노드 중에 하나를 임의로 선택하여 이동한다.
- 이동한 노드에서 이를 반복한다.

이런 과정을 통해 방문한 노드들을 순서대로 기록했을 때, 이 수열을 랜덤워크라고 부른다.
</div>

<br>

이 책에서는 eigenvector centrality 외에 betweenness centrality 와 closeness centrality도 소개한다.
- Betweenness Centrality
    - 해당 노드가 다른 두 노드 사이의 최단 경로에 있는 경우의 수
    - $c_u=\sum_{u \ne v_1 \ne v_2}\frac{\text{$v_1$과 $v_2$의 최단 경로에 u가 포함되는 횟수}}{\text{$v_1$과 $v_2$의 최단 경로의수}}$
    - (예) 대전
- Closeness Centrality
    - 해당 노드와 다른 노드 사이의 최단 경로의 평균값
    - $c_u=\frac{1}{\sum_{v \ne u} \text{$u$와 $v$사이의 최단 경로 길이}}$

<br>

---

#### The Clustering Coefficient
The clustering coefficient는 한 노드의 **이웃 노드들이 얼마나 긴밀하게 연결되어 있는가**를 나타낼 수 있는 척도이다. 다음 그림에서 노드 $u_1$과 $u_2$의 차수는 4로 같지만, 노드 $u_2$의 이웃 노드들은 서로 더 긴밀하게 연결되어 있음을 알 수 있다. 다양한 clustering coefficient들이 있지만, 이 책에서는 `local clustering coefficient`만을 소개한다.<br><br>

![multi-plex](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-13-grl_book_ch2/local-clustering-coefficient.png){: width="400" height="400"){: .center}
<center>[노드 u1과 u2는 차수가 서로 같지만, 이웃 노드들의 연결성은 다르다.]</center>

<br>

그래프 $G=(V, E)$의 한 노드 $u \in V$의 local clustering coefficient $c_u$는 다음과 같이 정의된다.
- $c_u = \frac{\mid (v_1, v_2) \in E : v_1, v_2 \in \mathcal{N}(u)\mid}{_{d_u}\mathrm{C}_2}, \quad \mathcal{N}(u)=\\{v \in V:(u, v) \in E\\} \quad \quad (2.5)$

<br>

$\mathcal{N}(u)$는 $u$의 이웃 노드들을 모아놓은 집합이다. $V$에 있는 노드 $v$ 중에 $(u, v)$가 $E$에 있는 애들, 즉, $u$와 연결된 노드들을 모아놓은 것이기 때문이다. $c_u$의 분자는 $u$의 이웃 노드들끼리 서로 연결된 엣지의 개수이다. 분모는 $u$의 이웃 노드들 중에서 2개를 뽑는 경우의 수이다. 따라서 $c_u$는 노드 $u$의 이웃 노드들끼리 서로 연결된 비율로 이해할 수 있다.<br><br>

위의 그림에서 $u_1$의 이웃 노드들끼리는 서로 연결되어 있지 않기 때문에 $c_{u_1}=0$이 된다. $u_2$의 이웃 노드들 중에서는 $(v_1, v_2)$, $(v_2, v_3)$, $(v_3, v_4)$, $(v_4, v_1)$가 연결되어 있기 때문에 분자는 4가 되고, 분모는 $\_{4}\mathrm{C}\_2=6$이 되어 $c\_{u_2}=2/3$이 된다. 만약, $(v_1, v_3)$과 $(v_2, v_4)$도 연결되어 있었다면 local clustering coefficient 값은 1이 되었을 것이다. 얼핏 보기에는 $(v_1, v_3)$과 $(v_2, v_4)$가 연결되어 있는 것처럼 보일 수도 있지만 $u_2$를 경유하는 상태라서 path는 있지만 서로가 이웃되어 있지는 않기 때문에 분자에 카운트되지 않는다.<br><br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

그래프 $G=(V, E)$의 한 노드 $u$의 `Ego 그래프` $G_{\text{ego}_u}$는 그래프 $G$ 안에서 노드 $u$와 그 이웃 노드들로 만들 수 있는 모든 엣지들을 포함하고 있는 $G$의 부분 그래프이다. 즉, 
- $G_{\text{ego}\_u}=(V_{\text{ego}\_u}, E_{\text{ego}\_u})$
- $V_{\text{ego}\_u}=\\{ u \\} \cup \mathcal{N}(u)$
- $E_{\text{ego}\_u}=\\{ (v_1, v_2): v_1, v_2 \in V_{\text{ego}\_u} \\}$
</div>

<br>

Local clustering coefficient를 다른 관점에서 보면 다음과 같이 생각할 수 있다.
- $u$의 `Ego 그래프`에서 만들 수 있는 
- $u$를 포함하는 **삼각형 모양의 부분 그래프의 개수**를 구해서
- 가능한 모든 삼각형 모양의 부분 그래프의 개수로 나눠준 것이다.

<br>

뒤에서 다룰 `Graphlet`은 **삼각형 모양의 부분 그래프** 뿐만 아니라 노드 $u$ 주변에서 만들 수 있는 **다양한 모양의 부분 그래프**를 사용해서 노드 또는 그래프의 feature를 생성해낸다.

<br>

---

### 2.1.2 Graph-level Features and Graph Kernels
먼저 이번 섹션에서 빈출하는 개념에 대해 정리하고 가자 !

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! Graph Kernel 이란?</p>

우리가 가지고 있는 그래프 자료구조는 숫자나 벡터로 표현되는 형태가 아니다. 따라서 그래프 사이의 연산도 하지 못하고 머신러닝 모델의 입력으로 사용할 수도 없다. 그래서 우리의 첫 번째 관심사는 그래프로부터 어떤 값 (feature)들을 뽑아내서 벡터로 만드는 것이다. 즉, 그래프 $G$를 입력 받아서 $d$-차원 feature 벡터 $\phi(G)$를 만들어주는 feature mapping $\phi:\mathcal{G} \rightarrow \mathbb{R}^d$를 만드는 것이다. 여기서 $\mathcal{G}$는 그래프 $G$가 살고 있는 공간이다.<br>

한편, 우리의 두 번째 관심사는 두 그래프 사이의 유사도를 구하는 것이다. 앞으로 이 유사도를 내적이라고 부르겠다. 내적 중에서 우리에게 가장 익숙한 것은 `dot product`이다. 두 벡터의 `dot product`는 $<\mathbf{u}, \mathbf{v}>=\mathbf{u}^T \mathbf{v}$로 정의된다. 하지만 내적은 벡터 사이에서 정의되기 때문에 두 그래프 사이의 내적을 바로 계산할 수는 없다. 여기서 가장 자연스러운 행위는 그래프의 feature 벡터를 먼저 만들고 그 벡터 사이에서 내적을 하는 것이다. 즉, 두 그래프 $G$와 $G'$ 사이의 유사도를 두 feature 벡터 사이의 내적 $<\phi(G), \phi(G')>$으로 구해주는 것이다.<br>

feature 벡터를 알면 내적 연산을 통해 두 그래프 사이의 유사도를 구할 수 있다. 하지만 feature 벡터를 계산하기 어렵거나 심지어 feature 벡터를 구할 수 없는 상황 속에서도 두 feature 벡터 사이의 내적은 구할 수 있는 경우가 있다. feature 벡터를 직접적으로 구하지 않고 내적을 구하는 행위를 `kernel trick`이라고 부른다. 정리하자면, feature 벡터로 내적을 계산할 수도 있고, feature 벡터를 구하지 않고도 내적을 계산할 수도 있다는 것이다.<br>

두 그래프를 입력 받아서 내적값을 출력해주는 함수 $K$를 `kernel function`이라고 부른다. 즉, $K: \mathcal{G} \times \mathcal{G} \rightarrow \mathbb{R}$ $\quad \text{such that}$ $\quad K(G, G')=<\phi(G), \phi(G')>$. 그리고 모든 두 그래프 사이의 유사도를 계산하여 저장해놓은 행렬 $\mathbf{K}$을 `kernel matrix`라고 한다. 즉, $\mathbf{K} \in \mathbb{R}^{\mid \mathcal{G} \mid \times \mid \mathcal{G} \mid}$ $\quad \text{where} \quad$ $\mathbf{K}[G, G']=K(G, G')$. 머신러닝 알고리즘들 중에 feature mapping $\phi$ 대신 커널 함수 $K$만 알면 학습시킬 수 있는 모델들이 있다. 이런 모델들을 `kernel based 모델` 또는 `kernel method 모델`이라고 부른다.<br>

요컨데 `kernel`은 두 그래프 사이의 유사도로 이해할 수 있다. 그래프의 feature 벡터를 구한 후 유사도를 계산할 수도 있지만, 바로 그래프 사이의 유사도를 계산할 수도 있을 것이다. 앞으로 배우는 내용들은 그래프에서 먼저 feature 벡터를 뽑고, 두 그래프 사이의 유사도를 feature 벡터 사이의 `dot product`를 사용하여 계산하게 된다. 
</div>

<br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 그래프 동형 (Isomorphic graphs)이란?</p>

어떤 두 그래프 $G=(V_G, E_G)$와 $H=(V_H, E_H)$이 동형인지 확인한다는 의미는
- 겉보기에는 달라보이지만, 사실은 똑같은 그래프인지 확인하는 것이다.
- 만약 $(u, v) \in E_G$ 이면, $(f(u), f(v)) \in E_H$인 $V_G$와 $V_H$ 사이의 일대일 대응 함수 $f: V_G \rightarrow V_H$를 찾을 수 있으면 두 그래프 $G$와 $H$는 동형이라고 부른다.
- 동형의 수학적 정의가 어려울 수도 있지만, 그냥 두 그래프가 겉보기에만 다르지 사실 같다는 것을 의미한다. 그래프 $G$의 노드 $u$를 그래프 $H$의 노드 $f(u)$라고 불러도 그래프의 엣지 구조에 전혀 영향을 미치지 않는다는 의미이기 때문이다.
</div>
    
![graph-isomorphism](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-13-grl_book_ch2/graph-isomorphism.png){: width="400" height="400"){: .center}
<center>[그래프 G와 그래프 H는 겉보기에는 서로 다르지만, 사실 같은 그래프이다. 출처: <a href="#ref2">Graph isomorphism - Wikipedia</a>]</center>

<br>

---

#### The Weisfeiler-Lehman Kernel
**Coming Soon!**

---

## 참고문헌
<p id="ref1">[1] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref2">[2] Graph isomorphism - Wikipedia, <a href="https://en.wikipedia.org/wiki/Graph_isomorphism" target="_blank">https://en.wikipedia.org/wiki/Graph_isomorphism</a></p>
