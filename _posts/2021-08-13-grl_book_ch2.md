---
layout: post
title:  "[GRL Book 정리] Chapter 2. Background and Traditional Approaches"
date:   2021-8-13 09:00
categories: [Others]
use_math: true
comments: true
---

# <center>Chapter 2. Background and Traditional Approaches</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 두 번째 이야기. 부디 완주하게 기도해주세요 !

<br>

---

이 책의 주된 주제는 그래프를 벡터로 표현하는 방법 **Graph Representation Learning**과 그래프에 딥러닝을 적용하는 방법 **Deep learning on graphs** 이다. 이번 챕터에서는 위 두 가지 주제를 보다 더 잘 이해하기 위한 사전지식을 배운다. 특히, 딥러닝 등장 이전에 그래프 자료구조에 사용됐던 기법들을 풀고자 하는 문제들에 따라서 알아본다.

1. *그래프 분류* 를 위한 **그래프 통계량 및 커널 메서드**
2. **이웃 노드 중복도**를 사용한 *relation prediction* 
3. *군집화 및 커뮤니티 탐지* 를 위한 **Laplacian 기반의 Spectral Clustering**

<br>

---

# Chapter 2.1 Graph Statistics and Kernel Methods
대부분의 머신러닝 모델들은 **벡터로 표현된 데이터**를 입력값으로 받는다. 하지만 노드, 엣지, 그래프는 벡터가 아니기 때문에 머신러닝 모델에 넣어줄 수 없다. 따라서, 그래프로 표현되는 데이터를 머신러닝 모델에 사용하기 위해서는 노드든 엣지든 그래프든 먼저 벡터로 표현해야 한다. 이번 섹션에서는 먼저 노드 레벨에서 feature 또는 통계량을 뽑아내는 방법을 알아본다. 다음으로 노드 레벨 feature를 그래프 레벨 feature로 확장시키는 방법과 추가적으로 그래프 커널 방법론에 대해 알아본다.

<br>

---

## 2.1.1 Node-level Statistics and Features
이번 서브 섹션에서는 대표적인 각 노드가 갖고 있는 통계량에 대해서 알아본다.

### Node Degree (차수)
주어진 그래프의 한 노드 $u$의 차수는 노드 $u$에 인접한 이웃 노드의 개수이며 $d_u$라고 표기한다. 노드 $u$의 차수 $d_u$를 인접행렬 $\mathbf{A}$로 나타내면 다음과 같다.
- $d_u=\sum\limits_{v \in V} \mathbf{A}[u, v] \quad\quad (2.1)$

인접행렬 $\mathbf{A}$의 $u$행의 각 원소를 더해주는 것이다. 인접행렬의 정의에 따라 노드 $v$가 노드 $u$와 이웃이라면 $\mathbf{A}[u,v]$ 값이 1이 된다. 반대로 노드 $v$가 노드 $u$와 이웃이 아니라면 $\mathbf{A}[u,v]$ 값이 0이다. 따라서 이를 모두 더하면 이웃 노드의 개수가 된다. 노드 차수는 오래 전부터 가장 핵심적이고 유익한 정보 중 하나였다. 하지만, 이 통계량은 노드의 중요도를 나타내기에는 조금 부족하다. 차수가 1로 동일한 두 노드가 있다고 생각해보자. 연결된 하나의 이웃 노드가 얼마나 중요하느냐에 따라서 두 노드들의 중요도도 달라질 것이다. 차수는 이를 반영해주지 못한다.

<br>

---

### Node Centrality
해당 노드가 그래프 안에서 얼마나 중요한지를 나타낼 수 있는 통계량이다. Node Centrality는 여러 종류가 있는데, 가장 먼저 `Eigenvector Centrality` 에 대해서 알아보자. `Eigenvector Centrality`의 핵심은 이웃 노드들의 중요도로 내 중요도를 계산한다. 주어진 그래프의 한 노드 $u$의 eigenvector centrality $e_u$는 다음과 같이 정의된다.

- $e_u=\frac{1}{\lambda}\sum_{v \in V}\mathbf{A}[u, v] \; e_v,\quad \forall u \in V \quad\quad (2.2)$

<br>

식 $(2.2)$는 recursive한 성질을 갖는다. 노드 $u$의 centrality $e_u$를 구할 때 이웃 노드의 centrality $e_v$를 사용한다. 그래서 먼저 $e_v$ 값을 구하러 가보면 노드 $v$의 이웃 노드 중에 $u$가 있기 때문에 $e_v$ 값 계산에 다시 $e_u$가 필요하다. 계란이 먼저냐 닭이 먼저냐의 문제가 되버린다. 계산하기 정말 막막하지만, 사실은 굉장히 쉽게 구해진다고 한다. 각 노드들의 eigenvector centrality를 원소로 갖는 벡터 $\mathbf{e}$ 는 인접행렬 $\mathbf{A}$의 eigen-equation을 풀어서 얻을 수 있다. 즉, $\mathbf{e}$는 다음을 만족한다.
- $\lambda\mathbf{e}=\mathbf{A}\mathbf{e} \quad \quad (2.3)$

<br>

사실, 이렇게 구한 벡터가 노드들의 중요도를 나타낼 수 있다는 것이 잘 와닿지 않는다. 그래서 내 나름대로 이해하기 위한 발버둥을 쳐봤다. 다음은 eigenvector centrality에 대한 나의 고찰이다.
- 인접행렬 $\mathbf{A}$에 어떤 벡터 $\mathbf{v}$를 곱한다는 것의 의미를 살펴보자.
- 쉬운 이해를 위해 $\mathbf{v}$에는 각 노드의 가치 또는 가격이 적혀있다고 생각하자. 예를 들어, 첫 번째 노드의 가격은 1,000원, 두 번째 노드의 가격은 20,000원 이런 식으로 가격을 갖는 노드 중요도 벡터를 상상해보자.  
- 인접행렬 $\mathbf{A}$의 $u$ 번 째 행은 노드 $u$의 이웃 노드에 1로 마킹한 행벡터이다.
- 그럼, $\mathbf{A} \mathbf{v}$의 $u$번 째 원소는 노드 $u$의 이웃 노드들의 가격을 모두 더한 것이다. 즉, 이웃들의 중요도를 다 더한 것이다.
    - 사실, $\mathbf{v}$를 어떻게 선택하느냐에 따라서, $\mathbf{A} \mathbf{v}$ 안의 원소들 (노드 중요도)의 크고 작음이 달라질 수 있다. 어떤 $\mathbf{v}$에 대해서는 세 번째 노드가 가장 중요했는데, 다른 $\mathbf{v}$에 대해서는 다섯 번째 노드가 가장 중요할 수도 있다는 의미이다. 
    - 뿐만 아니라, $\mathbf{A} \mathbf{v}$ 벡터를 다시 중요도 벡터로 사용할 수도 있을 것이다. 그럼 다시 $\mathbf{A}$를 곱하여 중요도 벡터를 구하게 되는데, 이때도 역시 노드 중요도의 순위가 달라질 수 있다. 
- 이를 대처할 수 있는 것이 eigenvector $\mathbf{e}$ 이다.
    - $A\mathbf{e}=\lambda\mathbf{e}$ 이다. 인접행렬 $\mathbf{A}$를 $n$번 곱해주면 $\lambda^n \mathbf{e}$가 된다. 상수배는 순위에 영향을 주지 않는다.


<br>

위와 같은 느낌에서 eigenvector를 중요도 벡터로 사용했을 때 얻는 이점이 많은 것 같다. 뿐만 아니라, Perron-Frobenius 이론이라는 것에 의하여 식 $(2.3)$을 풀었을 때 가장 큰 eigenvalue가 유일하게 존재하고, 대응하는 eigenvector의 원소는 모두 같은 부호를 갖는다고 한다. 따라서, 양수의 통계량을 얻을 수 있다는 이점도 있다.<br><br>

책에서는 eigenvector와 그래프 랜덤 워킹 사이의 관계에 대해서도 설명하고 있다. 깊게 다루고 있지는 않아서 가벼운 마음으로 받아들였다.
- Eigenvector centrality는 그래프 안에서 무한히 많이 랜덤워킹할 때, 해당 노드에 도착한 비율 또는 해당 노드에 도착할 확률이다. (이 둘은 무한 길이의 랜덤워킹의 경우 같다.)
- $\mathbf{e}^{(0)}=(1, 1, \cdots, 1)^T$에서 시작하여 다음과 같은 power iteration을 할 때,
    - $\mathbf{e}^{(t+1)}=A\mathbf{e}^{(t)} \quad \quad (2.4)$
- $\mathbf{e}^{(1)}$은 각 원소의 degree가 된다. 이는 해당 노드를 1회 워킹만으로 도착하는 경우의 수이다.
- $\mathbf{e}^{(t)}$는 해당 노드를 $t$회 워킹으로 도착하는 경우의 수이다.

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

그래프 랜덤워킹이란?
- 임의의 노드 $u$에서 시작해서 $u$의 이웃 노드 중에 하나를 임의로 선택하여 이동한다.
- 이동한 노드에서 이를 반복한다.

이런 과정을 통해 방문한 노드들을 순서대로 기록했을 때, 이 수열을 랜덤워크라고 부른다.
</div>

<br>

이 책에서는 eigenvector centrality 외에 betweenness centrality 와 closeness centrality도 소개한다.
- Betweenness Centrality
    - 해당 노드가 다른 두 노드 사이의 최단 경로에 있는 경우의 수
    - $c_u=\sum_{u \ne v_1 \ne v_2}\frac{\text{$v_1$과 $v_2$의 최단 경로에 u가 포함되는 횟수}}{\text{$v_1$과 $v_2$의 최단 경로의수}}$
    - (예) 대전
- Closeness Centrality
    - 해당 노드와 다른 노드 사이의 최단 경로의 평균값
    - $c_u=\frac{1}{\sum_{v \ne u} \text{$u$와 $v$사이의 최단 경로 길이}}$

<br>

---

### The Clustering Coefficient
The clustering coefficient는 한 노드의 **이웃 노드들이 얼마나 긴밀하게 연결되어 있는가**를 나타낼 수 있는 척도이다. 다음 그림에서 노드 $u_1$과 $u_2$의 차수는 4로 같지만, 노드 $u_2$의 이웃 노드들은 서로 더 긴밀하게 연결되어 있음을 알 수 있다. 다양한 clustering coefficient들이 있지만, 이 책에서는 `local clustering coefficient`만을 소개한다.<br><br>

![multi-plex](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-13-grl_book_ch2/local-clustering-coefficient.png){: width="400" height="400"){: .center}
<center>[노드 u1과 u2는 차수가 서로 같지만, 이웃 노드들의 연결성은 다르다.]</center>

<br>

그래프 $G=(V, E)$의 한 노드 $u \in V$의 local clustering coefficient $c_u$는 다음과 같이 정의된다.
- $c_u = \frac{\mid (v_1, v_2) \in E : v_1, v_2 \in \mathcal{N}(u)\mid}{_{d_u}\mathrm{C}_2}, \quad \mathcal{N}(u)=\\{v \in V:(u, v) \in E\\} \quad \quad (2.5)$

<br>

$\mathcal{N}(u)$는 $u$의 이웃 노드들을 모아놓은 집합이다. $V$에 있는 노드 $v$ 중에 $(u, v)$가 $E$에 있는 애들, 즉, $u$와 연결된 노드들을 모아놓은 것이기 때문이다. $c_u$의 분자는 $u$의 이웃 노드들끼리 서로 연결된 엣지의 개수이다. 분모는 $u$의 이웃 노드들 중에서 2개를 뽑는 경우의 수이다. 따라서 $c_u$는 노드 $u$의 이웃 노드들끼리 서로 연결된 비율로 이해할 수 있다.<br><br>

위의 그림에서 $u_1$의 이웃 노드들끼리는 서로 연결되어 있지 않기 때문에 $c_{u_1}=0$이 된다. $u_2$의 이웃 노드들 중에서는 $(v_1, v_2)$, $(v_2, v_3)$, $(v_3, v_4)$, $(v_4, v_1)$가 연결되어 있기 때문에 분자는 4가 되고, 분모는 $\_{4}\mathrm{C}\_2=6$이 되어 $c\_{u_2}=2/3$이 된다. 만약, $(v_1, v_3)$과 $(v_2, v_4)$도 연결되어 있었다면 local clustering coefficient 값은 1이 되었을 것이다. 얼핏 보기에는 $(v_1, v_3)$과 $(v_2, v_4)$가 연결되어 있는 것처럼 보일 수도 있지만 $u_2$를 경유하는 상태라서 path는 있지만 서로가 이웃되어 있지는 않기 때문에 분자에 카운트되지 않는다.<br><br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

그래프 $G=(V, E)$의 한 노드 $u$의 `Ego 그래프` $G_{\text{ego}_u}$는 그래프 $G$ 안에서 노드 $u$와 그 이웃 노드들로 만들 수 있는 모든 엣지들을 포함하고 있는 $G$의 부분 그래프이다. 즉, 
- $G_{\text{ego}\_u}=(V_{\text{ego}\_u}, E_{\text{ego}\_u})$
- $V_{\text{ego}\_u}=\\{ u \\} \cup \mathcal{N}(u)$
- $E_{\text{ego}\_u}=\\{ (v_1, v_2): v_1, v_2 \in V_{\text{ego}\_u} \\}$
</div>

<br>

Local clustering coefficient를 다른 관점에서 보면 다음과 같이 생각할 수 있다.
- $u$의 `Ego 그래프`에서 만들 수 있는 
- $u$를 포함하는 **삼각형 모양의 부분 그래프의 개수**를 구해서
- 가능한 모든 삼각형 모양의 부분 그래프의 개수로 나눠준 것이다.

바로 이어서 다룰 `Graphlet`은 **삼각형 모양의 부분 그래프** 뿐만 아니라 노드 $u$ 주변에서 만들 수 있는 **다양한 모양의 부분 그래프**를 사용해서 노드 또는 그래프의 feature를 생성해낸다.
 
<br>

---

### Closed Triangles, Ego Graphs, and Motifs
소제목에는 3개의 개념이 있다. `closed triangles`과 `ego graph`는 바로 이전에 `local clustering coefficient`에서 다룬 개념이다. 남은 개념은 `motifs`인데 책에는 설명이 너무 부실하고, 검색해도 잘 나오지 않는다. 따라서, 여기서는 `motifs` 대신 `graphlets` 이란 개념을 사용한 feature 벡터 만드는 방법을 이야기하고자 한다.<br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 !</p>

이번 섹션에서 다룰 `graphlets`은 위키피디아에 나오는 `graphlets`와는 다르다. 위키피디아에서는 `graphlets`을 다음과 같이 정의한다.<a href="ref3">[3]</a>

> Graphlets are the largest subgraph which contains a given set of nodes and all the edges involving those chosen nodes.

그래프 $G=(V, E)$의 주어진 노드 부분 집합 $S \subset V$에 대한 `graphlets`은 $S$를 노드 집합으로 갖는 $G$의 가장 큰 서브 그래프라고 한다. 즉, 노드 집합으로 $S$를 갖고, 엣지 집합으로 $\\{(u, v) \in E : u, v \in S \\}$으로 갖는 부분 그래프라는 것이다.

한편, 이번 섹션에서 사용할 `graphlet`은 논문 <a href="#ref4">[4]</a>에 등장하는 개념이다. 자세한 정의는 아래에서 소개한다.

</div>

`k-graphlets`은 $k$개의 노드로 만들 수 있는 서로 동형이 아닌 그래프들을 모아놓은 것이다. 노드가 $k$개 있다면 서로 다른 두 엣지를 고르는 경우의 수는 $\_{k}\mathrm{C}\_{2}=\frac{k(k-1)}{2}$개이다. 그리고 각 경우의 수마다 엣지가 있을 수도 있고 없을 수도 있다. 따라서 $k$개의 노드로 만들 수 있는 모든 그래프의 개수는 $2^{\frac{k(k-1)}{2}}$개일 것이다. 정말 많은 개수이다. 하지만 사실 이들 중에는 노드에 부여된 인덱스를 가리고 보면 겹치는 그래프가 굉장히 많을 것이다. `k-graphlets`은 인덱스가 부여되지 않은 $k$개의 노드로 만들 수 있는 그래프들이라고 생각하면 된다.아래 논문 <a href="#ref4">[4]</a>의 그림을 보면 편할 것이다. 노드를 2개부터 5개까지 사용하여 만들 수 있는 모든 겹치지 않는 동형 그래프이다. $G_0$부터 $G_{29}$까지 총 30개가 있다.<br><br>

![graphlets](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-13-grl_book_ch2/graphlets.png){: width="500" height="500"){: .center}
<center>[노드가 2개부터 5개까지의 가능한 모든 graphlets을 보여준다. 출처: <a href="#ref4">논문 [4]</a>]</center>

<br>

노드 $u$마다 feature 벡터를 만들기 위해서 우리는 $u$에서 만들 수 있는 `graphlets`의 개수를 종류별로 세서 히스토그램 벡터를 만든다. 이때, 같은 `graphlet`이더라도 `graphlet`의 어떤 노드를 $u$와 대응할 것이냐에 따라서 비교 대상이 되는 부분 그래프가 달라질 것이다. 예를 들어, 위 그림 $G_1$의 1번 노드를 $u$에 대응했을 때 만들어지는 부분 그래프와 2번 노드를 $u$에 대응했을 때 만들어지는 부분 그래프가 달라질 것이다. 따라서 노드의 feature 벡터를 만들기 위하여 한 노드에서 고려해야 할 경우의 수가 총 73가지이기 때문에 73차원 벡터가 만들어진다. 이렇게 만든 벡터를 `Graphlet Degree Vector (GDV)`라고 부른다.<br><br>

아래 그림은 <a href="#ref5">CS224W [5]</a>에서 등장하는 `GDV`의 예시이다. 노드 2개와 3개로 만들 수 있는 동형 그래프는 $G_1, G_2, G_3$가 있다. 그래프 $G_3$의 경우, 노드 $c$를 기준으로 하느냐, 노드 $d$를 기준으로 하느냐에 따라서 비교 대상이 되는 부분 그래프가 달라진다. 따라서 비교해야 할 경우의 수는 총 4가지이다. 한편 노드 $c$를 기준으로 하여 비교할 때 조심해야 할 점이 하나 있다. 분명, 노드 $v, u_1, u_2$로 만들어지는 삼각형은 $c$를 기준으로 하는 `graphlet`을 포함하고 있다. 하지만 노드 $v, u_1, u_2$와 비교할 때는 $v, u_1, u_2$로 만들 수 있는 엣지를 모두 포함하는 부분 그래프와 비교를 해야 한다. 이는 Wikipedia에 있는 `graphlets`의 정의에서 비롯된 것 같다. 따라서 엣지 $(u_1, u_2)$ 때문에 $c$를 기준으로 한 `graphlet`과는 일치하지 않는 것이다.<br><br>

![gdv](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-13-grl_book_ch2/gdv.png){: width="500" height="500"){: .center}
<center>[GDV 예시 그림. 노드 2개와 3개로 만들 수 있는 동형 그래프는 총 3개가 있다. 그리고, 어떤 노드를 기준으로 하여 볼 것이냐에 따라서는 4가지 경우의 수가 있다. Graphlet과 비교되는 부분 그래프는 기존 엣지를 모두 포함하고 있어야 한다. 출처: <a href="#ref5">CS224W [5]</a>]</center>

<br>

---

## 2.1.2 Graph-level Features and Graph Kernels
먼저 이번 섹션에서 빈출하는 개념에 대해 정리하고 가자 !

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! Graph Kernel 이란?</p>

우리가 가지고 있는 그래프 자료구조는 숫자나 벡터로 표현되는 형태가 아니다. 따라서 그래프 사이의 연산도 하지 못하고 머신러닝 모델의 입력으로 사용할 수도 없다. 그래서 우리의 첫 번째 관심사는 그래프로부터 어떤 값 (feature)들을 뽑아내서 벡터로 만드는 것이다. 즉, 그래프 $G$를 입력 받아서 $d$-차원 feature 벡터 $\phi(G)$를 만들어주는 feature mapping $\phi:\mathcal{G} \rightarrow \mathbb{R}^d$를 만드는 것이다. 여기서 $\mathcal{G}$는 그래프 $G$가 살고 있는 공간이다.<br>

한편, 우리의 두 번째 관심사는 두 그래프 사이의 유사도를 구하는 것이다. 앞으로 이 유사도를 내적이라고 부르겠다. 내적 중에서 우리에게 가장 익숙한 것은 `dot product`이다. 두 벡터의 `dot product`는 $<\mathbf{u}, \mathbf{v}>=\mathbf{u}^T \mathbf{v}$로 정의된다. 하지만 내적은 벡터 사이에서 정의되기 때문에 두 그래프 사이의 내적을 바로 계산할 수는 없다. 여기서 가장 자연스러운 행위는 그래프의 feature 벡터를 먼저 만들고 그 벡터 사이에서 내적을 하는 것이다. 즉, 두 그래프 $G$와 $G'$ 사이의 유사도를 두 feature 벡터 사이의 내적 $<\phi(G), \phi(G')>$으로 구해주는 것이다.<br>

feature 벡터를 알면 내적 연산을 통해 두 그래프 사이의 유사도를 구할 수 있다. 하지만 feature 벡터를 계산하기 어렵거나 심지어 feature 벡터를 구할 수 없는 상황 속에서도 두 feature 벡터 사이의 내적은 구할 수 있는 경우가 있다. feature 벡터를 직접적으로 구하지 않고 내적을 구하는 행위를 `kernel trick`이라고 부른다. 정리하자면, feature 벡터로 내적을 계산할 수도 있고, feature 벡터를 구하지 않고도 내적을 계산할 수도 있다는 것이다.<br>

두 그래프를 입력 받아서 내적값을 출력해주는 함수 $K$를 `kernel function`이라고 부른다. 즉, $K: \mathcal{G} \times \mathcal{G} \rightarrow \mathbb{R}$ $\quad \text{such that}$ $\quad K(G, G')=<\phi(G), \phi(G')>$. 그리고 모든 두 그래프 사이의 유사도를 계산하여 저장해놓은 행렬 $\mathbf{K}$을 `kernel matrix`라고 한다. 즉, $\mathbf{K} \in \mathbb{R}^{\mid \mathcal{G} \mid \times \mid \mathcal{G} \mid}$ $\quad \text{where} \quad$ $\mathbf{K}[G, G']=K(G, G')$. 머신러닝 알고리즘들 중에 feature mapping $\phi$ 대신 커널 함수 $K$만 알면 학습시킬 수 있는 모델들이 있다. 이런 모델들을 `kernel based 모델` 또는 `kernel method 모델`이라고 부른다.<br>

요컨데 `kernel`은 두 그래프 사이의 유사도로 이해할 수 있다. 그래프의 feature 벡터를 구한 후 유사도를 계산할 수도 있지만, 바로 그래프 사이의 유사도를 계산할 수도 있을 것이다. 앞으로 배우는 내용들은 그래프에서 먼저 feature 벡터를 뽑고, 두 그래프 사이의 유사도를 feature 벡터 사이의 `dot product`를 사용하여 계산하게 된다. 
</div>

<br>

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 그래프 동형 (Isomorphic graphs)이란?</p>

어떤 두 그래프 $G=(V_G, E_G)$와 $H=(V_H, E_H)$이 동형인지 확인한다는 의미는
- 겉보기에는 달라보이지만, 사실은 똑같은 그래프인지 확인하는 것이다.
- 만약 $(u, v) \in E_G$ 이면, $(f(u), f(v)) \in E_H$인 $V_G$와 $V_H$ 사이의 일대일 대응 함수 $f: V_G \rightarrow V_H$를 찾을 수 있으면 두 그래프 $G$와 $H$는 동형이라고 부른다.
- 동형의 수학적 정의가 어려울 수도 있지만, 그냥 두 그래프가 겉보기에만 다르지 사실 같다는 것을 의미한다. 그래프 $G$의 노드 $u$를 그래프 $H$의 노드 $f(u)$라고 불러도 그래프의 엣지 구조에 전혀 영향을 미치지 않는다는 의미이기 때문이다.
</div>
    
![graph-isomorphism](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-13-grl_book_ch2/graph-isomorphism.png){: width="400" height="400"){: .center}
<center>[그래프 G와 그래프 H는 겉보기에는 서로 다르지만, 사실 같은 그래프이다. 출처: <a href="#ref2">Graph isomorphism - Wikipedia</a>]</center>

<br>

---

### The Weisfeiler-Lehman Kernel

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! Node-level feature aggregation 이란?</p>

머신러닝을 공부하다보면 `aggregation`, `aggregate`란 단어를 제법 마주친다. 이 단어를 네이버에 검색하면 합계, 총액 등의 의미로 나온다. 하지만 두 단어가 실제 머신러닝 분야에서 사용되는 맥락과는 맞지 않는다는 느낌을 많이 받았다. `aggregation` 또는 `aggregate`는 **수집한 정보나 데이터를 취합하거나 요약하는 행위**라고 생각하면 편할 것이다.<br>

`Node-level feature aggregation`이란 노드 단위로 구한 feature를 취합해서 그래프의 feature 벡터를 만드는 행위를 뜻한다. 노드 차수를 예시로 들면, 그래프 안의 모든 노드들의 평균 차수를 그래프의 feature로 사용하는 것 또한 `aggregation`이라고 할 수 있다. 더 나은 방법은 노드 차수의 히스토그램을 그래프의 feature 벡터로 사용하는 것이다. 예를 들어, 그래프 안에서 차수가 1인 노드가 5개, 2인 노드가 10개, 3인 노드가 0개, 4인 노드가 7개라면 이 그래프의 feature 벡터는 $(5, 10, 0, 7)^T$이 되는 것이다. 이렇게 히스토그램을 사용해서 그래프의 feature 벡터를 만드는 방법을 `bag-of-something`이라고 부른다. 예시의 경우 차수 (degree)를 사용했기 때문에 `bag-of-degree` 방법을 이용해서 그래프를 벡터로 표현한 것이다.<br>
</div>

<br>

`Weisfeiler-Lehman` 알고리즘은 두 그래프가 동형인지 아닌지 확인하기 위한 알고리즘이다. 위에서 본 그래프 동형의 정의에 의하면 두 그래프가 동형임을 확인하기 위해서는 
- 두 그래프의 노드 집합 사이의 모든 일대일 대응 함수을 하나씩 가져와서
- 일대일 대응 함수에 의해 엣지 정보가 보존되는지 확인해야 했다.

<br>

그래프의 노드가 굉장히 많을 경우 가능한 모든 일대일 대응 함수를 탐색하는 것은 현실적으로 불가능한 일이다. `Weisfeiler-Lehman` 알고리즘은 반복적으로 이웃 노드들의 정보를 취합하는 방식 (`iterative neighborhood aggregation`)으로 두 그래프가 동형인지 확인할 수 있는 알고리즘이다. 이 알고리즘의 핵심 아이디어는 **한 노드의 다음 시점 레이블을 이웃 노드들의 현재 시점 레이블을 취합**하여 만드는 것이다. 알고리즘은 다음과 같다.<br><br>

- 그래프 $G=(V, E)$가 주어졌을 때, 먼저 각 노드 $u \in V$마다 초기 레이블 $c^{(0)}_u$를 부여한다. 일반적으로 $c^{(0)}_u=d_u$로 부여한다.
- $i$번 째 레이블 $c^{(i)}_u$가 주어졌을 때, $i-1$번 째 레이블을 다음과 같이 부여한다.
    - $c^{(i+1)}\_u=\text{HASH}(\\{ c^{(i)}\_u, \\{ c^{(i)}\_v\\}_{v \in \mathcal{N}(v)}\\})$
- K번 반복까지 등장한 레이블들에 대한 히스토그램을 그래프의 feature vector로 사용한다.

<br>

여기서 집합 괄호 `{`와 `}`는 `multi-set`이다. `multi-set`은 그냥 집합과는 다르게 원소의 중복을 허용한다. 예를 들어, `multi-set`은 $\\{ 1, 2, 2, 3, 3, 3\\}$을 허용한다는 것이다. $\text{HASH}$ 함수는 `multi-set`으로 표현될 뻔한 레이블에 겹치지 않는 인덱스 (숫자)를 부여해주는 역할을 한다. 요컨데, 한 노드의 다음 시점 레이블을 현재 시점의 이웃 노드들의 레이블을 모아놓은 `multi-set`으로 표현하고, 다른 레이블들과 겹치지 않는 인덱스를 부여하는 과정을 반복하는 것이다. 예시로 보는 것이 가장 편하다.<br><br>

**예시 그림 coming soon!**

<br>

---

### Graplets and Path-based method
`GDV`에서는 각 노드에서 만들 수 있는 `graphlets`의 개수를 세서 히스토그램 벡터를 만들었다. 비슷한 과정을 그래프를 대상으로 할 수 있다. 그래프 안에서 등장하는 `graphlets`의 개수를 세서 히스토그램 벡터를 만드는 것이다. 하지만 `GDV` 때와 다른 점이 몇 가지 있다.
- 한 `graphlet`에서 모든 노드가 연결되어 있을 필요는 없다. 
- 기준 노드 개념 없이 비교한다.

아래 그림 역시 <a href="#ref5">CS224W [5]</a>에 나오는 예시이다. 자세한 설명은 생략한다.

<br>

![graphlet-feature](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-13-grl_book_ch2/graphlet-feature.png){: width="500" height="500"){: .center}
<center>[노드 3개로 만들 수 있는 경우의 수는 4가지가 있다. 출처: <a href="#ref5">CS224W [5]</a>]</center>

<br>

이 외에도 그래프 안에서 랜덤 워크를 하고, 서로 다른 경로의 개수를 세서 히스토그램 벡터를 만드는 `path-based method`들도 있다. 예를 들어, 방문한 노드들의 차수로 수열을 만들고 서로 다른 수열의 개수를 세는 방법이 있다. 물론 랜덤 워크 대신 노드 사이의 최단 경로를 고려하여 벡터를 만들 수도 있다.

<br>

---

# 2.2 Neighborhood Overlap Detection
**Coming soon!**

---

## 참고문헌
<p id="ref1">[1] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref2">[2] Graph isomorphism - Wikipedia, <a href="https://en.wikipedia.org/wiki/Graph_isomorphism" target="_blank">https://en.wikipedia.org/wiki/Graph_isomorphism</a></p>
<p id="ref3">[3] Graphlets - Wikipedia, <a href="https://en.wikipedia.org/w/index.php?title=Graphlets&oldid=1035393985" target="_blank">https://en.wikipedia.org/w/index.php?title=Graphlets&oldid=1035393985</a></p>
<p id="ref4">[4] Nataša Pržulj, Biological network comparison using graphlet degree distribution, Bioinformatics, Volume 23, Issue 2, 15 January 2007, Pages e177–e183, <a href="https://doi.org/10.1093/bioinformatics/btl301" target="_blank">https://doi.org/10.1093/bioinformatics/btl301</a></p>
<p id="ref5">[5] <a href="http://web.stanford.edu/class/cs224w/" target="_blank">CS224W: Machine Learning with Graphs</a></p>
