---
layout: post
title:  "[논문 리뷰]실시간 이상 감지 모델 Robust Random Cut Forest (RRCF)"
date:   2021-7-14 20:00
categories: [PaperReview]
use_math: true
comments: true
---

# <center>Robust Random Cut Forest (RRCF)</center>
**<center>실시간 스트리밍 데이터에도 적합한 이상 감지 모델이 있다?</center>**<br/><br/>

오늘의 논문 먹방은 바로 "[Robust Random Cut Forest Based Anomaly Detection On Streams](https://proceedings.mlr.press/v48/guha16)" 으로 2016년 ICML에 게재된 논문입니다. 이 논문에서 제안하는 `Robust random cut forest (RRCF)` 모델은 트리 기반 이상 감지 모델입니다. `RRCF`는 가장 대표적인 트리 기반 이상 감지 모델인 [`Isolation Forest`](https://ieeexplore.ieee.org/document/4781136)와 차이가 거의 없습니다. 하지만 그 작은 차이가 엄청난 기여를 만들어냈습니다. <br/><br/>

![Figure1](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-07-14-rrcf/rrcf-figure1.png){: width="400" height="400"){: .center}
<center>[토이 데이터에 IF (위)와 RRCF (아래)를 적용한 결과. 파란색 선은 탑승객 수, 빨간색은 각 모델의 이상 스코어를 나타낸다. 출처: 참고문헌 <a href="#ref1">[1]</a>]</center>

<br>

위 그림은 제 마음대로 선정한 이 논문의 메인 figure 입니다. 파란색 선은 사인 그래프에서 235~255 구간을 이상 신호로 변환한 것입니다. 빨간색 선이 데이터의 이상 정도를 나타내는 이상 스코어입니다. 이 값이 높으면 데이터를 이상 데이터로 간주하게 됩니다. `IF` (위 그래프)의 경우 이상 신호가 모두 끝난 후에야 이상 스코어 값이 높아집니다. 반면에 `RRCF` (아래 그래프)의 경우 이상 신호의 시작점과 종료점에서 모두 이상 스코어가 높습니다. 실제 세계 문제에서는 대부분 실시간 이상 감지를 요구하는 것을 고려하면 RRCF의 이상 스코어가 더욱 합리적이라고 생각됩니다.<br><br>

**들어가기 전에**
- 이 논문이 2016년 논문이라는 것을 유념하고 읽어주시길 바랍니다. 최신 동향과는 맞지 않을 수 있습니다.
- `Isolation Forest`를 알고 있으면 포스팅 이해에 도움이 될 것입니다. 이와 관련하여 [고려대학교 강필성 교수님의 강의 영상](https://youtu.be/puVdwi5PjVA)을 추천드립니다.

<br>

**이 포스트에서 다룰 내용들**
- <a href="#section1">기존 이상 감지 모델들의 한계점</a>
- <a href="#section2">Isolation Forest</a>
- <a href="#section3">Robust Random Cut Forest</a>
- <a href="#section4">실험</a>

---


<span id="section1"></span>
## 기존 이상감지 모델들의 한계점

2008년 이전까지의 이상 감지 모델들은 밀도 (density) 기반 또는 거리 (distance) 기반이었습니다. 
- 밀도 기반 모델들은 데이터들의 확률밀도함수를 모델링하여, 특정 데이터가 등장할 확률이 낮으면 해당 데이터를 이상 데이터로 간주합니다.
- 거리 기반 모델들은 데이터 사이의 거리를 모두 계산하여, 다른 데이터들에 비해 멀리 동 떨어진 데이터를 이상 데이터로 간주하는 방법입니다.

<br>

두 방법론들 모두 각각의 장단점이 있겠지만, 데이터의 양과 차원 수가 높을 경우 계산 복잡도도 굉장히 높고 성능은 오히려 떨어지는 단점이 있었습니다. 직관적으로 생각했을 때,
- 데이터의 양이 증가할수록 이상 데이터도 증가할 것입니다. 만약 이상 데이터가 모여 작은 군집을 이룬다면 위 두 방법론들이 잘 적용되지 않을 것입니다. 
- 데이터의 차원이 증가할수록 확률밀도함수를 추정하기 어려울 것입니다.



---

<span id="section2"></span>
## Isolation Forest
2008년 기존의 방법론들과는 전혀 다른 방식인 트리 기반의 이상 감지 모델 `Isolation Forest (IF)`가 등장하게 됩니다. 오늘 소개하고자 하는 `RRCF`는 `IF`의 변형체이기 때문에 IF의 아이디어를 아주 간단하게 짚고 넘어가도록 하겠습니다. 보다 더 자세한 내용은 참고문헌 <a href="#ref2">[2]</a>를 참고해주시면 좋을 것 같습니다.<br><br>

`IF`는 `iTree`라고 불리는 트리를 다음과 같이 만듭니다.
- 임의의 feature $p$와 임의의 값 $q$를 선택합니다.
- 다음으로, feature $p$ 값이 $q$ 값보다 작은 데이터와 큰 데이터를 각각 left child와 right child로 분기시킵니다.
- 트리의 모든 leaf 노트가 (이론상) 하나의 데이터만 가질 때까지 위 과정을 수행합니다.

<br>

이런 트리를 만드는 이유는
- 이상 데이터의 경우 정상 데이터로부터 멀리 떨어져 있기 때문에 상대적으로 적은 분기만으로 고립시킬 수 있을겁니다. 따라서 이상 데이터는 `iTree`에서 루트 노드와 가까운 곳에 위치할 것입니다.
- 정상 데이터는 서로 뭉쳐 있기 때문에 각각을 고립시키기 위해서는 더 많은 분기를 필요로 할 것입니다. 따라서 정상 데이터는 `iTree`에서 루트 노드와 먼 곳에 위치할 것입니다.

![Figure2](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-07-14-rrcf/rrcf-figure2.png){: width="500" height="500"){: .center}
<center>[정상 데이터 $x_i$와 이상 데이터 $x_o$를 고립될 때까지 시행한 분기 기준선을 나타낸 그림. 출처: 참고문헌 <a href="#ref2">[2]</a>]</center>

<br>

위의 왼쪽 그림을 보면 정상 데이터 $x_i$를 나누기 위해서는 12번의 분기 (수직,수평선의 개수)가 필요했습니다. 반면, 오른쪽 그림을 보면 이상 데이터 $x_o$를 나누기 위해서 단 4번의 분기가 필요했습니다. 따라서 `IF`에서는 `iTree`를 여러 개 만들고 각 데이터가 고립되기까지의 **평균 분기 횟수**를 사용하여 이상 스코어를 정의합니다. 이상 스코어가 설정한 기준보다 높으면 이상 데이터로 간주하고 기준보다 낮으면 정상 데이터로 간주합니다.<br><br>

글만 읽었을 땐 이 알고리즘이 잘 작동할까 의문이 들 수 있습니다. 몇가지 의문점들을 해소하고 가도록 하겠습니다.
- 데이터가 많으면 모든 데이터를 고립시키기 위해 엄청난 연산이 필요하지 않은가?
    - `IF`에서는 모든 데이터를 사용하여 `iTree`를 만들지 않습니다. 하나의 `iTree`는 임의로 서브샘플링된 데이터, 예를 들어 256개,로 만들어집니다. `IF`는 여러 개의 트리를 만들기 때문에 모든 데이터를 충분히 여러 번 사용할 수 있을 것입니다. 
    - 각각의 트리가 무한하게 분기하는 것을 막기 위하여 트리의 최대 깊이를 설정합니다. 트리의 최대 깊이에 도달할 정도의 데이터라면 정상 데이터일 확률이 높을 것입니다. 
    - `iTree`는 이진 탐색 트리 (BST) 구조를 갖기 때문에 계산 복잡도가 굉장히 낮습니다. Parameter estimation 또는 pairwise distance를 필요로 하는 알고리즘에 비해 굉장히 빠릅니다.

- 이상 데이터가 서로 뭉쳐 있으면 이상 데이터 역시 필요한 분기 횟수가 많지 않은가?
    - 이 의문 역시 서브샘플링에 의해 완화됩니다. 전체 데이터에서 이상 데이터가 뭉쳐 있더라도, 서브샘플링을 통해 밀도가 희소해질 수 있습니다. 아래 그림을 보시면 전체 데이터 공간에서는 뭉쳐있던 이상 데이터 (세모)들이 서브 샘플링 공간에서는 희소하게 분포하는 것을 볼 수 있습니다.
    
![Figure3](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-07-14-rrcf/rrcf-figure3.png){: width="500" height="500"){: .center}
<center>[전체 데이터 공간과 서브샘플링된 데이터 공간. 파란 동그라미는 정상 데이터, 빨간 세모는 이상 데이터를 나타낸다. 출처: 참고문헌 <a href="#ref2">[2]</a>]</center>
    
<br>    
    
- 이상 데이터일 수록 평균 분기 횟수가 작다고 했는데, 왜 이를 사용한 이상 스코어는 높을 수록 이상 스코어로 간주하는가?
    - 이상 스코어는 $2^{-\frac{\text{평균 분기 횟수}}{\text{어떤 상수}}}$로 이해하시면 좋을 것 같습니다.
    
- 훈련 과정과 테스트 과정이 잘 구분되지 않는다.
    - 훈련 데이터로부터 여러 개의 `iTree`를 만드는 것이 훈련 과정입니다. 그리고 모든 과정이 레이블을 필요로 하지 않는 비지도학습입니다.
    - 새로운 데이터가 유입되면 훈련 과정에서 만든 `iTree`들을 통과시켜 평균 분기 횟수를 셈하여 이상 스코어를 계산하게 됩니다.
    

---

<span id="section3"></span>
## Robust Random Cut Forest

위에서 언급한 한계점들을 해결하기 위해 본 논문에서는 `Robust random cut forest (RRCF)`를 제안합니다. `RRCF`는 `IF`와 크게 두 부분만 다릅니다.
- Feature $p$를 선택할 때 uniform random하게 뽑는 대신, 각 feature가 갖는 값의 범위에 따라 확률을 다르게 부여하여 선택합니다.
- Average path length 대신, Collusive displacement (CoDisp)라는 새로운 이상 스코어를 사용합니다.

1번 변경사항은 실시간 스트리밍 환경에서도 이상 감지 모델이 잘 동작할 수 있게 만들어줍니다. 2번 변경사항은 이상 데이터를 다른 관점으로 정의함으로서 이상 감지 성능을 향상시켰습니다.<br><br>

### Robust random cut tree (RRCT)
`RRCF`의 각 트리는 다음과 같이 만들어집니다. 주어진 (서브샘플링 된) 데이터셋 $S$에 대하여 `robust random cut tree (RRCT)` $\mathcal{T}(S)$는 다음과 같이 만들어집니다. (`RRCT 생성 알고리즘`)
- 랜덤하게 feature $p$를 선택합니다. 이때, $i$번 째 feature가 선택될 확률은 $\frac{l_i}{\sum_j l_j}$ 입니다. 여기서 $l_i=\max_{x \in S}x_i-\min_{x \in S} x_i$ 입니다.
- $[\min_{x \in S} x_i, \max_{x \in S}x_i]$ 범위에서 uniform random하게 값 $q$를 선택합니다.
- Left child를 $S_1=\\{x \mid x \in S, x_i \le q\\}$ 로, right child로 $S_2=\\{x \mid x \in S, x_i > q\\}$로 분기합니다.
- 위를 반복합니다.

이렇게 만든 트리들을 모아놓은 것을 `RRCF`라고 부릅니다.<br><br>

`IF`와 비교하였을 때 다른 점은 feature $p$를 균등하게 선택하는 것이 아닌 각 feature가 갖는 값의 범위에 따라 서로 다른 확률을 부여하여 선택한다는 것입니다. 이 작은 차이만으로 시간에 따라 분포가 점점 달라지는 데이터에 대응하여 트리를 만들 수 있게 됩니다. 따라서 논문에서는 `RRCF`가 실시간 스트리밍 데이터에 적합한 알고리즘이라고 주장하고 있습니다. 잠시 이 주제를 짚고 넘어가도록 하겠습니다.


### 실시간 스트리밍 환경
실시간 스트리밍 환경에서는 시간의 흐름에 따라 유입되는 데이터의 분포가 달라질 수 있습니다. 과거 데이터를 학습한 모델이 앞으로 유입되는 데이터에 대해서도 좋은 성능을 보일 것이라는 보장은 없습니다. 따라서 새롭게 유입되는 데이터들을 계속 모델 학습에 사용해야 합니다. 예를 들어, 현재 시점이 $t$일 때, 가장 최근 256개의 데이터로 트리를 구성한다고 생각해보겠습니다.
- $S_t=\\{\mathbf{x}\_{t-255}, \mathbf{x}\_{t-254},\cdots, \mathbf{x}\_t\\}$
- $\mathcal{T}(S_t)$ : $S_t$로 만든 `RRCT`

다음 시점으로 넘어가면 $S\_{t+1}=\\{\mathbf{x}\_{t-254},\cdots,\mathbf{x}\_{t},\mathbf{x}\_{t+1}\\}$ 을 이용하여 `RRCT`를 다시 만들어야 합니다. 이때, `RRCT 생성 알고리즘`을 통해 $\mathcal{T}(S_{t+1})$ 을 만들 수도 있겠지만, 이미 만들어놓은 $\mathcal{T}(S_t)$를 사용해서 만들 수 있다면 더욱 효율적일 것입니다. `RRCT`는 결국 각 leaf 노드가 (이론상) 데이터 한 개인 트리이기 때문에 $\mathcal{T}(S_t)$에서 $\mathbf{x}\_{t-255}$를 삭제하고, $\mathbf{x}\_{t+1}$ 추가하여 $\mathcal{T}'(S\_{t+1})$을 만들 수 있습니다.<br/><br/>

이때, $\mathcal{T}(S_{t+1})$은 `RRCT 생성 알고리즘`으로 만든 것이고, $\mathcal{T}'(S_{t+1})$은 $\mathcal{T}(S_t)$을 변형해서 만든 것을 유념하셔야 합니다.
- `RRCT 생성 알고리즘`는 feature $p$와 값 $q$의 선택에 따라 무수히 많은 $\mathcal{T}(S_{t+1})$을 만들 수 있습니다. 
    - 즉, $\mathcal{T}(S_{t+1})$을 random variable으로 간주하고 확률분포를 생각해볼 수 있습니다.
    
- $\mathcal{T}'(S\_{t+1})$은 $\mathcal{T}(S\_t)$에서 $\mathbf{x}\_{t-255}$를 삭제하고, $\mathbf{x}\_{t+1}$ 추가하여 유일하게 결정됩니다. 즉, $\mathcal{T}'(S\_{t+1})$은 $\mathcal{T}(S\_t)$에 종속적입니다.
    - 마찬가지로, $\mathcal{T}(S\_t)$을 random variable으로 간주하고 $\mathcal{T}'(S\_{t+1})$의 확률분포를 생각해볼 수 있습니다.
<br><br>

여기서 주목해야할 것은 $\mathcal{T}(S_{t+1})$의 확률분포와 $\mathcal{T}'(S_{t+1})$의 확률분포가 같을 것이라고 기대하기는 힘듭니다. 단순히 생각해봐도 
- $\mathcal{T}(S_{t+1})$은 $S_{t+1}$의 feature 값들의 최소/최대값을 사용하여 만들어지고,
- $\mathcal{T}'(S_{t+1})$은 $S_{t}$의 feature 값들의 최소/최대값을 사용하여 만들어진 트리에 노드 삭제/추가 연산만 더해진 것

뿐이기 때문입니다. 그리고 두 확률분포가 다르다는 이야기는 다음과 같습니다.
- 실시간 스트리밍 환경에서 새로 유입된 데이터를 학습한 모델 $\mathcal{T}(S_{t+1})$의 분포와 다른 분포를 갖는 모델 $\mathcal{T}'(S_{t+1})$을 사용한다는 것입니다.
- 따라서 엉뚱한 트리들로 데이터들의 이상 스코어를 계산할 확률이 증가한다는 것입니다. 

하지만 놀랍게도 `RRCT 생성 알고리즘`으로 생성된 트리에 논문에서 제안하는 노드 삭제/추가 알고리즘을 사용하면  $\mathcal{T}(S_{t+1})$와  $\mathcal{T}'(S_{t+1})$의 분포가 같아진다고 합니다. 보다 더 정확한 이론의 statement와 증명은 논문을 참고하시면 좋을 것 같습니다. 왜냐하면 너무 어려워서 저는 읽기를 포기했기 때문입니다.

### Collusive displacement (CoDisp)
Coming soon !!

---

## 참고문헌
<p id="ref1">[1] S. Guha, N. Mishra, G. Roy, O. Schrijvers, "Robust Random Cut Forest: Based Anomaly Detection on Streams", <i>Proceedings of The 33rd International Conference on Machine Learning</i>, 48, pp. 2712-2721, 2016.<p>
<p id="ref2">[2] F. T. Liu, K. M. Ting and Z. Zhou, "Isolation Forest," <i>2008 Eighth IEEE International Conference on Data Mining</i>, 2008, pp. 413-422, doi: 10.1109/ICDM.2008.17.<p>
<p id="ref3">[3] 고려대학교 강필성 교수님의 강의 영상, https://youtu.be/puVdwi5PjVA<p> 
