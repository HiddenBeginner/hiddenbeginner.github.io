---
layout: post
title:  "[GRL Book 정리] Chapter 6. Graph Neural Networks in Practice"
date:   2021-9-27 23:00
categories: [Others]
use_math: true
comments: true
---

![intro](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/earth-network.jpg){: .center}
<center>사진 출처: <a href="#ref1">[1]</a></center>

# <center>Chapter 6. Graph Neural Networks in Practice</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 다섯 번째 이야기. 부디 완주하게 기도해주세요 !

<br>

---

# 6.1 Applications and Loss Functions
GNN은 주로 다음 세 가지 문제를 해결하는데 많이 사용된다.

- 노드 분류 및 회귀 (예) 소셜 네트워크에서 한 계정이 실제 사용자인지 봇인지 분류
- 그래프 분류 및 회귀 (예) 분자 성질 분류 및 회귀
- 관계 예측 (예) 추천 시스템 (한 사용자와 연결 확률이 높은 상품을 추천)

<br>

섹션 $6.1$에서는 위 세 가지 문제를 해결하기 위해서 각각 어떤 손실 함수를 사용해야 하는지 알아본다. 그리고 섹션의 후반부에서는 비지도 학습 기반으로 GNN을 사전 훈련 (pre-training)하는 방법에 대해 알아본다. 사전 훈련된 GNN을 위 세 가지 문제에 사용할 경우 모델의 예측 성능을 향상시킬 수도 있다. (지도/비지도/자가지도 학습으로 모델 가중치를 사전 훈련한 후 모델의 끝부분만 바꿔서 문제를 해결하는 접근 방법이 많다. 이때,사전 훈련된 네트워크로 풀려고자 하는 문제를 downstream task라고 부른다.)

<br>

---

## 6.1.1 GNN for Node Classification

노드 분류 문제 해결을 위해서는 다음 손실 함수를 사용한다.

$$\mathcal{L}=\sum\limits_{u \in \mathcal{V}_\text{train}}-\operatorname{log}(\operatorname{softmax}(\mathbf{z}_u, \mathbf{y}_u)), \quad \quad (6.1)$$

<br>

이때 손실 함수는 훈련 노드 집합 $\mathcal{V}\_{\text{train}}$에 있는 모든 원소에 대한 negative log likelihood를 나타낸다. 여기서 $\mathbf{z}_u$은 GNN의 마지막 레이어를 통과한 hidden state 벡터 $\mathbf{h}_u^{(K)}$이고, $\mathbf{y}_u$는 노드 $u$의 클래스에 대한 원핫 벡터이다. 물론 GNN의 출력값 $\mathbf{z}_u$를 바로 소프트 맥스 함수에 넣어줘도 되겠지만 일반적으로는 학습 가능한 벡터를 곱해준 후 넣어준다. 

$$\operatorname{softmax}(\mathbf{z}_u,\mathbf{y}_u)=\sum\limits_{i=1}^{c}\mathbf{y}_u[i]\frac{e^{-\mathbf{z}_u^\top\mathbf{w}_i}}{\sum_{j=1}^{c}e^{-\mathbf{z}_u^\top\mathbf{w}_j}}, \quad \quad (6.2)$$

<br>

여기서 $\mathbf{w}_i \in \mathbb{R}^d, i=1,2,\cdots,c$는 학습 가능한 벡터이다. $\mathbf{y}_u[i]$는 $i$가 $u$가 속한 클래스일 경우에만 1이고 나머지는 0이다. 따라서 소프트 맥스는 보이는 식보다 더 간단한데, 모델이 노드 $u$가 실제 $u$의 클래스에 속할 것이라고 예측한 확률 값이다. 식 $(6.1)$의 변형체들도 있지만 보통 식 $(6.1)$을 가장 많이 사용한다.

<br>

<div class="note-box" markdown="1">
 
<p class="note-box-title">잠깐 ! supervised, semi-supervised, transductive, inductive </p>

노드 분류 문제의 경우 한 그래프 안에서 어떤 노드들은 레이블링이 되어 있고, 어떤 노드들은 되어 있지 않을 수 있다. 

- 이때, GNN 메세지 전달 과정에 사용되고, 레이블링이 있어 손실 함수 계산에도 사용되는 노드들을 training node $\in \mathcal{V}\_\text{train}$ 라고 부른다.
- GNN 메세지 전달 과정에는 사용되지만, 레이블이 없어 손실 함수 계산에는 사용되지 않는 노드들을 transductive test node $\in \mathcal{V}\_\text{trans}$라고 한다. GNN은 transductive 노들에 대해서 여전히 hidden state 벡터를 만들지만 레이블이 없어서 손실 함수 계산에는 사용할 수 없다.
- 한편, 레이블링은 있지만 메세지 전달 과정과 손실 함수 계산에서 사용하지 않았다가 모델의 성능 평가용으로 사용하는 노드들을 inductive test 노드라고 부른다.

transductive test node, inductive test node 둘 다 학습이 완료된 모델의 예측 대상이 되는 테스트 노드이다. 노드 분류 문제의 경우 transductive test node를 훈련 과정에서 사용하는 성질 때문에 semi supervised learning이라고 불린다.

</div>

---

## 6.1.2 GNN for Graph Classification
**Coming soon!**

## 참고문헌
<p id="ref1">[1] <a href="https://pixabay.com/ko/illustrations/%ec%a7%80%ea%b5%ac-%ed%9a%8c%eb%a1%9c%eb%a7%9d-3537401/" target="_blank">https://pixabay.com/ko/illustrations/지구-회로망-3537401/</a></p>
<p id="ref2">[2] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref3">[3] <a href="http://web.stanford.edu/class/cs224w/" target="_blank">CS224W: Machine Learning with Graphs</a></p>
