---
layout: post
title:  "[GRL Book 정리] Chapter 6. Graph Neural Networks in Practice"
date:   2021-9-27 23:00
categories: [Others]
use_math: true
comments: true
---

![intro](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/earth-network.jpg){: .center}
<center>사진 출처: <a href="#ref1">[1]</a></center>

# <center>Chapter 6. Graph Neural Networks in Practice</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 다섯 번째 이야기. 부디 완주하게 기도해주세요 !

<br>

---

# 6.1 Applications and Loss Functions
GNN은 주로 다음 세 가지 문제를 해결하는데 많이 사용된다.

- 노드 분류 및 회귀 (예) 소셜 네트워크에서 한 계정이 실제 사용자인지 봇인지 분류
- 그래프 분류 및 회귀 (예) 분자 성질 분류 및 회귀
- 관계 예측 (예) 추천 시스템 (한 사용자와 연결 확률이 높은 상품을 추천)

<br>

섹션 $6.1$에서는 위 세 가지 문제를 해결하기 위해서 각각 어떤 손실 함수를 사용해야 하는지 알아본다. 그리고 섹션의 후반부에서는 비지도 학습 기반으로 GNN을 사전 훈련 (pre-training)하는 방법에 대해 알아본다. 사전 훈련된 GNN을 위 세 가지 문제에 사용할 경우 모델의 예측 성능을 향상시킬 수도 있다. (지도/비지도/자가지도 학습으로 모델 가중치를 사전 훈련한 후 모델의 끝부분만 바꿔서 문제를 해결하는 접근 방법이 많다. 이때,사전 훈련된 네트워크로 풀려고자 하는 문제를 downstream task라고 부른다.)

<br>

---

## 6.1.1 GNN for Node Classification

노드 분류 문제 해결을 위해서는 다음 손실 함수를 사용한다.

$$\mathcal{L}=\sum\limits_{u \in \mathcal{V}_\text{train}}-\operatorname{log}(\operatorname{softmax}(\mathbf{z}_u, \mathbf{y}_u)), \quad \quad (6.1)$$

<br>

이때 손실 함수는 훈련 노드 집합 $\mathcal{V}\_{\text{train}}$에 있는 모든 원소에 대한 negative log likelihood를 나타낸다. 여기서 $\mathbf{z}_u$은 GNN의 마지막 레이어를 통과한 hidden state 벡터 $\mathbf{h}_u^{(K)}$이고, $\mathbf{y}_u$는 노드 $u$의 클래스에 대한 원핫 벡터이다. 물론 GNN의 출력값 $\mathbf{z}_u$를 바로 소프트 맥스 함수에 넣어줘도 되겠지만 일반적으로는 학습 가능한 벡터를 곱해준 후 넣어준다. 

$$\operatorname{softmax}(\mathbf{z}_u,\mathbf{y}_u)=\sum\limits_{i=1}^{c}\mathbf{y}_u[i]\frac{e^{-\mathbf{z}_u^\top\mathbf{w}_i}}{\sum_{j=1}^{c}e^{-\mathbf{z}_u^\top\mathbf{w}_j}}, \quad \quad (6.2)$$

<br>

여기서 $\mathbf{w}_i \in \mathbb{R}^d, i=1,2,\cdots,c$는 학습 가능한 벡터이다. $\mathbf{y}_u[i]$는 $i$가 $u$가 속한 클래스일 경우에만 1이고 나머지는 0이다. 따라서 소프트 맥스는 보이는 식보다 더 간단한데, 모델이 노드 $u$가 실제 $u$의 클래스에 속할 것이라고 예측한 확률 값이다. 식 $(6.1)$의 변형체들도 있지만 보통 식 $(6.1)$을 가장 많이 사용한다.

<br>

<div class="note-box" markdown="1">
 
<p class="note-box-title">잠깐 ! supervised, semi-supervised, transductive, inductive </p>

노드 분류 문제의 경우 한 그래프 안에서 어떤 노드들은 레이블링이 되어 있고, 어떤 노드들은 되어 있지 않을 수 있다. 

- 이때, GNN 메세지 전달 과정에 사용되고, 레이블링이 있어 손실 함수 계산에도 사용되는 노드들을 training node $\in \mathcal{V}\_\text{train}$ 라고 부른다.
- GNN 메세지 전달 과정에는 사용되지만, 레이블이 없어 손실 함수 계산에는 사용되지 않는 노드들을 transductive test node $\in \mathcal{V}\_\text{trans}$라고 한다. GNN은 transductive 노들에 대해서 여전히 hidden state 벡터를 만들지만 레이블이 없어서 손실 함수 계산에는 사용할 수 없다.
- 한편, 레이블링은 있지만 메세지 전달 과정과 손실 함수 계산에서 사용하지 않았다가 모델의 성능 평가용으로 사용하는 노드들을 inductive test 노드라고 부른다.

transductive test node, inductive test node 둘 다 학습이 완료된 모델의 예측 대상이 되는 테스트 노드이다. 노드 분류 문제의 경우 transductive test node를 훈련 과정에서 사용하는 성질 때문에 semi supervised learning이라고 불린다.

</div>

---

## 6.1.2 GNN for Graph Classification
$\mathcal{T}=\\{\mathcal{G}_1, \mathcal{G}_2, \cdots, \mathcal{G}_n\\}$을 우리가 갖고 있는 $n$개의 그래프라고 하자. 그리고 $\mathbf{z}\_{\mathcal{G}_i}, i=1,2,\cdots,n$를 GNN을 통해 만든 각 그래프에 대한 임베딩 벡터라고 하자. 그래프의 임베딩 벡터는 GNN의 마지막 레이어를 통과한 노드들의 hidden state 벡터들을 적절히 취합해서 만들어진다. 보통은 $\mathbf{z}\_\mathcal{G}$를 바로 분류나 회귀에 사용하지 않고 다층 퍼텝트론을 통과시킨 $\operatorname{MLP}(\mathbf{z}\_\mathcal{G})$를 사용한다.  분류 문제의 경우 위에서 다뤘던 손실 함수를 사용하면 된다. 회귀 문제의 경우 평범한 평균 오차 제곱 손실 함수를 사용한다.

$$\mathcal{L}=\sum\limits_{i=1}^n \lVert \operatorname{MLP}(\mathbf{z}_{\mathcal{G}_i})-y_{\mathcal{G}_i} \rVert_2^2, \quad \quad (6.3)$$

<br>

---

## 6.1.3 GNNs for Relation Prediction
Shallow embedding 대신 GNN을 통과하여 만들어진 노드 임베딩 벡터들을 사용하여 Chapter $3$과 $4$에서 배웠던 손실 함수를 사용하면 된다고 한다. (자세히 좀 설명해달라고~)

<br>

---

## 6.1.4 Pretraining GNNs
우리가 Chapter $3$에서 배웠던 노드 임베딩 방법들은 모두 비지도 학습이었다. 따라서 Chapter $3$에서 다뤘던 손실 함수 (reconstruction loss)를 사용하면 레이블 없이도 네트워크 학습이 가능하다. 네트워크를 먼저 사전 훈련하고 이후 downstream 문제에 사용하면 성능 향상을 기대할 수 있다.<br><br>

하지만 놀랍게도 위 방법으로 사전 훈련한 네트워크를 사용해도 그렇게 큰 성능 향상은 없다고 한다. Chapter $3$에서 배운 방법들은 이웃 노드 또는 $k$-hop 노드 정보를 사용하는데, 임의로 초기화된 GNN이더라도 충분히 그런 정보를 학습할 수 있기 때문이다. 따라서 reconstruction loss를 사용해서 모델을 사전 훈련하는 것은 그렇게 좋은 방법은 아니다.<br><br>

비지도 학습 기반으로 모델을 사전 훈련시킬 수 있는 방법으로는 2019년에 소개된 `Deep Graph Infomax (DGI)` 이 있다.  `DGI`에서는 노드 임베딩 벡터 $\mathbf{z}_u$와 그래프 임베딩 벡터 $\mathbf{z}\_\mathcal{G}$ 사이의 상호 정보량 (mutual information)을 최대화하는 방법으로 네트워크를 사전 훈련한다. `DGI`의 손실 함수는 다음과 같다.

$$\mathcal{L}=-\sum\limits_{u \in \mathcal{V}_\text{train}
}\mathbb{E}_\mathcal{G}\operatorname{log}(D(\mathbf{z}_u, \mathbf{z}_\mathcal{G}))+\gamma\mathbb{E}_{\tilde{\mathcal{G}}}\operatorname{log}(1-D(\tilde{\mathbf{z}_u},\mathbf{z}_\mathcal{G})). \quad \quad  (6.4)$$

<br>

GAN의 손실 함수와 매우 유사하다. 표기를 설명하기 전에 직관적인 설명은 다음과 같다.

- $D$가 노드 임베딩 벡터와 그래프 임베딩 벡터를 입력 받아서 해당 그래프가 원래 그래프인지 또는 변형된 그래프인지 구분하는 분류 모델이다.
- $\tilde{\mathcal{G}}$는 변형된 (corrupted) 그래프이다. 원래 그래프 $\mathcal{G}$에서 노드의 feature 벡터나 인접 행렬의 원소의 순서를 임의로 섞어서 만들게 된다.
- $\mathbf{z}_u$는 원래 그래프 $\mathcal{G}$를 GNN에 입력했을 때 나온 노드 임베딩 벡터, $\tilde{\mathbf{z}}_u$는 변형된 그래프 $\tilde{\mathcal{G}}$를 입력했을 때 나온 임베딩 벡터이다.
- 분류기 $D$는 $\mathbf{z}_u$와 $\mathbf{z}\_\mathcal{G}$를 입력 받았을 때는 1이라 예측을 해야 손실 함수가 작아지고, $\tilde{\mathbf{z}}_u$와 $\mathbf{z}\_\mathcal{G}$를 입력 받았을 때는 0이라고 예측을 해야 손실 함수가 작아진다.
- 따라서 $\mathbf{z}_u$와 $\mathbf{z}\_\mathcal{G}$가 서로 서로 잘 예측할 수 있도록 임베딩 된다.

<br>

이런 류의 실제 그래프와 변형된 그래프를 사용해서 네트워크를 사전 훈련시키는 방법이 좋은 성과를 보이고 있고, 그래서 연구도 많이 되고 있다고 한다. 흥미로운 논문인 것 같다. 하지만 나는 논문을 읽지 않았기 떄문에 책을 읽고 이해한 내용만 적어 놓았다. 관심 있는 분들은 논문을 읽어보면 좋을 것 같다.

<br>

---

# 6.2 Efficiency Concerns and Node Sampling
**Coming soon!**

<br>

---

## 참고문헌
<p id="ref1">[1] <a href="https://pixabay.com/ko/illustrations/%ec%a7%80%ea%b5%ac-%ed%9a%8c%eb%a1%9c%eb%a7%9d-3537401/" target="_blank">https://pixabay.com/ko/illustrations/지구-회로망-3537401/</a></p>
<p id="ref2">[2] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref3">[3] <a href="http://web.stanford.edu/class/cs224w/" target="_blank">CS224W: Machine Learning with Graphs</a></p>
