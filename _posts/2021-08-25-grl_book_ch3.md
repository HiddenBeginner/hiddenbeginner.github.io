---
layout: post
title:  "[GRL Book 정리] Chapter 3. Neighborhood Reconstruction Methods"
date:   2021-8-25 20:00
categories: [Others]
use_math: true
comments: true
---

![intro](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/earth-network.jpg){: .center}
<center>사진 출처: <a href="#ref1">[1]</a></center>

# <center>Chapter 3. Neighborhood Reconstruction Methods</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 세 번째 이야기. 부디 완주하게 기도해주세요 !

<br>

---

이번 챕터에서는 노드를 벡터로 표현하는 방법들에 대해서 알아본다. 본격적인 시작에 앞서 이번 챕터에서 자주 마주치게 될 용어들을 알아보고 가자. 

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 빈출 어휘 정리</p>

- 노드를 $d$-차원 벡터로 나타내는 행위 또는 시도를 **임베딩** 또는 **인코딩**이라고 부를 것이다.
    - 임베딩이란 표현은 한 노드를 벡터 공간의 한 벡터로 보내는 느낌이 강하고
    - 인코딩이란 표현은 노드가 갖고 있는 정보를 임베딩 벡터에 반영 (부호화)하는 느낌이 강하다.
- 임베딩된 벡터들이 살고 있는 공간을 **latent space** 또는 **low dimensional space** 라고 부른다.
- 노드 임베딩을 수행하는 알고리즘을 **노드 임베딩 알고리즘**, **노드 임베딩 기법**, **노드 임베딩** 등으로 부를 것이다.
- 임베딩된 벡터를 **노드 임베딩 벡터** 또는 **임베딩 벡터**라고 부를 것이다.
</div>

노드 임베딩의 목표는 그래프 안의 각 노드를 $d$-차원 벡터로 나타내는 것이다. 이때,  임베딩된 벡터는 **그래프 안에서 해당 노드의 위치**나 **주변 이웃 노드들의 구조**를 반영하고 있어야 한다. 조금 더 뜬구름을 잡아보면, latent space에서 두 벡터 사이의 기하적인 관계가 그래프 안에서 대응하는 두 노드 사이의 관계가 될 수 있도록 임베딩 벡터를 만들고 싶은 것이다.

<br>

---

# 3.1 An Encoder-Decoder Perspective
대부분의 노드 임베딩 알고리즘들은 인코더-디코더 구조를 갖고 있다.  

- `인코더`는 각 노드를 입력 받아 임베딩 벡터를 출력해주는 역할을 한다.
- `디코더`는 임베딩 벡터들을 입력 받아서 그래프 안에서 관심 있는 통계량을 계산해주는 역할을 한다.
    - 예를 들어, 디코더는 2개의 노드 임베딩 벡터를 입력 받아서 두 노드가 그래프 안에서 서로 이웃인지 아닌지를 예측해줄 수도 있다.

이번 섹션에서는 인코더-디코더 구조를 갖는 노드 임베딩 알고리즘의 구성 요소에 대해 알아본다.

<br>

---

## 3.1.1 The Encoder
그래프 $G=(V, E)$가 있다고 하자. `인코더` $\text{ENC}$는 각 노드 $u \in V$를 입력 받아 $d$-차원 벡터 $\mathbf{z}_u \in \mathbb{R}^d$를 출력해주는 함수이다. 즉,

- $\text{ENC}:V \rightarrow \mathbb{R}^d \; \text{ such that } \; \text{ENC}(u)=\mathbf{z}_u \; \; \forall u \in V\quad \quad (3.1)$

더 정확하게는 노드 $u$를 숫자로 표현할 수 없기 때문에 노드 $u$의 인덱스를 입력 받아서 임베딩 벡터를 출력해준다. (각 노드마다 고유한 숫자를 부여 받은 상황을 가정하기 때문에 노드 $u$를 입력 받는다는 표현도 틀린 표현은 아니다.)

<br>

대부분의 노드 임베딩 알고리즘들은 각 노드마다 임베딩 벡터를 모두 구한 후 행렬에 저장해놓는다. 그리고 필요할 때 행렬에서 임베딩 벡터를 꺼내서 사용하는 `table-lookup` 방식을 택한다. 즉, 각 노드에 대응하는 임베딩 벡터를 행벡터로 갖는 행렬 $\mathbf{Z} \in \mathbb{R}^{\mid V \mid \times d}$이 있을 때, `인코더`는 관심 있는 노드 $v$를 입력 받아서 $\mathbf{Z}$의 $v$번 째 행을 출력해준다. 즉,

- $\text{ENC}(v)=\mathbf{Z}[v] \quad \quad (3.2)$

<br>

이렇게 노드 임베딩 벡터가 미리 구해진 상황에서 `table-lookup` 방식으로 노드를 임베딩 하는 방식을 `shallow embedding`이라고 부른다.  이번 장에서 다룰 대부분의 노드 임베딩 알고리즘은 `shallow embedding`이다. `shallow embedding`이 아닌 대표적인 알고리즘으로는 나중에 배울 GNN이 있다고 한다.

<br>

---

## 3.1.2 The Decoder
`디코더`는 임베딩 벡터들을 입력 받아서 그래프 안에서 관심 있는 통계량을 복원해주는 역할을 한다. 벡터들을 입력 받아 어떤 숫자를 내뱉는데, 그 숫자가 그래프 안에서의 어떤 통계량과 일치하기를 바라는 것이다. 대부분의 `디코더`는 두 개의 임베딩 벡터를 입력 받아서 두 노드 사이의 연결성 또는 유사성을 출력해주도록 모델링 된다. 즉,

- $\text{DEC}:\mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}\quad \quad (3.3)$

<br>

가장 쉬운 예시로서, 두 개의 임베딩 벡터 $\mathbf{z}_u$와 $\mathbf{z}_v$를 입력 받아서 두 노드 $u$와 $v$가 그래프 안에서 서로 이웃일 확률을 출력해주는 `디코더`를 생각해볼 수 있다.  좋은 디코더라면

- 두 노드 $u$와 $v$가 그래프 안에서 이웃이 아니라면, $\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)=0$을 출력해줄 것이고,
- 두 노드 $u$와 $v$가 그래프 안에서 이웃라면, $\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)=1$을 출력해줄 것이다.

<br>

따라서 인코더-디코더 구조를 갖는 노드 임베딩의 목표는 다음 식을 만족시키는 것이다.

- $\text{DEC}(\text{ENC}(u),\text{ENC}(v))=\text{DEC}(\mathbf{z}_u, \mathbf{z}_v) \approx \mathbf{S}[u,v] \;\; \forall u, v \in V \quad \quad (3.4)$

여기서 
$\mathbf{S} \in \mathbb{R}^{\mid V \mid \times \mid V \mid}$는 노드쌍 $(u, v)$마다 관심 있는 그래프 통계량이 저장된 행렬이다. 주로 그래프 안에서 두 노드 사이의 유사도를 나타내기 떄문에 similarity의 S를 사용한 것 같다. 두 노드 사이의 유사도를 정의하는 가장 쉬운 방법은 서로 이웃하면 유사하다고 정의하는 것이다. 즉, $\mathbf{S}:=\mathbf{A}$이다. 또는, 챕터 $2.2$에서 배운 neighborhood overlap statistic을 사용하여 행렬 $\mathbf{S}$를 만들 수도 있다.

<br>

---

## 3.1.3 Optimizing an Encoder-Decoder Model
우리는 식 $(3.4)$를 만족시키는 `인코더`와 `디코더`를 찾기 위하여 다음과 같은 손실 함수를 정의한다. 그리고 손실 함수를 최소로 만들어주는 `인코더`와 `디코더`를 찾게 된다.

- $\mathcal{L}=\sum\limits_{(u, v) \in \mathcal{D}} \mathcal{l} (\text{DEC}(\mathbf{z}_u, \mathbf{z}_v), \mathbf{S}[u,v]), \quad \quad (3.5)$

이때, $\mathcal{l}:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}$ 은 디코더가 예측한 값과 실제 값의 차이를 구해주는 함수이다. $\mathcal{D}$는 훈련 데이터로 사용할 수 있는 노드쌍들의 집합을 나타낸다. 식 $(3.5)$을 최소화시키기 위하여 대부분 노드 임베딩 알고리즘들은 stochastic gradient descent을 택하지만, 일부 알고리즘은 matrix factorization 등의 방법을 사용하기도 한다.

<br>

---

## 3.1.4 Overview of the Encoder-Decoder Approach
인코더-디코더 구조를 갖는 노드 임베딩은 다음 세 가지를 어떻게 선택하느냐에 따라서 달라질 수 있다. (`인코더`는 `shallow embedding`이다. 모든 임베딩 벡터들의 원소가 훈련을 통해 업데이트될 파라미터로 구성된다.) 

- 디코더 함수 $\text{DEC}$
- 유사도 행렬 $\mathbf{S}$
- 손실 함수 $\mathcal{l}$

<br>

위 세 가지 요소의 선택에 따라서 노드 임베딩 알고리즘 다음과 같이 요약할 수 있다.
![shallow-embedding](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/shallow-embedding.png){: width="600"}{: .center}
<center>사진 출처: <a href="#ref2">[2]</a></center>

<br>

이번 장의 나머지 부분에 대해서는 위 알고리즘들을 하나씩 알아본다.  

- 섹션 $(3.2)$에서는 matrix factorization 기반의 노드 임베딩 알고리즘에 대해 다뤄본다.
- 섹션 $(3.3)$에서는 비교적 최신 기법인 랜덤 워킹 기반 노드 임베딩 알고리즘에 대해 다뤄본다.

<br>

---

# 3.2 Factorization-based
### Laplacian Eigenmaps
섹션 $3.1$에서 디코더 함수, 손실 함수, 유사도 행렬에 따라서 노드 임베딩 알고리즘을 분류할 수 있다고 했다. 한편 디코더 함수는 두 개의 노드 임베딩 벡터를 입력 받아 그래프 안에서 두 노드 사이의 관계를 출력해주는 함수라고도 했다. 예를 들어, 두 임베딩 벡터 사이의 거리를 디코더 함수로 사용한다면 그래프 안에서 두 노드가 얼마나 다른지에 관심 있는 것이다.  

- $\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)=\lVert \mathbf{z}_u - \mathbf{z}_v \rVert^2_2 \quad \quad (3.5)$

<br>

한편, 행렬 $\mathbf{S} \in \mathbb{R}^{\mid V \mid \times \mid V \mid}$에는 노드쌍 $(u, v)$마다 그래프 안에서의 관계가 저장되어 있다. 주로 두 노드 사이의 유사도가 저장되어 있기 때문에 유사도 행렬이라고도 부른다. 생각해볼 수 있는 가장 쉬운 유사도 행렬은 인접 행렬 $\mathbf{A}$일 것이다. 그래프 안에서 유사한 두 노드에 대해서는 임베딩 공간에서도 가깝게 임베딩시키고 싶다면, 손실함수를 다음과 같이 디자인해야 할 것이다. 

- $\mathcal{L}=\sum\limits\_{(u, v) \in \mathcal{D}}\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)\cdot \mathbf{S}[u,v] \quad \quad (3.6)$

<br>

그래프 안에서 유사한 두 노드는 $\mathbf{S}[u, v]$ 값이 크기 때문에 손실 함수를 최소화시키기 위해서는 $\lVert \mathbf{z}_u - \mathbf{z}_v \rVert^2_2$ 값을 줄여야 할 것이다. 즉, 두 임베딩 벡터를 가깝게 위치시켜야 할 것이다. 한편, 그래프 안에서 관계가 없는 두 노드는 $\mathbf{S}[u, v]$가 0에 가깝다. 따라서 $\lVert \mathbf{z}_u - \mathbf{z}_v \rVert^2_2$ 값을 줄여주지 않아도 괜찮을 것이다. 

<br>

이때, 유사도 행렬 $\mathbf{S}$를 Laplacian 행렬의 성질을 만족하는 행렬로 사용하면 식 $(3.6)$을 최소화시키는 문제는 섹션 $2.3$에서 다루었던 `spectral clustering` 문제를 푸는 것과 같아진다고 한다. 즉, $d$-차원 노드 임베딩 벡터을 찾는다고 생각하면, $\min\limits_{\mathbf{Z} \in \mathbb{R}^{\mid V \mid \times d}} \mathcal{L}$의 해는 Laplacian 행렬의 가장 작은 eigenvalue를 제외한 $d$개의 가장 작은 eigenvalue 대응하는 eigenvector를 열벡터로 갖는 행렬이 된다. 위와 같은 세팅으로 노드 임베딩을 하는 알고리즘을 `Laplacian eigenmaps`라고 부른다고 한다.

<br>

한 가지 의문점이 있다. 책에서는 "Laplacian 행렬의 성질을 만족하는 행렬"을 사용해서 식 $(3.6)$을 최소화시킨다고 했다. 그럼 어떤 Laplacian을 사용한다는 것일까? 우리가 배운 Laplacian 행렬은 크게 두 가지가 있다.
- $\mathbf{L}=\mathbf{D}-\mathbf{A}$를 사용할 경우 서로 다른 두 노드쌍 $$(u, v)$$의 유사도는 모두 음수가 된다. 그럼 그래프 안에서 가까운 노드를 더 멀리 임베딩시킬 것이다.
- $\mathbf{L}=\mathbf{D}^{-1} \mathbf{A}$를 사용할 경우, $\mathbf{L}$의 모든 원소가 양수이다. 그리고 디코더 함수값도 항상 양수이다. 따라서 손실함수의 lower bound는 0이다. 그리고 모든 노드를 영벡터로 임베딩하면 최소값을 얻을 수 있다.

아마 관련 논문을 직접 읽어봐야 의문점이 해소될 것 같다. 미래의 나 ! 부탁해 !!

<br>

---

### Inner-product Methods
한편, 두 노드 임베딩 벡터 사이의 `dot product`를 디코더 함수로 사용할 수도 있다.  그래프 안에서 두 노드 사이의 유사도가 두 노드 임베딩 벡터의 `dot product`에 비례할 것이라고 가정하는 것이다.

- $\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)=\mathbf{z}_u^T \mathbf{z}_v \quad \quad (3.7)$

<br>

그리고 `mean-squared error`를 손실 함수로 사용할 수도 있다. 즉,

- $\mathcal{L}=\sum\limits\_{(u,v) \in \in \mathcal{D}} \lVert \text{DEC}(\mathbf{z}_u, \mathbf{z}_v) - \mathbf{S}[u,v] \rVert^2_2 \quad \quad (3.8)$

이때 유사도 행렬 $\mathbf{S}$로 인접행렬 $\mathbf{A}$를 사용하는 노드 임베딩 알고리즘을 `Graph Factorization`이라고 부른다. `GraRep` 노드 임베딩 알고리즘은 인접행렬 $\mathbf{A}$의 거듭제곱꼴 기반의 행렬을 유사도 행렬으로 사용한다. 한편, `HOPE` 알고리즘에서는 neighborhood overlap measure을 사용해서 유사도 행렬을 만들고 노드 임베딩을 수행하는 일반적인 방법을 제시한다.

<br>

위와 같은 방법들을 `matrix-factorization` 방법이라고 부른다. 각 $d$-차원 노드 임베딩 벡터를 행벡터로 갖는 행렬을 $\mathbf{Z} \in \mathbb{R}^{\mid V \mid \times d}$라고 하자. 식 $(3.8)$의 손실함수를 다음과 같이 행렬 형태로 바꿔서 근사시킬 수 있다.

- $\mathcal{L} \approx \lVert \mathbf{Z} \mathbf{Z}^T - \mathbf{S} \rVert^2_2 \quad \quad (3.9)$

<br>

식 $(3.9)$는 모든 노드쌍 $(u,v)$에 대해서 오차가 더해지고, 식 $(3.8)$은 우리가 갖고 있는 엣지 $(u,v)$에 대해서만 오차가 더해진다는 점에서 차이가 있다.  식 $(3.9)$를 최소화한다는 것은 $\mid V \mid \times \mid V \mid$ 크기의 행렬 $\mathbf{S}$를 정보의 손실을 최소화하며 $\mathbf{Z} \mathbf{Z}^T$로 분해한다는 것이다. 이런 관점에서 `matrix-factorization` 방법이라고 부르는 것이다.

<br>

---

# 3.3 Random Walk Embeddings
섹션 $3.2$에서 정의했던 유사도 행렬들은 그래프가 주어지면 딱 계산해서 구할 수 있었다. 그래프에 따라 유사도 행렬의 원소들이 딱 정해지기 때문에 `deterministic measure` 를 사용하여 행렬을 정의했다고 할 수 있다. 한편, 랜덤 워크를 이용해서 유사도 행렬을 정의하는 방법들도 있다. 이 경우 랜덤 워크의 랜덤한 성질 때문에 `stochastic measure` 를 사용해서 행렬을 정의한다고 말한다.  랜덤 워크 기반 노드 임베딩들은 **짧은 길이의 랜덤 워크에서 어떤 두 노드가 동시에 등장할 확률이 높다면 해당 두 노드가 서로 유사하다고 가정한다.** 

<br>

랜덤 워크를 이용한 가장 대표적인 알고리즘으로 `DeepWalk`와 `node2vec`이 있다. 이 책에는 두 알고리즘에 대해 아주 간략히 다루고 있다. 따라서 CS224W <a href="#ref3">[3]</a>에서 배운 내용을 정리하려고 한다. 따라서 내용과 표기법이 책과 차이가 있을 수 있다.

<br>

**Coming soon!**

---

## 참고문헌
<p id="ref1">[1] <a href="https://pixabay.com/ko/illustrations/%ec%a7%80%ea%b5%ac-%ed%9a%8c%eb%a1%9c%eb%a7%9d-3537401/" target="_blank">https://pixabay.com/ko/illustrations/지구-회로망-3537401/</a></p>
<p id="ref2">[2] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref3">[3] <a href="http://web.stanford.edu/class/cs224w/" target="_blank">CS224W: Machine Learning with Graphs</a></p>
