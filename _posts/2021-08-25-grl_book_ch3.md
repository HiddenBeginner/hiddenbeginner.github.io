---
layout: post
title:  "[GRL Book 정리] Chapter 3. Neighborhood Reconstruction Methods"
date:   2021-8-25 20:00
categories: [Others]
use_math: true
comments: true
---

![intro](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/earth-network.jpg){: .center}
<center>사진 출처: <a href="#ref1">[1]</a></center>

# <center>Chapter 3. Neighborhood Reconstruction Methods</center>

[Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/) 읽고 정리하기 시리즈 중 세 번째 이야기. 부디 완주하게 기도해주세요 !

<br>

---

이번 챕터에서는 노드를 벡터로 표현하는 방법들에 대해서 알아본다. 본격적인 시작에 앞서 이번 챕터에서 자주 마주치게 될 용어들을 알아보고 가자. 

<div class="note-box" markdown="1">

<p class="note-box-title">잠깐 ! 빈출 어휘 정리</p>

- 노드를 $d$-차원 벡터로 나타내는 행위 또는 시도를 **임베딩** 또는 **인코딩**이라고 부를 것이다.
    - 임베딩이란 표현은 한 노드를 벡터 공간의 한 벡터로 보내는 느낌이 강하고
    - 인코딩이란 표현은 노드가 갖고 있는 정보를 임베딩 벡터에 반영 (부호화)하는 느낌이 강하다.
- 임베딩된 벡터들이 살고 있는 공간을 **latent space** 또는 **low dimensional space** 라고 부른다.
- 노드 임베딩을 수행하는 알고리즘을 **노드 임베딩 알고리즘**, **노드 임베딩 기법**, **노드 임베딩** 등으로 부를 것이다.
- 임베딩된 벡터를 **노드 임베딩 벡터** 또는 **임베딩 벡터**라고 부를 것이다.
</div>

노드 임베딩의 목표는 그래프 안의 각 노드를 $d$-차원 벡터로 나타내는 것이다. 이때,  임베딩된 벡터는 **그래프 안에서 해당 노드의 위치**나 **주변 이웃 노드들의 구조**를 반영하고 있어야 한다. 조금 더 뜬구름을 잡아보면, latent space에서 두 벡터 사이의 기하적인 관계가 그래프 안에서 대응하는 두 노드 사이의 관계가 될 수 있도록 임베딩 벡터를 만들고 싶은 것이다.

<br>

---

# 3.1 An Encoder-Decoder Perspective
대부분의 노드 임베딩 알고리즘들은 인코더-디코더 구조를 갖고 있다.  

- `인코더`는 각 노드를 입력 받아 임베딩 벡터를 출력해주는 역할을 한다.
- `디코더`는 임베딩 벡터들을 입력 받아서 그래프 안에서 관심 있는 통계량을 계산해주는 역할을 한다.
    - 예를 들어, 디코더는 2개의 노드 임베딩 벡터를 입력 받아서 두 노드가 그래프 안에서 서로 이웃인지 아닌지를 예측해줄 수도 있다.

이번 섹션에서는 인코더-디코더 구조를 갖는 노드 임베딩 알고리즘의 구성 요소에 대해 알아본다.

<br>

---

## 3.1.1 The Encoder
그래프 $G=(V, E)$가 있다고 하자. `인코더` $\text{ENC}$는 각 노드 $u \in V$를 입력 받아 $d$-차원 벡터 $\mathbf{z}_u \in \mathbb{R}^d$를 출력해주는 함수이다. 즉,

- $\text{ENC}:V \rightarrow \mathbb{R}^d \; \text{ such that } \; \text{ENC}(u)=\mathbf{z}_u \; \; \forall u \in V\quad \quad (3.1)$

더 정확하게는 노드 $u$를 숫자로 표현할 수 없기 때문에 노드 $u$의 인덱스를 입력 받아서 임베딩 벡터를 출력해준다. (각 노드마다 고유한 숫자를 부여 받은 상황을 가정하기 때문에 노드 $u$를 입력 받는다는 표현도 틀린 표현은 아니다.)

<br>

대부분의 노드 임베딩 알고리즘들은 각 노드마다 임베딩 벡터를 모두 구한 후 행렬에 저장해놓는다. 그리고 필요할 때 행렬에서 임베딩 벡터를 꺼내서 사용하는 `table-lookup` 방식을 택한다. 즉, 각 노드에 대응하는 임베딩 벡터를 행벡터로 갖는 행렬 $\mathbf{Z} \in \mathbb{R}^{\mid V \mid \times d}$이 있을 때, `인코더`는 관심 있는 노드 $v$를 입력 받아서 $\mathbf{Z}$의 $v$번 째 행을 출력해준다. 즉,

- $\text{ENC}(v)=\mathbf{Z}[v] \quad \quad (3.2)$

<br>

이렇게 노드 임베딩 벡터가 미리 구해진 상황에서 `table-lookup` 방식으로 노드를 임베딩 하는 방식을 `shallow embedding`이라고 부른다.  이번 장에서 다룰 대부분의 노드 임베딩 알고리즘은 `shallow embedding`이다. `shallow embedding`이 아닌 대표적인 알고리즘으로는 나중에 배울 GNN이 있다고 한다.

<br>

---

## 3.1.2 The Decoder
`디코더`는 임베딩 벡터들을 입력 받아서 그래프 안에서 관심 있는 통계량을 복원해주는 역할을 한다. 벡터들을 입력 받아 어떤 숫자를 내뱉는데, 그 숫자가 그래프 안에서의 어떤 통계량과 일치하기를 바라는 것이다. 대부분의 `디코더`는 두 개의 임베딩 벡터를 입력 받아서 두 노드 사이의 연결성 또는 유사성을 출력해주도록 모델링 된다. 즉,

- $\text{DEC}:\mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}\quad \quad (3.3)$

<br>

가장 쉬운 예시로서, 두 개의 임베딩 벡터 $\mathbf{z}_u$와 $\mathbf{z}_v$를 입력 받아서 두 노드 $u$와 $v$가 그래프 안에서 서로 이웃일 확률을 출력해주는 `디코더`를 생각해볼 수 있다.  좋은 디코더라면

- 두 노드 $u$와 $v$가 그래프 안에서 이웃이 아니라면, $\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)=0$을 출력해줄 것이고,
- 두 노드 $u$와 $v$가 그래프 안에서 이웃라면, $\text{DEC}(\mathbf{z}_u, \mathbf{z}_v)=1$을 출력해줄 것이다.

<br>

따라서 인코더-디코더 구조를 갖는 노드 임베딩의 목표는 다음 식을 만족시키는 것이다.

- $\text{DEC}(\text{ENC}(u),\text{ENC}(v))=\text{DEC}(\mathbf{z}_u, \mathbf{z}_v) \approx \mathbf{S}[u,v] \;\; \forall u, v \in V \quad \quad (3.4)$

여기서 
$\mathbf{S} \in \mathbb{R}^{\mid V \mid \times \mid V \mid}$는 노드쌍 $(u, v)$마다 관심 있는 그래프 통계량이 저장된 행렬이다. 주로 그래프 안에서 두 노드 사이의 유사도를 나타내기 떄문에 similarity의 S를 사용한 것 같다. 두 노드 사이의 유사도를 정의하는 가장 쉬운 방법은 서로 이웃하면 유사하다고 정의하는 것이다. 즉, $\mathbf{S}:=\mathbf{A}$이다. 또는, 챕터 $2.2$에서 배운 neighborhood overlap statistic을 사용하여 행렬 $\mathbf{S}$를 만들 수도 있다.

<br>

---

## 3.1.3 Optimizing an Encoder-Decoder Model
우리는 식 $(3.4)$를 만족시키는 `인코더`와 `디코더`를 찾기 위하여 다음과 같은 손실 함수를 정의한다. 그리고 손실 함수를 최소로 만들어주는 `인코더`와 `디코더`를 찾게 된다.

- $\mathcal{L}=\sum\limits_{(u, v) \in \mathcal{D}} \mathcal{l} (\text{DEC}(\mathbf{z}_u, \mathbf{z}_v), \mathbf{S}[u,v]), \quad \quad (3.5)$

이때, $\mathcal{l}:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}$ 은 디코더가 예측한 값과 실제 값의 차이를 구해주는 함수이다. $\mathcal{D}$는 훈련 데이터로 사용할 수 있는 노드쌍들의 집합을 나타낸다. 식 $(3.5)$을 최소화시키기 위하여 대부분 노드 임베딩 알고리즘들은 stochastic gradient descent을 택하지만, 일부 알고리즘은 matrix factorization 등의 방법을 사용하기도 한다.

<br>

---

## 3.1.4 Overview of the Encoder-Decoder Approach
인코더-디코더 구조를 갖는 노드 임베딩은 다음 세 가지를 어떻게 선택하느냐에 따라서 달라질 수 있다. (`인코더`는 `shallow embedding`이다. 모든 임베딩 벡터들의 원소가 훈련을 통해 업데이트될 파라미터로 구성된다.) 

- 디코더 함수 $\text{DEC}$
- 유사도 행렬 $\mathbf{S}$
- 손실 함수 $\mathcal{l}$

<br>

위 세 가지 요소의 선택에 따라서 노드 임베딩 알고리즘 다음과 같이 요약할 수 있다.
![shallow-embedding](https://raw.githubusercontent.com/HiddenBeginner/hiddenbeginner.github.io/master/static/img/_posts/2021-08-25-grl_book_ch3/shallow-embedding.png){: width="600"}{: .center}
<center>사진 출처: <a href="#ref2">[2]</a></center>

<br>

이번 장의 나머지 부분에 대해서는 위 알고리즘들을 하나씩 알아본다.  

- 섹션 $(3.2)$에서는 matrix factorization 기반의 노드 임베딩 알고리즘에 대해 다뤄본다.
- 섹션 $(3.3)$에서는 비교적 최신 기법인 랜덤 워킹 기반 노드 임베딩 알고리즘에 대해 다뤄본다.

## 참고문헌
<p id="ref1">[1] <a href="https://pixabay.com/ko/illustrations/%ec%a7%80%ea%b5%ac-%ed%9a%8c%eb%a1%9c%eb%a7%9d-3537401/" target="_blank">https://pixabay.com/ko/illustrations/지구-회로망-3537401/</a></p>
<p id="ref2">[2] Hamilton, William L.,Graph Representation Learning, <i>Synthesis Lectures on Artificial Intelligence and Machine Learning</i>, 14, pp.1-159</p>
<p id="ref3">[3] <a href="http://web.stanford.edu/class/cs224w/" target="_blank">CS224W: Machine Learning with Graphs</a></p>
